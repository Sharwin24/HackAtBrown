{
    "1": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"A Predictor wrapping a one-step Predictor to make autoregressive predictions.\n\"\"\"\n\nfrom typing import Optional, cast\n\nfrom absl import logging\nfrom graphcast import predictor_base\nfrom graphcast import xarray_jax\nfrom graphcast import xarray_tree\nimport haiku as hk\nimport jax\nimport xarray\n\n\ndef _unflatten_and_expand_time(flat_variables, tree_def, time_coords):\n  variables = jax.tree_util.tree_unflatten(tree_def, flat_variables)\n  return variables.expand_dims(time=time_coords, axis=0)\n\n\ndef _get_flat_arrays_and_single_timestep_treedef(variables):\n  flat_arrays = jax.tree_util.tree_leaves(variables.transpose('time', ...))\n  _, treedef = jax.tree_util.tree_flatten(variables.isel(time=0, drop=True))\n  return flat_arrays, treedef\n\n\nclass Predictor(predictor_base.Predictor):\n  \"\"\"Wraps a one-step Predictor to make multi-step predictions autoregressively.\n\n  The wrapped Predictor will be used to predict a single timestep conditional\n  on the inputs passed to the outer Predictor. Its predictions are then\n  passed back in as inputs at the next timestep, for as many timesteps as are\n  requested in the targets_template. (When multiple timesteps of input are\n  used, a rolling window of inputs is maintained with new predictions\n  concatenated onto the end).\n\n  You may ask for additional variables to be predicted as targets which aren't\n  used as inputs. These will be predicted as output variables only and not fed\n  back in autoregressively. All target variables must be time-dependent however.\n\n  You may also specify static (non-time-dependent) inputs which will be passed\n  in at each timestep but are not predicted.\n\n  At present, any time-dependent inputs must also be present as targets so they\n  can be passed in autoregressively.\n\n  The loss of the wrapped one-step Predictor is averaged over all timesteps to\n  give a loss for the autoregressive Predictor.\n  \"\"\"\n\n  def __init__(\n      self,\n      predictor: predictor_base.Predictor,\n      noise_level: Optional[float] = None,\n      gradient_checkpointing: bool = False,\n      ):\n    \"\"\"Initializes an autoregressive predictor wrapper.\n\n    Args:\n      predictor: A predictor to wrap in an auto-regressive way.\n      noise_level: Optional value that multiplies the standard normal noise\n        added to the time-dependent variables of the predictor inputs. In\n        particular, no noise is added to the predictions that are fed back\n        auto-regressively. Defaults to not adding noise.\n      gradient_checkpointing: If True, gradient checkpointing will be\n        used at each step of the computation to save on memory. Roughtly this\n        should make the backwards pass two times more expensive, and the time\n        per step counting the forward pass, should only increase by about 50%.\n        Note this parameter will be ignored with a warning if the scan sequence\n        length is 1.\n    \"\"\"\n    self._predictor = predictor\n    self._noise_level = noise_level\n    self._gradient_checkpointing = gradient_checkpointing\n\n  def _get_and_validate_constant_inputs(self, inputs, targets, forcings):\n    constant_inputs = inputs.drop_vars(targets.keys(), errors='ignore')\n    constant_inputs = constant_inputs.drop_vars(\n        forcings.keys(), errors='ignore')\n    for name, var in constant_inputs.items():\n      if 'time' in var.dims:\n        raise ValueError(\n            f'Time-dependent input variable {name} must either be a forcing '\n            'variable, or a target variable to allow for auto-regressive '\n            'feedback.')\n    return constant_inputs\n\n  def _validate_targets_and_forcings(self, targets, forcings):\n    for name, var in targets.items():\n      if 'time' not in var.dims:\n        raise ValueError(f'Target variable {name} must be time-dependent.')\n\n    for name, var in forcings.items():\n      if 'time' not in var.dims:\n        raise ValueError(f'Forcing variable {name} must be time-dependent.')\n\n    overlap = forcings.keys() & targets.keys()\n    if overlap:\n      raise ValueError('The following were specified as both targets and '\n                       f'forcings, which isn\\'t allowed: {overlap}')\n\n  def _update_inputs(self, inputs, next_frame):\n    num_inputs = inputs.dims['time']\n\n    predicted_or_forced_inputs = next_frame[list(inputs.keys())]\n\n    # Combining datasets with inputs and target time stamps aligns them.\n    # Only keep the num_inputs trailing frames for use as next inputs.\n    return (xarray.concat([inputs, predicted_or_forced_inputs], dim='time')\n            .tail(time=num_inputs)\n            # Update the time coordinate to reset the lead times for\n            # next AR iteration.\n            .assign_coords(time=inputs.coords['time']))\n\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               **kwargs) -> xarray.Dataset:\n    \"\"\"Calls the Predictor.\n\n    Args:\n      inputs: input variable used to make predictions. Inputs can include both\n        time-dependent and time independent variables. Any time-dependent\n        input variables must also be present in the targets_template or the\n        forcings.\n      targets_template: A target template containing informations about which\n        variables should be predicted and the time alignment of the predictions.\n        All target variables must be time-dependent.\n        The number of time frames is used to set the number of unroll of the AR\n        predictor (e.g. multiple unroll of the inner predictor for one time step\n        in the targets is not supported yet).\n      forcings: Variables that will be fed to the model. The variables\n        should not overlap with the target ones. The time coordinates of the\n        forcing variables should match the target ones.\n        Forcing variables which are also present in the inputs, will be used to\n        supply ground-truth values for those inputs when they are passed to the\n        underlying predictor at timesteps beyond the first timestep.\n      **kwargs: Additional arguments passed along to the inner Predictor.\n\n    Returns:\n      predictions: the model predictions matching the target template.\n\n    Raise:\n      ValueError: if the time coordinates of the inputs and targets are not\n        different by a constant time step.\n    \"\"\"\n\n    constant_inputs = self._get_and_validate_constant_inputs(\n        inputs, targets_template, forcings)\n    self._validate_targets_and_forcings(targets_template, forcings)\n\n    # After the above checks, the remaining inputs must be time-dependent:\n    inputs = inputs.drop_vars(constant_inputs.keys())\n\n    # A predictions template only including the next time to predict.\n    target_template = targets_template.isel(time=[0])\n\n    flat_forcings, forcings_treedef = (\n        _get_flat_arrays_and_single_timestep_treedef(forcings))\n    scan_variables = flat_forcings\n\n    def one_step_prediction(inputs, scan_variables):\n\n      flat_forcings = scan_variables\n      forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef,\n                                            target_template.coords['time'])\n\n      # Add constant inputs:\n      all_inputs = xarray.merge([constant_inputs, inputs])\n      predictions: xarray.Dataset = self._predictor(\n          all_inputs, target_template,\n          forcings=forcings,\n          **kwargs)\n\n      next_frame = xarray.merge([predictions, forcings])\n      next_inputs = self._update_inputs(inputs, next_frame)\n\n      # Drop the length-1 time dimension, since scan will concat all the outputs\n      # for different times along a new leading time dimension:\n      predictions = predictions.squeeze('time', drop=True)\n      # We return the prediction flattened into plain jax arrays, because the\n      # extra leading dimension added by scan prevents the tree_util\n      # registrations in xarray_jax from unflattening them back into an\n      # xarray.Dataset automatically:\n      flat_pred = jax.tree_util.tree_leaves(predictions)\n      return next_inputs, flat_pred\n\n    if self._gradient_checkpointing:\n      scan_length = targets_template.dims['time']\n      if scan_length <= 1:\n        logging.warning(\n            'Skipping gradient checkpointing for sequence length of 1')\n      else:\n        # Just in case we take gradients (e.g. for control), although\n        # in most cases this will just be for a forward pass.\n        one_step_prediction = hk.remat(one_step_prediction)\n\n    # Loop (without unroll) with hk states in cell (jax.lax.scan won't do).\n    _, flat_preds = hk.scan(one_step_prediction, inputs, scan_variables)\n\n    # The result of scan will have an extra leading axis on all arrays,\n    # corresponding to the target times in this case. We need to be prepared for\n    # it when unflattening the arrays back into a Dataset:\n    scan_result_template = (\n        target_template.squeeze('time', drop=True)\n        .expand_dims(time=targets_template.coords['time'], axis=0))\n    _, scan_result_treedef = jax.tree_util.tree_flatten(scan_result_template)\n    predictions = jax.tree_util.tree_unflatten(scan_result_treedef, flat_preds)\n    return predictions\n\n  def loss(self,\n           inputs: xarray.Dataset,\n           targets: xarray.Dataset,\n           forcings: xarray.Dataset,\n           **kwargs\n           ) -> predictor_base.LossAndDiagnostics:\n    \"\"\"The mean of the per-timestep losses of the underlying predictor.\"\"\"\n    if targets.sizes['time'] == 1:\n      # If there is only a single target timestep then we don't need any\n      # autoregressive feedback and can delegate the loss directly to the\n      # underlying single-step predictor. This means the underlying predictor\n      # doesn't need to implement .loss_and_predictions.\n      return self._predictor.loss(inputs, targets, forcings, **kwargs)\n\n    constant_inputs = self._get_and_validate_constant_inputs(\n        inputs, targets, forcings)\n    self._validate_targets_and_forcings(targets, forcings)\n    # After the above checks, the remaining inputs must be time-dependent:\n    inputs = inputs.drop_vars(constant_inputs.keys())\n\n    if self._noise_level:\n      def add_noise(x):\n        return x + self._noise_level * jax.random.normal(\n            hk.next_rng_key(), shape=x.shape)\n      # Add noise to time-dependent variables of the inputs.\n      inputs = jax.tree_map(add_noise, inputs)\n\n    # The per-timestep targets passed by scan to one_step_loss below will have\n    # no leading time axis. We need a treedef without the time axis to use\n    # inside one_step_loss to unflatten it back into a dataset:\n    flat_targets, target_treedef = _get_flat_arrays_and_single_timestep_treedef(\n        targets)\n    scan_variables = flat_targets\n\n    flat_forcings, forcings_treedef = (\n        _get_flat_arrays_and_single_timestep_treedef(forcings))\n    scan_variables = (flat_targets, flat_forcings)\n\n    def one_step_loss(inputs, scan_variables):\n      flat_target, flat_forcings = scan_variables\n      forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef,\n                                            targets.coords['time'][:1])\n\n      target = _unflatten_and_expand_time(flat_target, target_treedef,\n                                          targets.coords['time'][:1])\n\n      # Add constant inputs:\n      all_inputs = xarray.merge([constant_inputs, inputs])\n\n      (loss, diagnostics), predictions = self._predictor.loss_and_predictions(\n          all_inputs,\n          target,\n          forcings=forcings,\n          **kwargs)\n\n      # Unwrap to jax arrays shape (batch,):\n      loss, diagnostics = xarray_tree.map_structure(\n          xarray_jax.unwrap_data, (loss, diagnostics))\n\n      predictions = cast(xarray.Dataset, predictions)  # Keeps pytype happy.\n      next_frame = xarray.merge([predictions, forcings])\n      next_inputs = self._update_inputs(inputs, next_frame)\n\n      return next_inputs, (loss, diagnostics)\n\n    if self._gradient_checkpointing:\n      scan_length = targets.dims['time']\n      if scan_length <= 1:\n        logging.warning(\n            'Skipping gradient checkpointing for sequence length of 1')\n      else:\n        one_step_loss = hk.remat(one_step_loss)\n\n    # We can pass inputs (the initial state of the loop) in directly as a\n    # Dataset because the shape we pass in to scan is the same as the shape scan\n    # passes to the inner function. But, for scan_variables, we must flatten the\n    # targets (and unflatten them inside the inner function) because they are\n    # passed to the inner function per-timestep without the original time axis.\n    # The same apply to the optional forcing.\n    _, (per_timestep_losses, per_timestep_diagnostics) = hk.scan(\n        one_step_loss, inputs, scan_variables)\n\n    # Re-wrap loss and diagnostics as DataArray and average them over time:\n    (loss, diagnostics) = jax.tree_util.tree_map(\n        lambda x: xarray_jax.DataArray(x, dims=('time', 'batch')).mean(  # pylint: disable=g-long-lambda\n            'time', skipna=False),\n        (per_timestep_losses, per_timestep_diagnostics))\n\n    return loss, diagnostics\n",
    "4": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Check that the checkpoint serialization is reversable.\"\"\"\n\nimport dataclasses\nimport io\nfrom typing import Any, Optional, Union\n\nfrom absl.testing import absltest\nfrom graphcast import checkpoint\nimport numpy as np\n\n\n@dataclasses.dataclass\nclass SubConfig:\n  a: int\n  b: str\n\n\n@dataclasses.dataclass\nclass Config:\n  bt: bool\n  bf: bool\n  i: int\n  f: float\n  o1: Optional[int]\n  o2: Optional[int]\n  o3: Union[int, None]\n  o4: Union[int, None]\n  o5: int | None\n  o6: int | None\n  li: list[int]\n  ls: list[str]\n  ldc: list[SubConfig]\n  tf: tuple[float, ...]\n  ts: tuple[str, ...]\n  t: tuple[str, int, SubConfig]\n  tdc: tuple[SubConfig, ...]\n  dsi: dict[str, int]\n  dss: dict[str, str]\n  dis: dict[int, str]\n  dsdis: dict[str, dict[int, str]]\n  dc: SubConfig\n  dco: Optional[SubConfig]\n  ddc: dict[str, SubConfig]\n\n\n@dataclasses.dataclass\nclass Checkpoint:\n  params: dict[str, Any]\n  config: Config\n\n\nclass DataclassTest(absltest.TestCase):\n\n  def test_serialize_dataclass(self):\n    ckpt = Checkpoint(\n        params={\n            \"layer1\": {\n                \"w\": np.arange(10).reshape(2, 5),\n                \"b\": np.array([2, 6]),\n            },\n            \"layer2\": {\n                \"w\": np.arange(8).reshape(2, 4),\n                \"b\": np.array([2, 6]),\n            },\n            \"blah\": np.array([3, 9]),\n        },\n        config=Config(\n            bt=True,\n            bf=False,\n            i=42,\n            f=3.14,\n            o1=1,\n            o2=None,\n            o3=2,\n            o4=None,\n            o5=3,\n            o6=None,\n            li=[12, 9, 7, 15, 16, 14, 1, 6, 11, 4, 10, 5, 13, 3, 8, 2],\n            ls=list(\"qhjfdxtpzgemryoikwvblcaus\"),\n            ldc=[SubConfig(1, \"hello\"), SubConfig(2, \"world\")],\n            tf=(1, 4, 2, 10, 5, 9, 13, 16, 15, 8, 12, 7, 11, 14, 3, 6),\n            ts=(\"hello\", \"world\"),\n            t=(\"foo\", 42, SubConfig(1, \"bar\")),\n            tdc=(SubConfig(1, \"hello\"), SubConfig(2, \"world\")),\n            dsi={\"a\": 1, \"b\": 2, \"c\": 3},\n            dss={\"d\": \"e\", \"f\": \"g\"},\n            dis={1: \"a\", 2: \"b\", 3: \"c\"},\n            dsdis={\"a\": {1: \"hello\", 2: \"world\"}, \"b\": {1: \"world\"}},\n            dc=SubConfig(1, \"hello\"),\n            dco=None,\n            ddc={\"a\": SubConfig(1, \"hello\"), \"b\": SubConfig(2, \"world\")},\n        ))\n\n    buffer = io.BytesIO()\n    checkpoint.dump(buffer, ckpt)\n    buffer.seek(0)\n    ckpt2 = checkpoint.load(buffer, Checkpoint)\n    np.testing.assert_array_equal(ckpt.params[\"layer1\"][\"w\"],\n                                  ckpt2.params[\"layer1\"][\"w\"])\n    np.testing.assert_array_equal(ckpt.params[\"layer1\"][\"b\"],\n                                  ckpt2.params[\"layer1\"][\"b\"])\n    np.testing.assert_array_equal(ckpt.params[\"layer2\"][\"w\"],\n                                  ckpt2.params[\"layer2\"][\"w\"])\n    np.testing.assert_array_equal(ckpt.params[\"layer2\"][\"b\"],\n                                  ckpt2.params[\"layer2\"][\"b\"])\n    np.testing.assert_array_equal(ckpt.params[\"blah\"], ckpt2.params[\"blah\"])\n    self.assertEqual(ckpt.config, ckpt2.config)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "15": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Wrappers for Predictors which allow them to work with normalized data.\n\nThe Predictor which is wrapped sees normalized inputs and targets, and makes\nnormalized predictions. The wrapper handles translating the predictions back\nto the original domain.\n\"\"\"\n\nimport logging\nfrom typing import Optional, Tuple\n\nfrom graphcast import predictor_base\nfrom graphcast import xarray_tree\nimport xarray\n\n\ndef normalize(values: xarray.Dataset,\n              scales: xarray.Dataset,\n              locations: Optional[xarray.Dataset],\n              ) -> xarray.Dataset:\n  \"\"\"Normalize variables using the given scales and (optionally) locations.\"\"\"\n  def normalize_array(array):\n    if array.name is None:\n      raise ValueError(\n          \"Can't look up normalization constants because array has no name.\")\n    if locations is not None:\n      if array.name in locations:\n        array = array - locations[array.name].astype(array.dtype)\n      else:\n        logging.warning('No normalization location found for %s', array.name)\n    if array.name in scales:\n      array = array / scales[array.name].astype(array.dtype)\n    else:\n      logging.warning('No normalization scale found for %s', array.name)\n    return array\n  return xarray_tree.map_structure(normalize_array, values)\n\n\ndef unnormalize(values: xarray.Dataset,\n                scales: xarray.Dataset,\n                locations: Optional[xarray.Dataset],\n                ) -> xarray.Dataset:\n  \"\"\"Unnormalize variables using the given scales and (optionally) locations.\"\"\"\n  def unnormalize_array(array):\n    if array.name is None:\n      raise ValueError(\n          \"Can't look up normalization constants because array has no name.\")\n    if array.name in scales:\n      array = array * scales[array.name].astype(array.dtype)\n    else:\n      logging.warning('No normalization scale found for %s', array.name)\n    if locations is not None:\n      if array.name in locations:\n        array = array + locations[array.name].astype(array.dtype)\n      else:\n        logging.warning('No normalization location found for %s', array.name)\n    return array\n  return xarray_tree.map_structure(unnormalize_array, values)\n\n\nclass InputsAndResiduals(predictor_base.Predictor):\n  \"\"\"Wraps with a residual connection, normalizing inputs and target residuals.\n\n  The inner predictor is given inputs that are normalized using `locations`\n  and `scales` to roughly zero-mean unit variance.\n\n  For target variables that are present in the inputs, the inner predictor is\n  trained to predict residuals (target - last_frame_of_input) that have been\n  normalized using `residual_scales` (and optionally `residual_locations`) to\n  roughly unit variance / zero mean.\n\n  This replaces `residual.Predictor` in the case where you want normalization\n  that's based on the scales of the residuals.\n\n  Since we return the underlying predictor's loss on the normalized residuals,\n  if the underlying predictor is a sum of per-variable losses, the normalization\n  will affect the relative weighting of the per-variable loss terms (hopefully\n  in a good way).\n\n  For target variables *not* present in the inputs, the inner predictor is\n  trained to predict targets directly, that have been normalized in the same\n  way as the inputs.\n\n  The transforms applied to the targets (the residual connection and the\n  normalization) are applied in reverse to the predictions before returning\n  them.\n  \"\"\"\n\n  def __init__(\n      self,\n      predictor: predictor_base.Predictor,\n      stddev_by_level: xarray.Dataset,\n      mean_by_level: xarray.Dataset,\n      diffs_stddev_by_level: xarray.Dataset):\n    self._predictor = predictor\n    self._scales = stddev_by_level\n    self._locations = mean_by_level\n    self._residual_scales = diffs_stddev_by_level\n    self._residual_locations = None\n\n  def _unnormalize_prediction_and_add_input(self, inputs, norm_prediction):\n    if norm_prediction.sizes.get('time') != 1:\n      raise ValueError(\n          'normalization.InputsAndResiduals only supports predicting a '\n          'single timestep.')\n    if norm_prediction.name in inputs:\n      # Residuals are assumed to be predicted as normalized (unit variance),\n      # but the scale and location they need mapping to is that of the residuals\n      # not of the values themselves.\n      prediction = unnormalize(\n          norm_prediction, self._residual_scales, self._residual_locations)\n      # A prediction for which we have a corresponding input -- we are\n      # predicting the residual:\n      last_input = inputs[norm_prediction.name].isel(time=-1)\n      prediction = prediction + last_input\n      return prediction\n    else:\n      # A predicted variable which is not an input variable. We are predicting\n      # it directly, so unnormalize it directly to the target scale/location:\n      return unnormalize(norm_prediction, self._scales, self._locations)\n\n  def _subtract_input_and_normalize_target(self, inputs, target):\n    if target.sizes.get('time') != 1:\n      raise ValueError(\n          'normalization.InputsAndResiduals only supports wrapping predictors'\n          'that predict a single timestep.')\n    if target.name in inputs:\n      target_residual = target\n      last_input = inputs[target.name].isel(time=-1)\n      target_residual = target_residual - last_input\n      return normalize(\n          target_residual, self._residual_scales, self._residual_locations)\n    else:\n      return normalize(target, self._scales, self._locations)\n\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               **kwargs\n               ) -> xarray.Dataset:\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_predictions = self._predictor(\n        norm_inputs, targets_template, forcings=norm_forcings, **kwargs)\n    return xarray_tree.map_structure(\n        lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred),\n        norm_predictions)\n\n  def loss(self,\n           inputs: xarray.Dataset,\n           targets: xarray.Dataset,\n           forcings: xarray.Dataset,\n           **kwargs,\n           ) -> predictor_base.LossAndDiagnostics:\n    \"\"\"Returns the loss computed on normalized inputs and targets.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(\n        lambda t: self._subtract_input_and_normalize_target(inputs, t),\n        targets)\n    return self._predictor.loss(\n        norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n\n  def loss_and_predictions(  # pytype: disable=signature-mismatch  # jax-ndarray\n      self,\n      inputs: xarray.Dataset,\n      targets: xarray.Dataset,\n      forcings: xarray.Dataset,\n      **kwargs,\n      ) -> Tuple[predictor_base.LossAndDiagnostics,\n                 xarray.Dataset]:\n    \"\"\"The loss computed on normalized data, with unnormalized predictions.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(\n        lambda t: self._subtract_input_and_normalize_target(inputs, t),\n        targets)\n    (loss, scalars), norm_predictions = self._predictor.loss_and_predictions(\n        norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n    predictions = xarray_tree.map_structure(\n        lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred),\n        norm_predictions)\n    return (loss, scalars), predictions\n",
    "27": "def _get_flat_arrays_and_single_timestep_treedef(variables):\n    flat_arrays = jax.tree_util.tree_leaves(variables.transpose('time', ...))\n    (_, treedef) = jax.tree_util.tree_flatten(variables.isel(time=0, drop=True))\n    return (flat_arrays, treedef)",
    "29": "def __init__(self, predictor: predictor_base.Predictor, noise_level: Optional[float]=None, gradient_checkpointing: bool=False):\n    \"\"\"Initializes an autoregressive predictor wrapper.\n\n    Args:\n      predictor: A predictor to wrap in an auto-regressive way.\n      noise_level: Optional value that multiplies the standard normal noise\n        added to the time-dependent variables of the predictor inputs. In\n        particular, no noise is added to the predictions that are fed back\n        auto-regressively. Defaults to not adding noise.\n      gradient_checkpointing: If True, gradient checkpointing will be\n        used at each step of the computation to save on memory. Roughtly this\n        should make the backwards pass two times more expensive, and the time\n        per step counting the forward pass, should only increase by about 50%.\n        Note this parameter will be ignored with a warning if the scan sequence\n        length is 1.\n    \"\"\"\n    self._predictor = predictor\n    self._noise_level = noise_level\n    self._gradient_checkpointing = gradient_checkpointing",
    "30": "def _get_and_validate_constant_inputs(self, inputs, targets, forcings):\n    constant_inputs = inputs.drop_vars(targets.keys(), errors='ignore')\n    constant_inputs = constant_inputs.drop_vars(forcings.keys(), errors='ignore')\n    for (name, var) in constant_inputs.items():\n        if 'time' in var.dims:\n            raise ValueError(f'Time-dependent input variable {name} must either be a forcing variable, or a target variable to allow for auto-regressive feedback.')\n    return constant_inputs",
    "31": "def _validate_targets_and_forcings(self, targets, forcings):\n    for (name, var) in targets.items():\n        if 'time' not in var.dims:\n            raise ValueError(f'Target variable {name} must be time-dependent.')\n    for (name, var) in forcings.items():\n        if 'time' not in var.dims:\n            raise ValueError(f'Forcing variable {name} must be time-dependent.')\n    overlap = forcings.keys() & targets.keys()\n    if overlap:\n        raise ValueError(f\"The following were specified as both targets and forcings, which isn't allowed: {overlap}\")",
    "34": "def one_step_prediction(inputs, scan_variables):\n    flat_forcings = scan_variables\n    forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, target_template.coords['time'])\n    all_inputs = xarray.merge([constant_inputs, inputs])\n    predictions: xarray.Dataset = self._predictor(all_inputs, target_template, forcings=forcings, **kwargs)\n    next_frame = xarray.merge([predictions, forcings])\n    next_inputs = self._update_inputs(inputs, next_frame)\n    predictions = predictions.squeeze('time', drop=True)\n    flat_pred = jax.tree_util.tree_leaves(predictions)\n    return (next_inputs, flat_pred)",
    "37": "def one_step_loss(inputs, scan_variables):\n    (flat_target, flat_forcings) = scan_variables\n    forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, targets.coords['time'][:1])\n    target = _unflatten_and_expand_time(flat_target, target_treedef, targets.coords['time'][:1])\n    all_inputs = xarray.merge([constant_inputs, inputs])\n    ((loss, diagnostics), predictions) = self._predictor.loss_and_predictions(all_inputs, target, forcings=forcings, **kwargs)\n    (loss, diagnostics) = xarray_tree.map_structure(xarray_jax.unwrap_data, (loss, diagnostics))\n    predictions = cast(xarray.Dataset, predictions)\n    next_frame = xarray.merge([predictions, forcings])\n    next_inputs = self._update_inputs(inputs, next_frame)\n    return (next_inputs, (loss, diagnostics))",
    "38": "class Bfloat16Cast(predictor_base.Predictor):\n    \"\"\"Wrapper that casts all inputs to bfloat16 and outputs to targets dtype.\"\"\"\n\n    def __init__(self, predictor: predictor_base.Predictor, enabled: bool=True):\n        \"\"\"Inits the wrapper.\n\n    Args:\n      predictor: predictor being wrapped.\n      enabled: disables the wrapper if False, for simpler hyperparameter scans.\n\n    \"\"\"\n        self._enabled = enabled\n        self._predictor = predictor\n\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n        if not self._enabled:\n            return self._predictor(inputs, targets_template, forcings, **kwargs)\n        with bfloat16_variable_view():\n            predictions = self._predictor(*_all_inputs_to_bfloat16(inputs, targets_template, forcings), **kwargs)\n        predictions_dtype = infer_floating_dtype(predictions)\n        if predictions_dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n        targets_dtype = infer_floating_dtype(targets_template)\n        return tree_map_cast(predictions, input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n        if not self._enabled:\n            return self._predictor.loss(inputs, targets, forcings, **kwargs)\n        with bfloat16_variable_view():\n            (loss, scalars) = self._predictor.loss(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n        if loss.dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n        targets_dtype = infer_floating_dtype(targets)\n        return tree_map_cast((loss, scalars), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n    def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n        if not self._enabled:\n            return self._predictor.loss_and_predictions(inputs, targets, forcings, **kwargs)\n        with bfloat16_variable_view():\n            ((loss, scalars), predictions) = self._predictor.loss_and_predictions(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n        if loss.dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n        predictions_dtype = infer_floating_dtype(predictions)\n        if predictions_dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n        targets_dtype = infer_floating_dtype(targets)\n        return tree_map_cast(((loss, scalars), predictions), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "39": "def __init__(self, predictor: predictor_base.Predictor, enabled: bool=True):\n    \"\"\"Inits the wrapper.\n\n    Args:\n      predictor: predictor being wrapped.\n      enabled: disables the wrapper if False, for simpler hyperparameter scans.\n\n    \"\"\"\n    self._enabled = enabled\n    self._predictor = predictor",
    "40": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n    if not self._enabled:\n        return self._predictor(inputs, targets_template, forcings, **kwargs)\n    with bfloat16_variable_view():\n        predictions = self._predictor(*_all_inputs_to_bfloat16(inputs, targets_template, forcings), **kwargs)\n    predictions_dtype = infer_floating_dtype(predictions)\n    if predictions_dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n    targets_dtype = infer_floating_dtype(targets_template)\n    return tree_map_cast(predictions, input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "41": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n    if not self._enabled:\n        return self._predictor.loss(inputs, targets, forcings, **kwargs)\n    with bfloat16_variable_view():\n        (loss, scalars) = self._predictor.loss(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n    if loss.dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n    targets_dtype = infer_floating_dtype(targets)\n    return tree_map_cast((loss, scalars), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "42": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    if not self._enabled:\n        return self._predictor.loss_and_predictions(inputs, targets, forcings, **kwargs)\n    with bfloat16_variable_view():\n        ((loss, scalars), predictions) = self._predictor.loss_and_predictions(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n    if loss.dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n    predictions_dtype = infer_floating_dtype(predictions)\n    if predictions_dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n    targets_dtype = infer_floating_dtype(targets)\n    return tree_map_cast(((loss, scalars), predictions), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "43": "def infer_floating_dtype(data_vars: Mapping[str, chex.Array]) -> np.dtype:\n    \"\"\"Infers a floating dtype from an input mapping of data.\"\"\"\n    dtypes = {v.dtype for (k, v) in data_vars.items() if jnp.issubdtype(v.dtype, np.floating)}\n    if len(dtypes) != 1:\n        dtypes_and_shapes = {k: (v.dtype, v.shape) for (k, v) in data_vars.items() if jnp.issubdtype(v.dtype, np.floating)}\n        raise ValueError(f'Did not found exactly one floating dtype {dtypes} in input variables:{dtypes_and_shapes}')\n    return list(dtypes)[0]",
    "45": "def tree_map_cast(inputs: PyTree, input_dtype: np.dtype, output_dtype: np.dtype) -> PyTree:\n\n    def cast_fn(x):\n        if x.dtype == input_dtype:\n            return x.astype(output_dtype)\n    return jax.tree_map(cast_fn, inputs)",
    "47": "@contextlib.contextmanager\ndef bfloat16_variable_view(enabled: bool=True):\n    \"\"\"Context for Haiku modules with float32 params, but bfloat16 activations.\n\n  It works as follows:\n  * Every time a variable is requested to be created/set as np.bfloat16,\n    it will create an underlying float32 variable, instead.\n  * Every time a variable a variable is requested as bfloat16, it will check the\n    variable is of float32 type, and cast the variable to bfloat16.\n\n  Note the gradients are still computed and accumulated as float32, because\n  the params returned by init are float32, so the gradient function with\n  respect to the params will already include an implicit casting to float32.\n\n  Args:\n    enabled: Only enables bfloat16 behavior if True.\n\n  Yields:\n    None\n  \"\"\"\n    if enabled:\n        with hk.custom_creator(_bfloat16_creator, state=True), hk.custom_getter(_bfloat16_getter, state=True), hk.custom_setter(_bfloat16_setter):\n            yield\n    else:\n        yield",
    "49": "def _bfloat16_getter(next_getter, value, context):\n    \"\"\"Casts float32 to bfloat16 when bfloat16 was originally requested.\"\"\"\n    if context.original_dtype == jnp.bfloat16:\n        assert value.dtype == jnp.float32\n        value = value.astype(jnp.bfloat16)\n    return next_getter(value)",
    "51": "def dump(dest: BinaryIO, value: Any) -> None:\n    \"\"\"Dump a tree of dicts/dataclasses to a file object.\n\n  Args:\n    dest: a file object to write to.\n    value: A tree of dicts, lists, tuples and dataclasses of numpy arrays and\n      other basic types. Unions are not supported, other than Optional/None\n      which is only supported in dataclasses, not in dicts, lists or tuples.\n      All leaves must be coercible to a numpy array, and recoverable as a single\n      arg to a type.\n  \"\"\"\n    buffer = io.BytesIO()\n    np.savez(buffer, **_flatten(value))\n    dest.write(buffer.getvalue())",
    "52": "def load(source: BinaryIO, typ: type[_T]) -> _T:\n    \"\"\"Load from a file object and convert it to the specified type.\n\n  Args:\n    source: a file object to read from.\n    typ: a type object that acts as a schema for deserialization. It must match\n      what was serialized. If a type is Any, it will be returned however numpy\n      serialized it, which is what you want for a tree of numpy arrays.\n\n  Returns:\n    the deserialized value as the specified type.\n  \"\"\"\n    return _convert_types(typ, _unflatten(np.load(source)))",
    "53": "def _flatten(tree: Any) -> dict[str, Any]:\n    \"\"\"Flatten a tree of dicts/dataclasses/lists/tuples to a single dict.\"\"\"\n    if dataclasses.is_dataclass(tree):\n        tree = {f.name: v for f in dataclasses.fields(tree) if (v := getattr(tree, f.name)) is not None}\n    elif isinstance(tree, (list, tuple)):\n        tree = dict(enumerate(tree))\n    assert isinstance(tree, dict)\n    flat = {}\n    for (k, v) in tree.items():\n        k = str(k)\n        assert _SEP not in k\n        if dataclasses.is_dataclass(v) or isinstance(v, (dict, list, tuple)):\n            for (a, b) in _flatten(v).items():\n                flat[f'{k}{_SEP}{a}'] = b\n        else:\n            assert v is not None\n            flat[k] = v\n    return flat",
    "54": "def _unflatten(flat: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Unflatten a dict to a tree of dicts.\"\"\"\n    tree = {}\n    for (flat_key, v) in flat.items():\n        node = tree\n        keys = flat_key.split(_SEP)\n        for k in keys[:-1]:\n            if k not in node:\n                node[k] = {}\n            node = node[k]\n        node[keys[-1]] = v\n    return tree",
    "57": "@dataclasses.dataclass\nclass Config:\n    bt: bool\n    bf: bool\n    i: int\n    f: float\n    o1: Optional[int]\n    o2: Optional[int]\n    o3: Union[int, None]\n    o4: Union[int, None]\n    o5: int | None\n    o6: int | None\n    li: list[int]\n    ls: list[str]\n    ldc: list[SubConfig]\n    tf: tuple[float, ...]\n    ts: tuple[str, ...]\n    t: tuple[str, int, SubConfig]\n    tdc: tuple[SubConfig, ...]\n    dsi: dict[str, int]\n    dss: dict[str, str]\n    dis: dict[int, str]\n    dsdis: dict[str, dict[int, str]]\n    dc: SubConfig\n    dco: Optional[SubConfig]\n    ddc: dict[str, SubConfig]",
    "59": "class DataclassTest(absltest.TestCase):\n\n    def test_serialize_dataclass(self):\n        ckpt = Checkpoint(params={'layer1': {'w': np.arange(10).reshape(2, 5), 'b': np.array([2, 6])}, 'layer2': {'w': np.arange(8).reshape(2, 4), 'b': np.array([2, 6])}, 'blah': np.array([3, 9])}, config=Config(bt=True, bf=False, i=42, f=3.14, o1=1, o2=None, o3=2, o4=None, o5=3, o6=None, li=[12, 9, 7, 15, 16, 14, 1, 6, 11, 4, 10, 5, 13, 3, 8, 2], ls=list('qhjfdxtpzgemryoikwvblcaus'), ldc=[SubConfig(1, 'hello'), SubConfig(2, 'world')], tf=(1, 4, 2, 10, 5, 9, 13, 16, 15, 8, 12, 7, 11, 14, 3, 6), ts=('hello', 'world'), t=('foo', 42, SubConfig(1, 'bar')), tdc=(SubConfig(1, 'hello'), SubConfig(2, 'world')), dsi={'a': 1, 'b': 2, 'c': 3}, dss={'d': 'e', 'f': 'g'}, dis={1: 'a', 2: 'b', 3: 'c'}, dsdis={'a': {1: 'hello', 2: 'world'}, 'b': {1: 'world'}}, dc=SubConfig(1, 'hello'), dco=None, ddc={'a': SubConfig(1, 'hello'), 'b': SubConfig(2, 'world')}))\n        buffer = io.BytesIO()\n        checkpoint.dump(buffer, ckpt)\n        buffer.seek(0)\n        ckpt2 = checkpoint.load(buffer, Checkpoint)\n        np.testing.assert_array_equal(ckpt.params['layer1']['w'], ckpt2.params['layer1']['w'])\n        np.testing.assert_array_equal(ckpt.params['layer1']['b'], ckpt2.params['layer1']['b'])\n        np.testing.assert_array_equal(ckpt.params['layer2']['w'], ckpt2.params['layer2']['w'])\n        np.testing.assert_array_equal(ckpt.params['layer2']['b'], ckpt2.params['layer2']['b'])\n        np.testing.assert_array_equal(ckpt.params['blah'], ckpt2.params['blah'])\n        self.assertEqual(ckpt.config, ckpt2.config)",
    "60": "def test_serialize_dataclass(self):\n    ckpt = Checkpoint(params={'layer1': {'w': np.arange(10).reshape(2, 5), 'b': np.array([2, 6])}, 'layer2': {'w': np.arange(8).reshape(2, 4), 'b': np.array([2, 6])}, 'blah': np.array([3, 9])}, config=Config(bt=True, bf=False, i=42, f=3.14, o1=1, o2=None, o3=2, o4=None, o5=3, o6=None, li=[12, 9, 7, 15, 16, 14, 1, 6, 11, 4, 10, 5, 13, 3, 8, 2], ls=list('qhjfdxtpzgemryoikwvblcaus'), ldc=[SubConfig(1, 'hello'), SubConfig(2, 'world')], tf=(1, 4, 2, 10, 5, 9, 13, 16, 15, 8, 12, 7, 11, 14, 3, 6), ts=('hello', 'world'), t=('foo', 42, SubConfig(1, 'bar')), tdc=(SubConfig(1, 'hello'), SubConfig(2, 'world')), dsi={'a': 1, 'b': 2, 'c': 3}, dss={'d': 'e', 'f': 'g'}, dis={1: 'a', 2: 'b', 3: 'c'}, dsdis={'a': {1: 'hello', 2: 'world'}, 'b': {1: 'world'}}, dc=SubConfig(1, 'hello'), dco=None, ddc={'a': SubConfig(1, 'hello'), 'b': SubConfig(2, 'world')}))\n    buffer = io.BytesIO()\n    checkpoint.dump(buffer, ckpt)\n    buffer.seek(0)\n    ckpt2 = checkpoint.load(buffer, Checkpoint)\n    np.testing.assert_array_equal(ckpt.params['layer1']['w'], ckpt2.params['layer1']['w'])\n    np.testing.assert_array_equal(ckpt.params['layer1']['b'], ckpt2.params['layer1']['b'])\n    np.testing.assert_array_equal(ckpt.params['layer2']['w'], ckpt2.params['layer2']['w'])\n    np.testing.assert_array_equal(ckpt.params['layer2']['b'], ckpt2.params['layer2']['b'])\n    np.testing.assert_array_equal(ckpt.params['blah'], ckpt2.params['blah'])\n    self.assertEqual(ckpt.config, ckpt2.config)",
    "61": "def get_year_progress(seconds_since_epoch: np.ndarray) -> np.ndarray:\n    \"\"\"Computes year progress for times in seconds.\n\n  Args:\n    seconds_since_epoch: Times in seconds since the \"epoch\" (the point at which\n      UNIX time starts).\n\n  Returns:\n    Year progress normalized to be in the [0, 1) interval for each time point.\n  \"\"\"\n    years_since_epoch = seconds_since_epoch / SEC_PER_DAY / np.float64(_AVG_DAY_PER_YEAR)\n    return np.mod(years_since_epoch, 1.0).astype(np.float32)",
    "62": "def get_day_progress(seconds_since_epoch: np.ndarray, longitude: np.ndarray) -> np.ndarray:\n    \"\"\"Computes day progress for times in seconds at each longitude.\n\n  Args:\n    seconds_since_epoch: 1D array of times in seconds since the 'epoch' (the\n      point at which UNIX time starts).\n    longitude: 1D array of longitudes at which day progress is computed.\n\n  Returns:\n    2D array of day progress values normalized to be in the [0, 1) inverval\n      for each time point at each longitude.\n  \"\"\"\n    day_progress_greenwich = np.mod(seconds_since_epoch, SEC_PER_DAY) / SEC_PER_DAY\n    longitude_offsets = np.deg2rad(longitude) / (2 * np.pi)\n    day_progress = np.mod(day_progress_greenwich[..., np.newaxis] + longitude_offsets, 1.0)\n    return day_progress.astype(np.float32)",
    "63": "def featurize_progress(name: str, dims: Sequence[str], progress: np.ndarray) -> Mapping[str, xarray.Variable]:\n    \"\"\"Derives features used by ML models from the `progress` variable.\n\n  Args:\n    name: Base variable name from which features are derived.\n    dims: List of the output feature dimensions, e.g. (\"day\", \"lon\").\n    progress: Progress variable values.\n\n  Returns:\n    Dictionary of xarray variables derived from the `progress` values. It\n    includes the original `progress` variable along with its sin and cos\n    transformations.\n\n  Raises:\n    ValueError if the number of feature dimensions is not equal to the number\n      of data dimensions.\n  \"\"\"\n    if len(dims) != progress.ndim:\n        raise ValueError(f'Number of feature dimensions ({len(dims)}) must be equal to the number of data dimensions: {progress.ndim}.')\n    progress_phase = progress * (2 * np.pi)\n    return {name: xarray.Variable(dims, progress), name + '_sin': xarray.Variable(dims, np.sin(progress_phase)), name + '_cos': xarray.Variable(dims, np.cos(progress_phase))}",
    "64": "def add_derived_vars(data: xarray.Dataset) -> None:\n    \"\"\"Adds year and day progress features to `data` in place if missing.\n\n  Args:\n    data: Xarray dataset to which derived features will be added.\n\n  Raises:\n    ValueError if `datetime` or `lon` are not in `data` coordinates.\n  \"\"\"\n    for coord in ('datetime', 'lon'):\n        if coord not in data.coords:\n            raise ValueError(f\"'{coord}' must be in `data` coordinates.\")\n    seconds_since_epoch = data.coords['datetime'].data.astype('datetime64[s]').astype(np.int64)\n    batch_dim = ('batch',) if 'batch' in data.dims else ()\n    if YEAR_PROGRESS not in data.data_vars:\n        year_progress = get_year_progress(seconds_since_epoch)\n        data.update(featurize_progress(name=YEAR_PROGRESS, dims=batch_dim + ('time',), progress=year_progress))\n    if DAY_PROGRESS not in data.data_vars:\n        longitude_coord = data.coords['lon']\n        day_progress = get_day_progress(seconds_since_epoch, longitude_coord.data)\n        data.update(featurize_progress(name=DAY_PROGRESS, dims=batch_dim + ('time',) + longitude_coord.dims, progress=day_progress))",
    "65": "def add_tisr_var(data: xarray.Dataset) -> None:\n    \"\"\"Adds TISR feature to `data` in place if missing.\n\n  Args:\n    data: Xarray dataset to which TISR feature will be added.\n\n  Raises:\n    ValueError if `datetime`, 'lat', or `lon` are not in `data` coordinates.\n  \"\"\"\n    if TISR in data.data_vars:\n        return\n    for coord in ('datetime', 'lat', 'lon'):\n        if coord not in data.coords:\n            raise ValueError(f\"'{coord}' must be in `data` coordinates.\")\n    data_no_batch = data.squeeze('batch') if 'batch' in data.dims else data\n    tisr = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data_no_batch, use_jit=True)\n    if 'batch' in data.dims:\n        tisr = tisr.expand_dims('batch', axis=0)\n    data.update({TISR: tisr})",
    "67": "def _process_target_lead_times_and_get_duration(target_lead_times: TargetLeadTimes) -> TimedeltaLike:\n    \"\"\"Returns the minimum duration for the target lead times.\"\"\"\n    if isinstance(target_lead_times, slice):\n        if target_lead_times.start is None:\n            target_lead_times = slice(pd.Timedelta(1, 'ns'), target_lead_times.stop, target_lead_times.step)\n        target_duration = pd.Timedelta(target_lead_times.stop)\n    else:\n        if not isinstance(target_lead_times, (list, tuple, set)):\n            target_lead_times = [target_lead_times]\n        target_lead_times = [pd.Timedelta(x) for x in target_lead_times]\n        target_lead_times.sort()\n        target_duration = target_lead_times[-1]\n    return (target_lead_times, target_duration)",
    "68": "def extract_inputs_targets_forcings(dataset: xarray.Dataset, *, input_variables: Tuple[str, ...], target_variables: Tuple[str, ...], forcing_variables: Tuple[str, ...], pressure_levels: Tuple[int, ...], input_duration: TimedeltaLike, target_lead_times: TargetLeadTimes) -> Tuple[xarray.Dataset, xarray.Dataset, xarray.Dataset]:\n    \"\"\"Extracts inputs, targets and forcings according to requirements.\"\"\"\n    dataset = dataset.sel(level=list(pressure_levels))\n    if set(forcing_variables) & _DERIVED_VARS:\n        add_derived_vars(dataset)\n    if set(forcing_variables) & {TISR}:\n        add_tisr_var(dataset)\n    dataset = dataset.drop_vars('datetime')\n    (inputs, targets) = extract_input_target_times(dataset, input_duration=input_duration, target_lead_times=target_lead_times)\n    if set(forcing_variables) & set(target_variables):\n        raise ValueError(f'Forcing variables {forcing_variables} should not overlap with target variables {target_variables}.')\n    inputs = inputs[list(input_variables)]\n    forcings = targets[list(forcing_variables)]\n    targets = targets[list(target_variables)]\n    return (inputs, targets, forcings)",
    "69": "class DataUtilsTest(parameterized.TestCase):\n\n    def setUp(self):\n        super().setUp()\n        np.random.seed(0)\n\n    def test_year_progress_is_zero_at_year_start_or_end(self):\n        year_progress = data_utils.get_year_progress(np.array([0, data_utils.AVG_SEC_PER_YEAR, data_utils.AVG_SEC_PER_YEAR * 42]))\n        np.testing.assert_array_equal(year_progress, np.zeros(year_progress.shape))\n\n    def test_year_progress_is_almost_one_before_year_ends(self):\n        year_progress = data_utils.get_year_progress(np.array([data_utils.AVG_SEC_PER_YEAR - 1, (data_utils.AVG_SEC_PER_YEAR - 1) * 42]))\n        with self.subTest('Year progress values are close to 1'):\n            self.assertTrue(np.all(year_progress > 0.999))\n        with self.subTest('Year progress values != 1'):\n            self.assertTrue(np.all(year_progress < 1.0))\n\n    def test_day_progress_computes_for_all_times_and_longitudes(self):\n        times = np.random.randint(low=0, high=10000000000.0, size=10)\n        longitudes = np.arange(0, 360.0, 1.0)\n        day_progress = data_utils.get_day_progress(times, longitudes)\n        with self.subTest('Day progress is computed for all times and longinutes'):\n            self.assertSequenceEqual(day_progress.shape, (len(times), len(longitudes)))\n\n    @parameterized.named_parameters(dict(testcase_name='random_date_1', year=1988, month=11, day=7, hour=2, minute=45, second=34), dict(testcase_name='random_date_2', year=2022, month=3, day=12, hour=7, minute=1, second=0))\n    def test_day_progress_is_in_between_zero_and_one(self, year, month, day, hour, minute, second):\n        dt = datetime.datetime(year, month, day, hour, minute, second)\n        epoch_time = datetime.datetime(1970, 1, 1)\n        seconds_since_epoch = np.array([(dt - epoch_time).total_seconds()])\n        longitudes = np.arange(0, 360.0, 1.0)\n        day_progress = data_utils.get_day_progress(seconds_since_epoch, longitudes)\n        with self.subTest('Day progress >= 0'):\n            self.assertTrue(np.all(day_progress >= 0.0))\n        with self.subTest('Day progress < 1'):\n            self.assertTrue(np.all(day_progress < 1.0))\n\n    def test_day_progress_is_zero_at_day_start_or_end(self):\n        day_progress = data_utils.get_day_progress(seconds_since_epoch=np.array([0, data_utils.SEC_PER_DAY, data_utils.SEC_PER_DAY * 42]), longitude=np.array([0.0]))\n        np.testing.assert_array_equal(day_progress, np.zeros(day_progress.shape))\n\n    def test_day_progress_specific_value(self):\n        day_progress = data_utils.get_day_progress(seconds_since_epoch=np.array([123]), longitude=np.array([0.0]))\n        np.testing.assert_array_almost_equal(day_progress, np.array([[0.00142361]]), decimal=6)\n\n    def test_featurize_progress_valid_values_and_dimensions(self):\n        day_progress = np.array([0.0, 0.45, 0.213])\n        feature_dimensions = ('time',)\n        progress_features = data_utils.featurize_progress(name='day_progress', dims=feature_dimensions, progress=day_progress)\n        for feature in progress_features.values():\n            with self.subTest(f'Valid dimensions for {feature}'):\n                self.assertSequenceEqual(feature.dims, feature_dimensions)\n        with self.subTest('Valid values for day_progress'):\n            np.testing.assert_array_equal(day_progress, progress_features['day_progress'].values)\n        with self.subTest('Valid values for day_progress_sin'):\n            np.testing.assert_array_almost_equal(np.array([0.0, 0.30901699, 0.97309851]), progress_features['day_progress_sin'].values, decimal=6)\n        with self.subTest('Valid values for day_progress_cos'):\n            np.testing.assert_array_almost_equal(np.array([1.0, -0.95105652, 0.23038943]), progress_features['day_progress_cos'].values, decimal=6)\n\n    def test_featurize_progress_invalid_dimensions(self):\n        year_progress = np.array([0.0, 0.45, 0.213])\n        feature_dimensions = ('time', 'longitude')\n        with self.assertRaises(ValueError):\n            data_utils.featurize_progress(name='year_progress', dims=feature_dimensions, progress=year_progress)\n\n    def test_add_derived_vars_variables_added(self):\n        data = xa.Dataset(data_vars={'var1': (['x', 'lon', 'datetime'], 8 * np.random.randn(2, 2, 3))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n        data_utils.add_derived_vars(data)\n        all_variables = set(data.variables)\n        with self.subTest('Original value was not removed'):\n            self.assertIn('var1', all_variables)\n        with self.subTest('Year progress feature was added'):\n            self.assertIn(data_utils.YEAR_PROGRESS, all_variables)\n        with self.subTest('Day progress feature was added'):\n            self.assertIn(data_utils.DAY_PROGRESS, all_variables)\n\n    def test_add_derived_vars_existing_vars_not_overridden(self):\n        dims = ['x', 'lon', 'datetime']\n        data = xa.Dataset(data_vars={'var1': (dims, 8 * np.random.randn(2, 2, 3)), data_utils.YEAR_PROGRESS: (dims, np.full((2, 2, 3), 0.111)), data_utils.DAY_PROGRESS: (dims, np.full((2, 2, 3), 0.222))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n        data_utils.add_derived_vars(data)\n        with self.subTest('Year progress feature was not overridden'):\n            np.testing.assert_allclose(data[data_utils.YEAR_PROGRESS], 0.111)\n        with self.subTest('Day progress feature was not overridden'):\n            np.testing.assert_allclose(data[data_utils.DAY_PROGRESS], 0.222)\n\n    @parameterized.named_parameters(dict(testcase_name='missing_datetime', coord_name='lon'), dict(testcase_name='missing_lon', coord_name='datetime'))\n    def test_add_derived_vars_missing_coordinate_raises_value_error(self, coord_name):\n        with self.subTest(f'Missing {coord_name} coordinate'):\n            data = xa.Dataset(data_vars={'var1': (['x', coord_name], 8 * np.random.randn(2, 2))}, coords={coord_name: np.array([0.0, 0.5])})\n            with self.assertRaises(ValueError):\n                data_utils.add_derived_vars(data)\n\n    def test_add_tisr_var_variable_added(self):\n        data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.full((2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n        data_utils.add_tisr_var(data)\n        self.assertIn(data_utils.TISR, set(data.variables))\n\n    def test_add_tisr_var_existing_var_not_overridden(self):\n        dims = ['time', 'lat', 'lon']\n        data = xa.Dataset(data_vars={'var1': (dims, np.full((2, 2, 2), 8.0)), data_utils.TISR: (dims, np.full((2, 2, 2), 1200.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n        data_utils.add_derived_vars(data)\n        np.testing.assert_allclose(data[data_utils.TISR], 1200.0)\n\n    def test_add_tisr_var_works_with_batch_dim_size_one(self):\n        data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((1, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20]], dtype='datetime64[D]'))})\n        data_utils.add_tisr_var(data)\n        self.assertIn(data_utils.TISR, set(data.variables))\n\n    def test_add_tisr_var_fails_with_batch_dim_size_greater_than_one(self):\n        data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((2, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20], [100, 200]], dtype='datetime64[D]'))})\n        with self.assertRaisesRegex(ValueError, 'cannot select a dimension'):\n            data_utils.add_tisr_var(data)",
    "71": "def test_year_progress_is_zero_at_year_start_or_end(self):\n    year_progress = data_utils.get_year_progress(np.array([0, data_utils.AVG_SEC_PER_YEAR, data_utils.AVG_SEC_PER_YEAR * 42]))\n    np.testing.assert_array_equal(year_progress, np.zeros(year_progress.shape))",
    "72": "def test_year_progress_is_almost_one_before_year_ends(self):\n    year_progress = data_utils.get_year_progress(np.array([data_utils.AVG_SEC_PER_YEAR - 1, (data_utils.AVG_SEC_PER_YEAR - 1) * 42]))\n    with self.subTest('Year progress values are close to 1'):\n        self.assertTrue(np.all(year_progress > 0.999))\n    with self.subTest('Year progress values != 1'):\n        self.assertTrue(np.all(year_progress < 1.0))",
    "73": "def test_day_progress_computes_for_all_times_and_longitudes(self):\n    times = np.random.randint(low=0, high=10000000000.0, size=10)\n    longitudes = np.arange(0, 360.0, 1.0)\n    day_progress = data_utils.get_day_progress(times, longitudes)\n    with self.subTest('Day progress is computed for all times and longinutes'):\n        self.assertSequenceEqual(day_progress.shape, (len(times), len(longitudes)))",
    "74": "@parameterized.named_parameters(dict(testcase_name='random_date_1', year=1988, month=11, day=7, hour=2, minute=45, second=34), dict(testcase_name='random_date_2', year=2022, month=3, day=12, hour=7, minute=1, second=0))\ndef test_day_progress_is_in_between_zero_and_one(self, year, month, day, hour, minute, second):\n    dt = datetime.datetime(year, month, day, hour, minute, second)\n    epoch_time = datetime.datetime(1970, 1, 1)\n    seconds_since_epoch = np.array([(dt - epoch_time).total_seconds()])\n    longitudes = np.arange(0, 360.0, 1.0)\n    day_progress = data_utils.get_day_progress(seconds_since_epoch, longitudes)\n    with self.subTest('Day progress >= 0'):\n        self.assertTrue(np.all(day_progress >= 0.0))\n    with self.subTest('Day progress < 1'):\n        self.assertTrue(np.all(day_progress < 1.0))",
    "76": "def test_day_progress_specific_value(self):\n    day_progress = data_utils.get_day_progress(seconds_since_epoch=np.array([123]), longitude=np.array([0.0]))\n    np.testing.assert_array_almost_equal(day_progress, np.array([[0.00142361]]), decimal=6)",
    "77": "def test_featurize_progress_valid_values_and_dimensions(self):\n    day_progress = np.array([0.0, 0.45, 0.213])\n    feature_dimensions = ('time',)\n    progress_features = data_utils.featurize_progress(name='day_progress', dims=feature_dimensions, progress=day_progress)\n    for feature in progress_features.values():\n        with self.subTest(f'Valid dimensions for {feature}'):\n            self.assertSequenceEqual(feature.dims, feature_dimensions)\n    with self.subTest('Valid values for day_progress'):\n        np.testing.assert_array_equal(day_progress, progress_features['day_progress'].values)\n    with self.subTest('Valid values for day_progress_sin'):\n        np.testing.assert_array_almost_equal(np.array([0.0, 0.30901699, 0.97309851]), progress_features['day_progress_sin'].values, decimal=6)\n    with self.subTest('Valid values for day_progress_cos'):\n        np.testing.assert_array_almost_equal(np.array([1.0, -0.95105652, 0.23038943]), progress_features['day_progress_cos'].values, decimal=6)",
    "79": "def test_add_derived_vars_variables_added(self):\n    data = xa.Dataset(data_vars={'var1': (['x', 'lon', 'datetime'], 8 * np.random.randn(2, 2, 3))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n    data_utils.add_derived_vars(data)\n    all_variables = set(data.variables)\n    with self.subTest('Original value was not removed'):\n        self.assertIn('var1', all_variables)\n    with self.subTest('Year progress feature was added'):\n        self.assertIn(data_utils.YEAR_PROGRESS, all_variables)\n    with self.subTest('Day progress feature was added'):\n        self.assertIn(data_utils.DAY_PROGRESS, all_variables)",
    "80": "def test_add_derived_vars_existing_vars_not_overridden(self):\n    dims = ['x', 'lon', 'datetime']\n    data = xa.Dataset(data_vars={'var1': (dims, 8 * np.random.randn(2, 2, 3)), data_utils.YEAR_PROGRESS: (dims, np.full((2, 2, 3), 0.111)), data_utils.DAY_PROGRESS: (dims, np.full((2, 2, 3), 0.222))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n    data_utils.add_derived_vars(data)\n    with self.subTest('Year progress feature was not overridden'):\n        np.testing.assert_allclose(data[data_utils.YEAR_PROGRESS], 0.111)\n    with self.subTest('Day progress feature was not overridden'):\n        np.testing.assert_allclose(data[data_utils.DAY_PROGRESS], 0.222)",
    "81": "@parameterized.named_parameters(dict(testcase_name='missing_datetime', coord_name='lon'), dict(testcase_name='missing_lon', coord_name='datetime'))\ndef test_add_derived_vars_missing_coordinate_raises_value_error(self, coord_name):\n    with self.subTest(f'Missing {coord_name} coordinate'):\n        data = xa.Dataset(data_vars={'var1': (['x', coord_name], 8 * np.random.randn(2, 2))}, coords={coord_name: np.array([0.0, 0.5])})\n        with self.assertRaises(ValueError):\n            data_utils.add_derived_vars(data)",
    "82": "def test_add_tisr_var_variable_added(self):\n    data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.full((2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n    data_utils.add_tisr_var(data)\n    self.assertIn(data_utils.TISR, set(data.variables))",
    "83": "def test_add_tisr_var_existing_var_not_overridden(self):\n    dims = ['time', 'lat', 'lon']\n    data = xa.Dataset(data_vars={'var1': (dims, np.full((2, 2, 2), 8.0)), data_utils.TISR: (dims, np.full((2, 2, 2), 1200.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n    data_utils.add_derived_vars(data)\n    np.testing.assert_allclose(data[data_utils.TISR], 1200.0)",
    "84": "def test_add_tisr_var_works_with_batch_dim_size_one(self):\n    data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((1, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20]], dtype='datetime64[D]'))})\n    data_utils.add_tisr_var(data)\n    self.assertIn(data_utils.TISR, set(data.variables))",
    "85": "def test_add_tisr_var_fails_with_batch_dim_size_greater_than_one(self):\n    data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((2, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20], [100, 200]], dtype='datetime64[D]'))})\n    with self.assertRaisesRegex(ValueError, 'cannot select a dimension'):\n        data_utils.add_tisr_var(data)",
    "86": "class DeepTypedGraphNet(hk.Module):\n    \"\"\"Deep Graph Neural Network.\n\n  It works with TypedGraphs with typed nodes and edges. It runs message\n  passing on all of the node sets and all of the edge sets in the graph. For\n  each message passing step a `typed_graph_net.InteractionNetwork` is used to\n  update the full TypedGraph by using different MLPs for each of the node sets\n  and each of the edge sets.\n\n  If embed_{nodes,edges} is specified the node/edge features will be embedded\n  into a fixed dimensionality before running the first step of message passing.\n\n  If {node,edge}_output_size the final node/edge features will be embedded into\n  the specified output size.\n\n  This class may be used for shared or unshared message passing:\n  * num_message_passing_steps = N, num_processor_repetitions = 1, gives\n    N layers of message passing with fully unshared weights:\n    [W_1, W_2, ... , W_M] (default)\n  * num_message_passing_steps = 1, num_processor_repetitions = M, gives\n    N layers of message passing with fully shared weights:\n    [W_1] * M\n  * num_message_passing_steps = N, num_processor_repetitions = M, gives\n    M*N layers of message passing with both shared and unshared message passing\n    such that the weights used at each iteration are:\n    [W_1, W_2, ... , W_N] * M\n\n  \"\"\"\n\n    def __init__(self, *, node_latent_size: Mapping[str, int], edge_latent_size: Mapping[str, int], mlp_hidden_size: int, mlp_num_hidden_layers: int, num_message_passing_steps: int, num_processor_repetitions: int=1, embed_nodes: bool=True, embed_edges: bool=True, node_output_size: Optional[Mapping[str, int]]=None, edge_output_size: Optional[Mapping[str, int]]=None, include_sent_messages_in_node_update: bool=False, use_layer_norm: bool=True, activation: str='relu', f32_aggregation: bool=False, aggregate_edges_for_nodes_fn: str='segment_sum', aggregate_normalization: Optional[float]=None, name: str='DeepTypedGraphNet'):\n        \"\"\"Inits the model.\n\n    Args:\n      node_latent_size: Size of the node latent representations.\n      edge_latent_size: Size of the edge latent representations.\n      mlp_hidden_size: Hidden layer size for all MLPs.\n      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n      num_message_passing_steps: Number of unshared message passing steps\n         in the processor steps.\n      num_processor_repetitions: Number of times that the same processor is\n         applied sequencially.\n      embed_nodes: If False, the node embedder will be omitted.\n      embed_edges: If False, the edge embedder will be omitted.\n      node_output_size: Size of the output node representations for\n         each node type. For node types not specified here, the latent node\n         representation from the output of the processor will be returned.\n      edge_output_size: Size of the output edge representations for\n         each edge type. For edge types not specified here, the latent edge\n         representation from the output of the processor will be returned.\n      include_sent_messages_in_node_update: Whether to include pooled sent\n          messages from each node in the node update.\n      use_layer_norm: Whether it uses layer norm or not.\n      activation: name of activation function.\n      f32_aggregation: Use float32 in the edge aggregation.\n      aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n        node.\n      aggregate_normalization: An optional constant that normalizes the output\n        of aggregate_edges_for_nodes_fn. For context, this can be used to\n        reduce the shock the model undergoes when switching resolution, which\n        increase the number of edges connected to a node. In particular, this is\n        useful when using segment_sum, but should not be combined with\n        segment_mean.\n      name: Name of the model.\n    \"\"\"\n        super().__init__(name=name)\n        self._node_latent_size = node_latent_size\n        self._edge_latent_size = edge_latent_size\n        self._mlp_hidden_size = mlp_hidden_size\n        self._mlp_num_hidden_layers = mlp_num_hidden_layers\n        self._num_message_passing_steps = num_message_passing_steps\n        self._num_processor_repetitions = num_processor_repetitions\n        self._embed_nodes = embed_nodes\n        self._embed_edges = embed_edges\n        self._node_output_size = node_output_size\n        self._edge_output_size = edge_output_size\n        self._include_sent_messages_in_node_update = include_sent_messages_in_node_update\n        self._use_layer_norm = use_layer_norm\n        self._activation = _get_activation_fn(activation)\n        self._initialized = False\n        self._f32_aggregation = f32_aggregation\n        self._aggregate_edges_for_nodes_fn = _get_aggregate_edges_for_nodes_fn(aggregate_edges_for_nodes_fn)\n        self._aggregate_normalization = aggregate_normalization\n        if aggregate_normalization:\n            assert aggregate_edges_for_nodes_fn == 'segment_sum'\n\n    def __call__(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n        self._networks_builder(input_graph)\n        latent_graph_0 = self._embed(input_graph)\n        latent_graph_m = self._process(latent_graph_0)\n        return self._output(latent_graph_m)\n\n    def _networks_builder(self, graph_template):\n        if self._initialized:\n            return\n        self._initialized = True\n\n        def build_mlp(name, output_size):\n            mlp = hk.nets.MLP(output_sizes=[self._mlp_hidden_size] * self._mlp_num_hidden_layers + [output_size], name=name + '_mlp', activation=self._activation)\n            return jraph.concatenated_args(mlp)\n\n        def build_mlp_with_maybe_layer_norm(name, output_size):\n            network = build_mlp(name, output_size)\n            if self._use_layer_norm:\n                layer_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True, name=name + '_layer_norm')\n                network = hk.Sequential([network, layer_norm])\n            return jraph.concatenated_args(network)\n        if self._embed_edges:\n            embed_edge_fn = _build_update_fns_for_edge_types(build_mlp_with_maybe_layer_norm, graph_template, 'encoder_edges_', output_sizes=self._edge_latent_size)\n        else:\n            embed_edge_fn = None\n        if self._embed_nodes:\n            embed_node_fn = _build_update_fns_for_node_types(build_mlp_with_maybe_layer_norm, graph_template, 'encoder_nodes_', output_sizes=self._node_latent_size)\n        else:\n            embed_node_fn = None\n        embedder_kwargs = dict(embed_edge_fn=embed_edge_fn, embed_node_fn=embed_node_fn)\n        self._embedder_network = typed_graph_net.GraphMapFeatures(**embedder_kwargs)\n        if self._f32_aggregation:\n\n            def aggregate_fn(data, *args, **kwargs):\n                dtype = data.dtype\n                data = data.astype(jnp.float32)\n                output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n                if self._aggregate_normalization:\n                    output = output / self._aggregate_normalization\n                output = output.astype(dtype)\n                return output\n        else:\n\n            def aggregate_fn(data, *args, **kwargs):\n                output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n                if self._aggregate_normalization:\n                    output = output / self._aggregate_normalization\n                return output\n        self._processor_networks = []\n        for step_i in range(self._num_message_passing_steps):\n            self._processor_networks.append(typed_graph_net.InteractionNetwork(update_edge_fn=_build_update_fns_for_edge_types(build_mlp_with_maybe_layer_norm, graph_template, f'processor_edges_{step_i}_', output_sizes=self._edge_latent_size), update_node_fn=_build_update_fns_for_node_types(build_mlp_with_maybe_layer_norm, graph_template, f'processor_nodes_{step_i}_', output_sizes=self._node_latent_size), aggregate_edges_for_nodes_fn=aggregate_fn, include_sent_messages_in_node_update=self._include_sent_messages_in_node_update))\n        output_kwargs = dict(embed_edge_fn=_build_update_fns_for_edge_types(build_mlp, graph_template, 'decoder_edges_', self._edge_output_size) if self._edge_output_size else None, embed_node_fn=_build_update_fns_for_node_types(build_mlp, graph_template, 'decoder_nodes_', self._node_output_size) if self._node_output_size else None)\n        self._output_network = typed_graph_net.GraphMapFeatures(**output_kwargs)\n\n    def _embed(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Embeds the input graph features into a latent graph.\"\"\"\n        context_features = input_graph.context.features\n        if jax.tree_util.tree_leaves(context_features):\n            assert len(jax.tree_util.tree_leaves(context_features)) == 1\n            new_nodes = {}\n            for (node_set_name, node_set) in input_graph.nodes.items():\n                node_features = node_set.features\n                broadcasted_context = jnp.repeat(context_features, node_set.n_node, axis=0, total_repeat_length=node_features.shape[0])\n                new_nodes[node_set_name] = node_set._replace(features=jnp.concatenate([node_features, broadcasted_context], axis=-1))\n            input_graph = input_graph._replace(nodes=new_nodes, context=input_graph.context._replace(features=()))\n        latent_graph_0 = self._embedder_network(input_graph)\n        return latent_graph_0\n\n    def _process(self, latent_graph_0: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n        latent_graph = latent_graph_0\n        for unused_repetition_i in range(self._num_processor_repetitions):\n            for processor_network in self._processor_networks:\n                latent_graph = self._process_step(processor_network, latent_graph)\n        return latent_graph\n\n    def _process_step(self, processor_network_k, latent_graph_prev_k: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n        latent_graph_k = processor_network_k(latent_graph_prev_k)\n        nodes_with_residuals = {}\n        for (k, prev_set) in latent_graph_prev_k.nodes.items():\n            nodes_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.nodes[k].features)\n        edges_with_residuals = {}\n        for (k, prev_set) in latent_graph_prev_k.edges.items():\n            edges_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.edges[k].features)\n        latent_graph_k = latent_graph_k._replace(nodes=nodes_with_residuals, edges=edges_with_residuals)\n        return latent_graph_k\n\n    def _output(self, latent_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Produces the output from the latent graph.\"\"\"\n        return self._output_network(latent_graph)",
    "88": "def __call__(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n    self._networks_builder(input_graph)\n    latent_graph_0 = self._embed(input_graph)\n    latent_graph_m = self._process(latent_graph_0)\n    return self._output(latent_graph_m)",
    "91": "def build_mlp_with_maybe_layer_norm(name, output_size):\n    network = build_mlp(name, output_size)\n    if self._use_layer_norm:\n        layer_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True, name=name + '_layer_norm')\n        network = hk.Sequential([network, layer_norm])\n    return jraph.concatenated_args(network)",
    "92": "def aggregate_fn(data, *args, **kwargs):\n    dtype = data.dtype\n    data = data.astype(jnp.float32)\n    output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n    if self._aggregate_normalization:\n        output = output / self._aggregate_normalization\n    output = output.astype(dtype)\n    return output",
    "94": "def _embed(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Embeds the input graph features into a latent graph.\"\"\"\n    context_features = input_graph.context.features\n    if jax.tree_util.tree_leaves(context_features):\n        assert len(jax.tree_util.tree_leaves(context_features)) == 1\n        new_nodes = {}\n        for (node_set_name, node_set) in input_graph.nodes.items():\n            node_features = node_set.features\n            broadcasted_context = jnp.repeat(context_features, node_set.n_node, axis=0, total_repeat_length=node_features.shape[0])\n            new_nodes[node_set_name] = node_set._replace(features=jnp.concatenate([node_features, broadcasted_context], axis=-1))\n        input_graph = input_graph._replace(nodes=new_nodes, context=input_graph.context._replace(features=()))\n    latent_graph_0 = self._embedder_network(input_graph)\n    return latent_graph_0",
    "95": "def _process(self, latent_graph_0: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n    latent_graph = latent_graph_0\n    for unused_repetition_i in range(self._num_processor_repetitions):\n        for processor_network in self._processor_networks:\n            latent_graph = self._process_step(processor_network, latent_graph)\n    return latent_graph",
    "96": "def _process_step(self, processor_network_k, latent_graph_prev_k: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n    latent_graph_k = processor_network_k(latent_graph_prev_k)\n    nodes_with_residuals = {}\n    for (k, prev_set) in latent_graph_prev_k.nodes.items():\n        nodes_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.nodes[k].features)\n    edges_with_residuals = {}\n    for (k, prev_set) in latent_graph_prev_k.edges.items():\n        edges_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.edges[k].features)\n    latent_graph_k = latent_graph_k._replace(nodes=nodes_with_residuals, edges=edges_with_residuals)\n    return latent_graph_k",
    "98": "def _build_update_fns_for_node_types(builder_fn, graph_template, prefix, output_sizes=None):\n    \"\"\"Builds an update function for all node types or a subset of them.\"\"\"\n    output_fns = {}\n    for node_set_name in graph_template.nodes.keys():\n        if output_sizes is None:\n            output_size = None\n        elif node_set_name in output_sizes:\n            output_size = output_sizes[node_set_name]\n        else:\n            continue\n        output_fns[node_set_name] = builder_fn(f'{prefix}{node_set_name}', output_size)\n    return output_fns",
    "99": "def _build_update_fns_for_edge_types(builder_fn, graph_template, prefix, output_sizes=None):\n    \"\"\"Builds an edge function for all node types or a subset of them.\"\"\"\n    output_fns = {}\n    for edge_set_key in graph_template.edges.keys():\n        edge_set_name = edge_set_key.name\n        if output_sizes is None:\n            output_size = None\n        elif edge_set_name in output_sizes:\n            output_size = output_sizes[edge_set_name]\n        else:\n            continue\n        output_fns[edge_set_name] = builder_fn(f'{prefix}{edge_set_name}', output_size)\n    return output_fns",
    "100": "def _get_activation_fn(name):\n    \"\"\"Return activation function corresponding to function_name.\"\"\"\n    if name == 'identity':\n        return lambda x: x\n    if hasattr(jax.nn, name):\n        return getattr(jax.nn, name)\n    if hasattr(jnp, name):\n        return getattr(jnp, name)\n    raise ValueError(f'Unknown activation function {name} specified.')",
    "102": "@chex.dataclass(frozen=True, eq=True)\nclass TaskConfig:\n    \"\"\"Defines inputs and targets on which a model is trained and/or evaluated.\"\"\"\n    input_variables: tuple[str, ...]\n    target_variables: tuple[str, ...]\n    forcing_variables: tuple[str, ...]\n    pressure_levels: tuple[int, ...]\n    input_duration: str",
    "103": "@chex.dataclass(frozen=True, eq=True)\nclass ModelConfig:\n    \"\"\"Defines the architecture of the GraphCast neural network architecture.\n\n  Properties:\n    resolution: The resolution of the data, in degrees (e.g. 0.25 or 1.0).\n    mesh_size: How many refinements to do on the multi-mesh.\n    gnn_msg_steps: How many Graph Network message passing steps to do.\n    latent_size: How many latent features to include in the various MLPs.\n    hidden_layers: How many hidden layers for each MLP.\n    radius_query_fraction_edge_length: Scalar that will be multiplied by the\n        length of the longest edge of the finest mesh to define the radius of\n        connectivity to use in the Grid2Mesh graph. Reasonable values are\n        between 0.6 and 1. 0.6 reduces the number of grid points feeding into\n        multiple mesh nodes and therefore reduces edge count and memory use, but\n        1 gives better predictions.\n    mesh2grid_edge_normalization_factor: Allows explicitly controlling edge\n        normalization for mesh2grid edges. If None, defaults to max edge length.\n        This supports using pre-trained model weights with a different graph\n        structure to what it was trained on.\n  \"\"\"\n    resolution: float\n    mesh_size: int\n    latent_size: int\n    gnn_msg_steps: int\n    hidden_layers: int\n    radius_query_fraction_edge_length: float\n    mesh2grid_edge_normalization_factor: Optional[float] = None",
    "106": "def __init__(self, model_config: ModelConfig, task_config: TaskConfig):\n    \"\"\"Initializes the predictor.\"\"\"\n    self._spatial_features_kwargs = dict(add_node_positions=False, add_node_latitude=True, add_node_longitude=True, add_relative_positions=True, relative_longitude_local_coordinates=True, relative_latitude_local_coordinates=True)\n    self._meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=model_config.mesh_size)\n    self._grid2mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(embed_nodes=True, embed_edges=True, edge_latent_size=dict(grid2mesh=model_config.latent_size), node_latent_size=dict(mesh_nodes=model_config.latent_size, grid_nodes=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=1, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=True, aggregate_normalization=None, name='grid2mesh_gnn')\n    self._mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(embed_nodes=False, embed_edges=True, node_latent_size=dict(mesh_nodes=model_config.latent_size), edge_latent_size=dict(mesh=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=model_config.gnn_msg_steps, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=False, name='mesh_gnn')\n    num_surface_vars = len(set(task_config.target_variables) - set(ALL_ATMOSPHERIC_VARS))\n    num_atmospheric_vars = len(set(task_config.target_variables) & set(ALL_ATMOSPHERIC_VARS))\n    num_outputs = num_surface_vars + len(task_config.pressure_levels) * num_atmospheric_vars\n    self._mesh2grid_gnn = deep_typed_graph_net.DeepTypedGraphNet(node_output_size=dict(grid_nodes=num_outputs), embed_nodes=False, embed_edges=True, edge_latent_size=dict(mesh2grid=model_config.latent_size), node_latent_size=dict(mesh_nodes=model_config.latent_size, grid_nodes=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=1, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=False, name='mesh2grid_gnn')\n    self._query_radius = _get_max_edge_distance(self._finest_mesh) * model_config.radius_query_fraction_edge_length\n    self._mesh2grid_edge_normalization_factor = model_config.mesh2grid_edge_normalization_factor\n    self._initialized = False\n    self._num_mesh_nodes = None\n    self._mesh_nodes_lat = None\n    self._mesh_nodes_lon = None\n    self._grid_lat = None\n    self._grid_lon = None\n    self._num_grid_nodes = None\n    self._grid_nodes_lat = None\n    self._grid_nodes_lon = None\n    self._grid2mesh_graph_structure = None\n    self._mesh_graph_structure = None\n    self._mesh2grid_graph_structure = None",
    "108": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, is_training: bool=False) -> xarray.Dataset:\n    self._maybe_init(inputs)\n    grid_node_features = self._inputs_to_grid_node_features(inputs, forcings)\n    (latent_mesh_nodes, latent_grid_nodes) = self._run_grid2mesh_gnn(grid_node_features)\n    updated_latent_mesh_nodes = self._run_mesh_gnn(latent_mesh_nodes)\n    output_grid_nodes = self._run_mesh2grid_gnn(updated_latent_mesh_nodes, latent_grid_nodes)\n    return self._grid_node_outputs_to_prediction(output_grid_nodes, targets_template)",
    "109": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset) -> tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    predictions = self(inputs, targets_template=targets, forcings=forcings, is_training=True)\n    loss = losses.weighted_mse_per_level(predictions, targets, per_variable_weights={'2m_temperature': 1.0, '10m_u_component_of_wind': 0.1, '10m_v_component_of_wind': 0.1, 'mean_sea_level_pressure': 0.1, 'total_precipitation_6hr': 0.1})\n    return (loss, predictions)",
    "111": "def _maybe_init(self, sample_inputs: xarray.Dataset):\n    \"\"\"Inits everything that has a dependency on the input coordinates.\"\"\"\n    if not self._initialized:\n        self._init_mesh_properties()\n        self._init_grid_properties(grid_lat=sample_inputs.lat, grid_lon=sample_inputs.lon)\n        self._grid2mesh_graph_structure = self._init_grid2mesh_graph()\n        self._mesh_graph_structure = self._init_mesh_graph()\n        self._mesh2grid_graph_structure = self._init_mesh2grid_graph()\n        self._initialized = True",
    "112": "def _init_mesh_properties(self):\n    \"\"\"Inits static properties that have to do with mesh nodes.\"\"\"\n    self._num_mesh_nodes = self._finest_mesh.vertices.shape[0]\n    (mesh_phi, mesh_theta) = model_utils.cartesian_to_spherical(self._finest_mesh.vertices[:, 0], self._finest_mesh.vertices[:, 1], self._finest_mesh.vertices[:, 2])\n    (mesh_nodes_lat, mesh_nodes_lon) = model_utils.spherical_to_lat_lon(phi=mesh_phi, theta=mesh_theta)\n    self._mesh_nodes_lat = mesh_nodes_lat.astype(np.float32)\n    self._mesh_nodes_lon = mesh_nodes_lon.astype(np.float32)",
    "113": "def _init_grid_properties(self, grid_lat: np.ndarray, grid_lon: np.ndarray):\n    \"\"\"Inits static properties that have to do with grid nodes.\"\"\"\n    self._grid_lat = grid_lat.astype(np.float32)\n    self._grid_lon = grid_lon.astype(np.float32)\n    self._num_grid_nodes = grid_lat.shape[0] * grid_lon.shape[0]\n    (grid_nodes_lon, grid_nodes_lat) = np.meshgrid(grid_lon, grid_lat)\n    self._grid_nodes_lon = grid_nodes_lon.reshape([-1]).astype(np.float32)\n    self._grid_nodes_lat = grid_nodes_lat.reshape([-1]).astype(np.float32)",
    "114": "def _init_grid2mesh_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Grid2Mesh graph.\"\"\"\n    assert self._grid_lat is not None and self._grid_lon is not None\n    (grid_indices, mesh_indices) = grid_mesh_connectivity.radius_query_indices(grid_latitude=self._grid_lat, grid_longitude=self._grid_lon, mesh=self._finest_mesh, radius=self._query_radius)\n    senders = grid_indices\n    receivers = mesh_indices\n    (senders_node_features, receivers_node_features, edge_features) = model_utils.get_bipartite_graph_spatial_features(senders_node_lat=self._grid_nodes_lat, senders_node_lon=self._grid_nodes_lon, receivers_node_lat=self._mesh_nodes_lat, receivers_node_lon=self._mesh_nodes_lon, senders=senders, receivers=receivers, edge_normalization_factor=None, **self._spatial_features_kwargs)\n    n_grid_node = np.array([self._num_grid_nodes])\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([mesh_indices.shape[0]])\n    grid_node_set = typed_graph.NodeSet(n_node=n_grid_node, features=senders_node_features)\n    mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=receivers_node_features)\n    edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n    nodes = {'grid_nodes': grid_node_set, 'mesh_nodes': mesh_node_set}\n    edges = {typed_graph.EdgeSetKey('grid2mesh', ('grid_nodes', 'mesh_nodes')): edge_set}\n    grid2mesh_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n    return grid2mesh_graph",
    "115": "def _init_mesh_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Mesh graph.\"\"\"\n    merged_mesh = icosahedral_mesh.merge_meshes(self._meshes)\n    (senders, receivers) = icosahedral_mesh.faces_to_edges(merged_mesh.faces)\n    assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n    (node_features, edge_features) = model_utils.get_graph_spatial_features(node_lat=self._mesh_nodes_lat, node_lon=self._mesh_nodes_lon, senders=senders, receivers=receivers, **self._spatial_features_kwargs)\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([senders.shape[0]])\n    assert n_mesh_node == len(node_features)\n    mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=node_features)\n    edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n    nodes = {'mesh_nodes': mesh_node_set}\n    edges = {typed_graph.EdgeSetKey('mesh', ('mesh_nodes', 'mesh_nodes')): edge_set}\n    mesh_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n    return mesh_graph",
    "116": "def _init_mesh2grid_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Mesh2Grid graph.\"\"\"\n    (grid_indices, mesh_indices) = grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=self._grid_lat, grid_longitude=self._grid_lon, mesh=self._finest_mesh)\n    senders = mesh_indices\n    receivers = grid_indices\n    assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n    (senders_node_features, receivers_node_features, edge_features) = model_utils.get_bipartite_graph_spatial_features(senders_node_lat=self._mesh_nodes_lat, senders_node_lon=self._mesh_nodes_lon, receivers_node_lat=self._grid_nodes_lat, receivers_node_lon=self._grid_nodes_lon, senders=senders, receivers=receivers, edge_normalization_factor=self._mesh2grid_edge_normalization_factor, **self._spatial_features_kwargs)\n    n_grid_node = np.array([self._num_grid_nodes])\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([senders.shape[0]])\n    grid_node_set = typed_graph.NodeSet(n_node=n_grid_node, features=receivers_node_features)\n    mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=senders_node_features)\n    edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n    nodes = {'grid_nodes': grid_node_set, 'mesh_nodes': mesh_node_set}\n    edges = {typed_graph.EdgeSetKey('mesh2grid', ('mesh_nodes', 'grid_nodes')): edge_set}\n    mesh2grid_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n    return mesh2grid_graph",
    "117": "def _run_grid2mesh_gnn(self, grid_node_features: chex.Array) -> tuple[chex.Array, chex.Array]:\n    \"\"\"Runs the grid2mesh_gnn, extracting latent mesh and grid nodes.\"\"\"\n    batch_size = grid_node_features.shape[1]\n    grid2mesh_graph = self._grid2mesh_graph_structure\n    assert grid2mesh_graph is not None\n    grid_nodes = grid2mesh_graph.nodes['grid_nodes']\n    mesh_nodes = grid2mesh_graph.nodes['mesh_nodes']\n    new_grid_nodes = grid_nodes._replace(features=jnp.concatenate([grid_node_features, _add_batch_second_axis(grid_nodes.features.astype(grid_node_features.dtype), batch_size)], axis=-1))\n    dummy_mesh_node_features = jnp.zeros((self._num_mesh_nodes,) + grid_node_features.shape[1:], dtype=grid_node_features.dtype)\n    new_mesh_nodes = mesh_nodes._replace(features=jnp.concatenate([dummy_mesh_node_features, _add_batch_second_axis(mesh_nodes.features.astype(dummy_mesh_node_features.dtype), batch_size)], axis=-1))\n    grid2mesh_edges_key = grid2mesh_graph.edge_key_by_name('grid2mesh')\n    edges = grid2mesh_graph.edges[grid2mesh_edges_key]\n    new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(dummy_mesh_node_features.dtype), batch_size))\n    input_graph = self._grid2mesh_graph_structure._replace(edges={grid2mesh_edges_key: new_edges}, nodes={'grid_nodes': new_grid_nodes, 'mesh_nodes': new_mesh_nodes})\n    grid2mesh_out = self._grid2mesh_gnn(input_graph)\n    latent_mesh_nodes = grid2mesh_out.nodes['mesh_nodes'].features\n    latent_grid_nodes = grid2mesh_out.nodes['grid_nodes'].features\n    return (latent_mesh_nodes, latent_grid_nodes)",
    "118": "def _run_mesh_gnn(self, latent_mesh_nodes: chex.Array) -> chex.Array:\n    \"\"\"Runs the mesh_gnn, extracting updated latent mesh nodes.\"\"\"\n    batch_size = latent_mesh_nodes.shape[1]\n    mesh_graph = self._mesh_graph_structure\n    assert mesh_graph is not None\n    mesh_edges_key = mesh_graph.edge_key_by_name('mesh')\n    edges = mesh_graph.edges[mesh_edges_key]\n    msg = 'The setup currently requires to only have one kind of edge in the mesh GNN.'\n    assert len(mesh_graph.edges) == 1, msg\n    new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(latent_mesh_nodes.dtype), batch_size))\n    nodes = mesh_graph.nodes['mesh_nodes']\n    nodes = nodes._replace(features=latent_mesh_nodes)\n    input_graph = mesh_graph._replace(edges={mesh_edges_key: new_edges}, nodes={'mesh_nodes': nodes})\n    return self._mesh_gnn(input_graph).nodes['mesh_nodes'].features",
    "119": "def _run_mesh2grid_gnn(self, updated_latent_mesh_nodes: chex.Array, latent_grid_nodes: chex.Array) -> chex.Array:\n    \"\"\"Runs the mesh2grid_gnn, extracting the output grid nodes.\"\"\"\n    batch_size = updated_latent_mesh_nodes.shape[1]\n    mesh2grid_graph = self._mesh2grid_graph_structure\n    assert mesh2grid_graph is not None\n    mesh_nodes = mesh2grid_graph.nodes['mesh_nodes']\n    grid_nodes = mesh2grid_graph.nodes['grid_nodes']\n    new_mesh_nodes = mesh_nodes._replace(features=updated_latent_mesh_nodes)\n    new_grid_nodes = grid_nodes._replace(features=latent_grid_nodes)\n    mesh2grid_key = mesh2grid_graph.edge_key_by_name('mesh2grid')\n    edges = mesh2grid_graph.edges[mesh2grid_key]\n    new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(latent_grid_nodes.dtype), batch_size))\n    input_graph = mesh2grid_graph._replace(edges={mesh2grid_key: new_edges}, nodes={'mesh_nodes': new_mesh_nodes, 'grid_nodes': new_grid_nodes})\n    output_graph = self._mesh2grid_gnn(input_graph)\n    output_grid_nodes = output_graph.nodes['grid_nodes'].features\n    return output_grid_nodes",
    "120": "def _inputs_to_grid_node_features(self, inputs: xarray.Dataset, forcings: xarray.Dataset) -> chex.Array:\n    \"\"\"xarrays -> [num_grid_nodes, batch, num_channels].\"\"\"\n    stacked_inputs = model_utils.dataset_to_stacked(inputs)\n    stacked_forcings = model_utils.dataset_to_stacked(forcings)\n    stacked_inputs = xarray.concat([stacked_inputs, stacked_forcings], dim='channels')\n    grid_xarray_lat_lon_leading = model_utils.lat_lon_to_leading_axes(stacked_inputs)\n    return xarray_jax.unwrap(grid_xarray_lat_lon_leading.data).reshape((-1,) + grid_xarray_lat_lon_leading.data.shape[2:])",
    "121": "def _grid_node_outputs_to_prediction(self, grid_node_outputs: chex.Array, targets_template: xarray.Dataset) -> xarray.Dataset:\n    \"\"\"[num_grid_nodes, batch, num_outputs] -> xarray.\"\"\"\n    assert self._grid_lat is not None and self._grid_lon is not None\n    grid_shape = (self._grid_lat.shape[0], self._grid_lon.shape[0])\n    grid_outputs_lat_lon_leading = grid_node_outputs.reshape(grid_shape + grid_node_outputs.shape[1:])\n    dims = ('lat', 'lon', 'batch', 'channels')\n    grid_xarray_lat_lon_leading = xarray_jax.DataArray(data=grid_outputs_lat_lon_leading, dims=dims)\n    grid_xarray = model_utils.restore_leading_axes(grid_xarray_lat_lon_leading)\n    return model_utils.stacked_to_dataset(grid_xarray.variable, targets_template)",
    "123": "def _get_max_edge_distance(mesh):\n    (senders, receivers) = icosahedral_mesh.faces_to_edges(mesh.faces)\n    edge_distances = np.linalg.norm(mesh.vertices[senders] - mesh.vertices[receivers], axis=-1)\n    return edge_distances.max()",
    "124": "def _grid_lat_lon_to_coordinates(grid_latitude: np.ndarray, grid_longitude: np.ndarray) -> np.ndarray:\n    \"\"\"Lat [num_lat] lon [num_lon] to 3d coordinates [num_lat, num_lon, 3].\"\"\"\n    (phi_grid, theta_grid) = np.meshgrid(np.deg2rad(grid_longitude), np.deg2rad(90 - grid_latitude))\n    return np.stack([np.cos(phi_grid) * np.sin(theta_grid), np.sin(phi_grid) * np.sin(theta_grid), np.cos(theta_grid)], axis=-1)",
    "125": "def radius_query_indices(*, grid_latitude: np.ndarray, grid_longitude: np.ndarray, mesh: icosahedral_mesh.TriangularMesh, radius: float) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns mesh-grid edge indices for radius query.\n\n  Args:\n    grid_latitude: Latitude values for the grid [num_lat_points]\n    grid_longitude: Longitude values for the grid [num_lon_points]\n    mesh: Mesh object.\n    radius: Radius of connectivity in R3. for a sphere of unit radius.\n\n  Returns:\n    tuple with `grid_indices` and `mesh_indices` indicating edges between the\n    grid and the mesh such that the distances in a straight line (not geodesic)\n    are smaller than or equal to `radius`.\n    * grid_indices: Indices of shape [num_edges], that index into a\n      [num_lat_points, num_lon_points] grid, after flattening the leading axes.\n    * mesh_indices: Indices of shape [num_edges], that index into mesh.vertices.\n  \"\"\"\n    grid_positions = _grid_lat_lon_to_coordinates(grid_latitude, grid_longitude).reshape([-1, 3])\n    mesh_positions = mesh.vertices\n    kd_tree = scipy.spatial.cKDTree(mesh_positions)\n    query_indices = kd_tree.query_ball_point(x=grid_positions, r=radius)\n    grid_edge_indices = []\n    mesh_edge_indices = []\n    for (grid_index, mesh_neighbors) in enumerate(query_indices):\n        grid_edge_indices.append(np.repeat(grid_index, len(mesh_neighbors)))\n        mesh_edge_indices.append(mesh_neighbors)\n    grid_edge_indices = np.concatenate(grid_edge_indices, axis=0).astype(int)\n    mesh_edge_indices = np.concatenate(mesh_edge_indices, axis=0).astype(int)\n    return (grid_edge_indices, mesh_edge_indices)",
    "126": "def in_mesh_triangle_indices(*, grid_latitude: np.ndarray, grid_longitude: np.ndarray, mesh: icosahedral_mesh.TriangularMesh) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns mesh-grid edge indices for grid points contained in mesh triangles.\n\n  Args:\n    grid_latitude: Latitude values for the grid [num_lat_points]\n    grid_longitude: Longitude values for the grid [num_lon_points]\n    mesh: Mesh object.\n\n  Returns:\n    tuple with `grid_indices` and `mesh_indices` indicating edges between the\n    grid and the mesh vertices of the triangle that contain each grid point.\n    The number of edges is always num_lat_points * num_lon_points * 3\n    * grid_indices: Indices of shape [num_edges], that index into a\n      [num_lat_points, num_lon_points] grid, after flattening the leading axes.\n    * mesh_indices: Indices of shape [num_edges], that index into mesh.vertices.\n  \"\"\"\n    grid_positions = _grid_lat_lon_to_coordinates(grid_latitude, grid_longitude).reshape([-1, 3])\n    mesh_trimesh = trimesh.Trimesh(vertices=mesh.vertices, faces=mesh.faces)\n    (_, _, query_face_indices) = trimesh.proximity.closest_point(mesh_trimesh, grid_positions)\n    mesh_edge_indices = mesh.faces[query_face_indices]\n    grid_indices = np.arange(grid_positions.shape[0])\n    grid_edge_indices = np.tile(grid_indices.reshape([-1, 1]), [1, 3])\n    mesh_edge_indices = mesh_edge_indices.reshape([-1])\n    grid_edge_indices = grid_edge_indices.reshape([-1])\n    return (grid_edge_indices, mesh_edge_indices)",
    "127": "class GridMeshConnectivityTest(absltest.TestCase):\n\n    def test_grid_lat_lon_to_coordinates(self):\n        grid_latitude = np.array([-45.0, 0.0, 45])\n        grid_longitude = np.array([0.0, 90.0, 180.0, 270.0])\n        inv_sqrt2 = 1 / np.sqrt(2)\n        expected_coordinates = np.array([[[inv_sqrt2, 0.0, -inv_sqrt2], [0.0, inv_sqrt2, -inv_sqrt2], [-inv_sqrt2, 0.0, -inv_sqrt2], [0.0, -inv_sqrt2, -inv_sqrt2]], [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, -1.0, 0.0]], [[inv_sqrt2, 0.0, inv_sqrt2], [0.0, inv_sqrt2, inv_sqrt2], [-inv_sqrt2, 0.0, inv_sqrt2], [0.0, -inv_sqrt2, inv_sqrt2]]])\n        coordinates = grid_mesh_connectivity._grid_lat_lon_to_coordinates(grid_latitude, grid_longitude)\n        np.testing.assert_allclose(expected_coordinates, coordinates, atol=1e-15)\n\n    def test_radius_query_indices_smoke(self):\n        grid_latitude = np.linspace(-75, 75, 6)\n        grid_longitude = np.arange(12) * 30.0\n        mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n        grid_mesh_connectivity.radius_query_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh, radius=0.2)\n\n    def test_in_mesh_triangle_indices_smoke(self):\n        grid_latitude = np.linspace(-75, 75, 6)\n        grid_longitude = np.arange(12) * 30.0\n        mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n        grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh)",
    "128": "def test_grid_lat_lon_to_coordinates(self):\n    grid_latitude = np.array([-45.0, 0.0, 45])\n    grid_longitude = np.array([0.0, 90.0, 180.0, 270.0])\n    inv_sqrt2 = 1 / np.sqrt(2)\n    expected_coordinates = np.array([[[inv_sqrt2, 0.0, -inv_sqrt2], [0.0, inv_sqrt2, -inv_sqrt2], [-inv_sqrt2, 0.0, -inv_sqrt2], [0.0, -inv_sqrt2, -inv_sqrt2]], [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, -1.0, 0.0]], [[inv_sqrt2, 0.0, inv_sqrt2], [0.0, inv_sqrt2, inv_sqrt2], [-inv_sqrt2, 0.0, inv_sqrt2], [0.0, -inv_sqrt2, inv_sqrt2]]])\n    coordinates = grid_mesh_connectivity._grid_lat_lon_to_coordinates(grid_latitude, grid_longitude)\n    np.testing.assert_allclose(expected_coordinates, coordinates, atol=1e-15)",
    "129": "def test_radius_query_indices_smoke(self):\n    grid_latitude = np.linspace(-75, 75, 6)\n    grid_longitude = np.arange(12) * 30.0\n    mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n    grid_mesh_connectivity.radius_query_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh, radius=0.2)",
    "130": "def test_in_mesh_triangle_indices_smoke(self):\n    grid_latitude = np.linspace(-75, 75, 6)\n    grid_longitude = np.arange(12) * 30.0\n    mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n    grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh)",
    "131": "class TriangularMesh(NamedTuple):\n    \"\"\"Data structure for triangular meshes.\n\n  Attributes:\n    vertices: spatial positions of the vertices of the mesh of shape\n        [num_vertices, num_dims].\n    faces: triangular faces of the mesh of shape [num_faces, 3]. Contains\n        integer indices into `vertices`.\n\n  \"\"\"\n    vertices: np.ndarray\n    faces: np.ndarray",
    "132": "def merge_meshes(mesh_list: Sequence[TriangularMesh]) -> TriangularMesh:\n    \"\"\"Merges all meshes into one. Assumes the last mesh is the finest.\n\n  Args:\n     mesh_list: Sequence of meshes, from coarse to fine refinement levels. The\n       vertices and faces may contain those from preceding, coarser levels.\n\n  Returns:\n     `TriangularMesh` for which the vertices correspond to the highest\n     resolution mesh in the hierarchy, and the faces are the join set of the\n     faces at all levels of the hierarchy.\n  \"\"\"\n    for (mesh_i, mesh_ip1) in itertools.pairwise(mesh_list):\n        num_nodes_mesh_i = mesh_i.vertices.shape[0]\n        assert np.allclose(mesh_i.vertices, mesh_ip1.vertices[:num_nodes_mesh_i])\n    return TriangularMesh(vertices=mesh_list[-1].vertices, faces=np.concatenate([mesh.faces for mesh in mesh_list], axis=0))",
    "133": "def get_hierarchy_of_triangular_meshes_for_sphere(splits: int) -> List[TriangularMesh]:\n    \"\"\"Returns a sequence of meshes, each with triangularization sphere.\n\n  Starting with a regular icosahedron (12 vertices, 20 faces, 30 edges) with\n  circumscribed unit sphere. Then, each triangular face is iteratively\n  subdivided into 4 triangular faces `splits` times. The new vertices are then\n  projected back onto the unit sphere. All resulting meshes are returned in a\n  list, from lowest to highest resolution.\n\n  The vertices in each face are specified in counter-clockwise order as\n  observed from the outside the icosahedron.\n\n  Args:\n     splits: How many times to split each triangle.\n  Returns:\n     Sequence of `TriangularMesh`s of length `splits + 1` each with:\n\n       vertices: [num_vertices, 3] vertex positions in 3D, all with unit norm.\n       faces: [num_faces, 3] with triangular faces joining sets of 3 vertices.\n           Each row contains three indices into the vertices array, indicating\n           the vertices adjacent to the face. Always with positive orientation\n           (counterclock-wise when looking from the outside).\n  \"\"\"\n    current_mesh = get_icosahedron()\n    output_meshes = [current_mesh]\n    for _ in range(splits):\n        current_mesh = _two_split_unit_sphere_triangle_faces(current_mesh)\n        output_meshes.append(current_mesh)\n    return output_meshes",
    "134": "def get_icosahedron() -> TriangularMesh:\n    \"\"\"Returns a regular icosahedral mesh with circumscribed unit sphere.\n\n  See https://en.wikipedia.org/wiki/Regular_icosahedron#Cartesian_coordinates\n  for details on the construction of the regular icosahedron.\n\n  The vertices in each face are specified in counter-clockwise order as observed\n  from the outside of the icosahedron.\n\n  Returns:\n     TriangularMesh with:\n\n     vertices: [num_vertices=12, 3] vertex positions in 3D, all with unit norm.\n     faces: [num_faces=20, 3] with triangular faces joining sets of 3 vertices.\n         Each row contains three indices into the vertices array, indicating\n         the vertices adjacent to the face. Always with positive orientation (\n         counterclock-wise when looking from the outside).\n\n  \"\"\"\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = []\n    for c1 in [1.0, -1.0]:\n        for c2 in [phi, -phi]:\n            vertices.append((c1, c2, 0.0))\n            vertices.append((0.0, c1, c2))\n            vertices.append((c2, 0.0, c1))\n    vertices = np.array(vertices, dtype=np.float32)\n    vertices /= np.linalg.norm([1.0, phi])\n    faces = [(0, 1, 2), (0, 6, 1), (8, 0, 2), (8, 4, 0), (3, 8, 2), (3, 2, 7), (7, 2, 1), (0, 4, 6), (4, 11, 6), (6, 11, 5), (1, 5, 7), (4, 10, 11), (4, 8, 10), (10, 8, 3), (10, 3, 9), (11, 10, 9), (11, 9, 5), (5, 9, 7), (9, 3, 7), (1, 6, 5)]\n    angle_between_faces = 2 * np.arcsin(phi / np.sqrt(3))\n    rotation_angle = (np.pi - angle_between_faces) / 2\n    rotation = transform.Rotation.from_euler(seq='y', angles=rotation_angle)\n    rotation_matrix = rotation.as_matrix()\n    vertices = np.dot(vertices, rotation_matrix)\n    return TriangularMesh(vertices=vertices.astype(np.float32), faces=np.array(faces, dtype=np.int32))",
    "135": "def _two_split_unit_sphere_triangle_faces(triangular_mesh: TriangularMesh) -> TriangularMesh:\n    \"\"\"Splits each triangular face into 4 triangles keeping the orientation.\"\"\"\n    new_vertices_builder = _ChildVerticesBuilder(triangular_mesh.vertices)\n    new_faces = []\n    for (ind1, ind2, ind3) in triangular_mesh.faces:\n        ind12 = new_vertices_builder.get_new_child_vertex_index((ind1, ind2))\n        ind23 = new_vertices_builder.get_new_child_vertex_index((ind2, ind3))\n        ind31 = new_vertices_builder.get_new_child_vertex_index((ind3, ind1))\n        new_faces.extend([[ind1, ind12, ind31], [ind12, ind2, ind23], [ind31, ind23, ind3], [ind12, ind23, ind31]])\n    return TriangularMesh(vertices=new_vertices_builder.get_all_vertices(), faces=np.array(new_faces, dtype=np.int32))",
    "136": "class _ChildVerticesBuilder(object):\n    \"\"\"Bookkeeping of new child vertices added to an existing set of vertices.\"\"\"\n\n    def __init__(self, parent_vertices):\n        self._child_vertices_index_mapping = {}\n        self._parent_vertices = parent_vertices\n        self._all_vertices_list = list(parent_vertices)\n\n    def _get_child_vertex_key(self, parent_vertex_indices):\n        return tuple(sorted(parent_vertex_indices))\n\n    def _create_child_vertex(self, parent_vertex_indices):\n        \"\"\"Creates a new vertex.\"\"\"\n        child_vertex_position = self._parent_vertices[list(parent_vertex_indices)].mean(0)\n        child_vertex_position /= np.linalg.norm(child_vertex_position)\n        child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n        self._child_vertices_index_mapping[child_vertex_key] = len(self._all_vertices_list)\n        self._all_vertices_list.append(child_vertex_position)\n\n    def get_new_child_vertex_index(self, parent_vertex_indices):\n        \"\"\"Returns index for a child vertex, creating it if necessary.\"\"\"\n        child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n        if child_vertex_key not in self._child_vertices_index_mapping:\n            self._create_child_vertex(parent_vertex_indices)\n        return self._child_vertices_index_mapping[child_vertex_key]\n\n    def get_all_vertices(self):\n        \"\"\"Returns an array with old vertices.\"\"\"\n        return np.array(self._all_vertices_list)",
    "138": "def _get_child_vertex_key(self, parent_vertex_indices):\n    return tuple(sorted(parent_vertex_indices))",
    "139": "def _create_child_vertex(self, parent_vertex_indices):\n    \"\"\"Creates a new vertex.\"\"\"\n    child_vertex_position = self._parent_vertices[list(parent_vertex_indices)].mean(0)\n    child_vertex_position /= np.linalg.norm(child_vertex_position)\n    child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n    self._child_vertices_index_mapping[child_vertex_key] = len(self._all_vertices_list)\n    self._all_vertices_list.append(child_vertex_position)",
    "140": "def get_new_child_vertex_index(self, parent_vertex_indices):\n    \"\"\"Returns index for a child vertex, creating it if necessary.\"\"\"\n    child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n    if child_vertex_key not in self._child_vertices_index_mapping:\n        self._create_child_vertex(parent_vertex_indices)\n    return self._child_vertices_index_mapping[child_vertex_key]",
    "142": "def faces_to_edges(faces: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Transforms polygonal faces to sender and receiver indices.\n\n  It does so by transforming every face into N_i edges. Such if the triangular\n  face has indices [0, 1, 2], three edges are added 0->1, 1->2, and 2->0.\n\n  If all faces have consistent orientation, and the surface represented by the\n  faces is closed, then every edge in a polygon with a certain orientation\n  is also part of another polygon with the opposite orientation. In this\n  situation, the edges returned by the method are always bidirectional.\n\n  Args:\n    faces: Integer array of shape [num_faces, 3]. Contains node indices\n        adjacent to each face.\n  Returns:\n    Tuple with sender/receiver indices, each of shape [num_edges=num_faces*3].\n\n  \"\"\"\n    assert faces.ndim == 2\n    assert faces.shape[-1] == 3\n    senders = np.concatenate([faces[:, 0], faces[:, 1], faces[:, 2]])\n    receivers = np.concatenate([faces[:, 1], faces[:, 2], faces[:, 0]])\n    return (senders, receivers)",
    "144": "class IcosahedralMeshTest(parameterized.TestCase):\n\n    def test_icosahedron(self):\n        mesh = icosahedral_mesh.get_icosahedron()\n        _assert_valid_mesh(mesh, num_expected_vertices=12, num_expected_faces=20)\n\n    @parameterized.parameters(list(range(5)))\n    def test_get_hierarchy_of_triangular_meshes_for_sphere(self, splits):\n        meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n        prev_vertices = None\n        for (mesh_i, mesh) in enumerate(meshes):\n            (num_expected_vertices, num_expected_faces) = _get_mesh_spec(mesh_i)\n            _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces)\n            if prev_vertices is not None:\n                leading_mesh_vertices = mesh.vertices[:prev_vertices.shape[0]]\n                np.testing.assert_array_equal(leading_mesh_vertices, prev_vertices)\n            if mesh_i < len(meshes) - 1:\n                prev_vertices = mesh.vertices\n\n    @parameterized.parameters(list(range(4)))\n    def test_merge_meshes(self, splits):\n        mesh_hierarchy = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n        mesh = icosahedral_mesh.merge_meshes(mesh_hierarchy)\n        expected_faces = np.concatenate([m.faces for m in mesh_hierarchy], axis=0)\n        np.testing.assert_array_equal(mesh.vertices, mesh_hierarchy[-1].vertices)\n        np.testing.assert_array_equal(mesh.faces, expected_faces)\n\n    def test_faces_to_edges(self):\n        faces = np.array([[0, 1, 2], [3, 4, 5]])\n        expected_edges = np.array([[0, 1], [3, 4], [1, 2], [4, 5], [2, 0], [5, 3]])\n        expected_senders = expected_edges[:, 0]\n        expected_receivers = expected_edges[:, 1]\n        (senders, receivers) = icosahedral_mesh.faces_to_edges(faces)\n        np.testing.assert_array_equal(senders, expected_senders)\n        np.testing.assert_array_equal(receivers, expected_receivers)",
    "146": "@parameterized.parameters(list(range(5)))\ndef test_get_hierarchy_of_triangular_meshes_for_sphere(self, splits):\n    meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n    prev_vertices = None\n    for (mesh_i, mesh) in enumerate(meshes):\n        (num_expected_vertices, num_expected_faces) = _get_mesh_spec(mesh_i)\n        _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces)\n        if prev_vertices is not None:\n            leading_mesh_vertices = mesh.vertices[:prev_vertices.shape[0]]\n            np.testing.assert_array_equal(leading_mesh_vertices, prev_vertices)\n        if mesh_i < len(meshes) - 1:\n            prev_vertices = mesh.vertices",
    "147": "@parameterized.parameters(list(range(4)))\ndef test_merge_meshes(self, splits):\n    mesh_hierarchy = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n    mesh = icosahedral_mesh.merge_meshes(mesh_hierarchy)\n    expected_faces = np.concatenate([m.faces for m in mesh_hierarchy], axis=0)\n    np.testing.assert_array_equal(mesh.vertices, mesh_hierarchy[-1].vertices)\n    np.testing.assert_array_equal(mesh.faces, expected_faces)",
    "148": "def test_faces_to_edges(self):\n    faces = np.array([[0, 1, 2], [3, 4, 5]])\n    expected_edges = np.array([[0, 1], [3, 4], [1, 2], [4, 5], [2, 0], [5, 3]])\n    expected_senders = expected_edges[:, 0]\n    expected_receivers = expected_edges[:, 1]\n    (senders, receivers) = icosahedral_mesh.faces_to_edges(faces)\n    np.testing.assert_array_equal(senders, expected_senders)\n    np.testing.assert_array_equal(receivers, expected_receivers)",
    "149": "def _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces):\n    vertices = mesh.vertices\n    faces = mesh.faces\n    chex.assert_shape(vertices, [num_expected_vertices, 3])\n    chex.assert_shape(faces, [num_expected_faces, 3])\n    vertices_norm = np.linalg.norm(vertices, axis=-1)\n    np.testing.assert_allclose(vertices_norm, 1.0, rtol=1e-06)\n    _assert_positive_face_orientation(vertices, faces)",
    "150": "def _assert_positive_face_orientation(vertices, faces):\n    face_orientation = np.cross(vertices[faces[:, 1]] - vertices[faces[:, 0]], vertices[faces[:, 2]] - vertices[faces[:, 1]])\n    face_orientation /= np.linalg.norm(face_orientation, axis=-1, keepdims=True)\n    face_centers = vertices[faces].mean(1)\n    face_centers /= np.linalg.norm(face_centers, axis=-1, keepdims=True)\n    dot_center_orientation = np.einsum('ik,ik->i', face_orientation, face_centers)\n    np.testing.assert_allclose(dot_center_orientation, 1.0, atol=0.0006)",
    "151": "class LossFunction(Protocol):\n    \"\"\"A loss function.\n\n  This is a protocol so it's fine to use a plain function which 'quacks like'\n  this. This is just to document the interface.\n  \"\"\"\n\n    def __call__(self, predictions: xarray.Dataset, targets: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n        \"\"\"Computes a loss function.\n\n    Args:\n      predictions: Dataset of predictions.\n      targets: Dataset of targets.\n      **optional_kwargs: Implementations may support extra optional kwargs.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n    \"\"\"",
    "152": "def __call__(self, predictions: xarray.Dataset, targets: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n    \"\"\"Computes a loss function.\n\n    Args:\n      predictions: Dataset of predictions.\n      targets: Dataset of targets.\n      **optional_kwargs: Implementations may support extra optional kwargs.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n    \"\"\"",
    "153": "def weighted_mse_per_level(predictions: xarray.Dataset, targets: xarray.Dataset, per_variable_weights: Mapping[str, float]) -> LossAndDiagnostics:\n    \"\"\"Latitude- and pressure-level-weighted MSE loss.\"\"\"\n\n    def loss(prediction, target):\n        loss = (prediction - target) ** 2\n        loss *= normalized_latitude_weights(target).astype(loss.dtype)\n        if 'level' in target.dims:\n            loss *= normalized_level_weights(target).astype(loss.dtype)\n        return _mean_preserving_batch(loss)\n    losses = xarray_tree.map_structure(loss, predictions, targets)\n    return sum_per_variable_losses(losses, per_variable_weights)",
    "155": "def _mean_preserving_batch(x: xarray.DataArray) -> xarray.DataArray:\n    return x.mean([d for d in x.dims if d != 'batch'], skipna=False)",
    "156": "def sum_per_variable_losses(per_variable_losses: Mapping[str, xarray.DataArray], weights: Mapping[str, float]) -> LossAndDiagnostics:\n    \"\"\"Weighted sum of per-variable losses.\"\"\"\n    if not set(weights.keys()).issubset(set(per_variable_losses.keys())):\n        raise ValueError(f'Passing a weight that does not correspond to any variable {set(weights.keys()) - set(per_variable_losses.keys())}')\n    weighted_per_variable_losses = {name: loss * weights.get(name, 1) for (name, loss) in per_variable_losses.items()}\n    total = xarray.concat(weighted_per_variable_losses.values(), dim='variable', join='exact').sum('variable', skipna=False)\n    return (total, per_variable_losses)",
    "159": "def _weight_for_latitude_vector_without_poles(latitude):\n    \"\"\"Weights for uniform latitudes of the form [+-90-+d/2, ..., -+90+-d/2].\"\"\"\n    delta_latitude = np.abs(_check_uniform_spacing_and_get_delta(latitude))\n    if not np.isclose(np.max(latitude), 90 - delta_latitude / 2) or not np.isclose(np.min(latitude), -90 + delta_latitude / 2):\n        raise ValueError(f'Latitude vector {latitude} does not start/end at +- (90 - delta_latitude/2) degrees.')\n    return np.cos(np.deg2rad(latitude))",
    "160": "def _weight_for_latitude_vector_with_poles(latitude):\n    \"\"\"Weights for uniform latitudes of the form [+- 90, ..., -+90].\"\"\"\n    delta_latitude = np.abs(_check_uniform_spacing_and_get_delta(latitude))\n    if not np.isclose(np.max(latitude), 90.0) or not np.isclose(np.min(latitude), -90.0):\n        raise ValueError(f'Latitude vector {latitude} does not start/end at +- 90 degrees.')\n    weights = np.cos(np.deg2rad(latitude)) * np.sin(np.deg2rad(delta_latitude / 2))\n    weights[[0, -1]] = np.sin(np.deg2rad(delta_latitude / 4)) ** 2\n    return weights",
    "162": "def get_graph_spatial_features(*, node_lat: np.ndarray, node_lon: np.ndarray, senders: np.ndarray, receivers: np.ndarray, add_node_positions: bool, add_node_latitude: bool, add_node_longitude: bool, add_relative_positions: bool, relative_longitude_local_coordinates: bool, relative_latitude_local_coordinates: bool, sine_cosine_encoding: bool=False, encoding_num_freqs: int=10, encoding_multiplicative_factor: float=1.2) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Computes spatial features for the nodes.\n\n  Args:\n    node_lat: Latitudes in the [-90, 90] interval of shape [num_nodes]\n    node_lon: Longitudes in the [0, 360] interval of shape [num_nodes]\n    senders: Sender indices of shape [num_edges]\n    receivers: Receiver indices of shape [num_edges]\n    add_node_positions: Add unit norm absolute positions.\n    add_node_latitude: Add a feature for latitude (cos(90 - lat))\n        Note even if this is set to False, the model may be able to infer the\n        longitude from relative features, unless\n        `relative_latitude_local_coordinates` is also True, or if there is any\n        bias on the relative edge sizes for different longitudes.\n    add_node_longitude: Add features for longitude (cos(lon), sin(lon)).\n        Note even if this is set to False, the model may be able to infer the\n        longitude from relative features, unless\n        `relative_longitude_local_coordinates` is also True, or if there is any\n        bias on the relative edge sizes for different longitudes.\n    add_relative_positions: Whether to relative positions in R3 to the edges.\n    relative_longitude_local_coordinates: If True, relative positions are\n        computed in a local space where the receiver is at 0 longitude.\n    relative_latitude_local_coordinates: If True, relative positions are\n        computed in a local space where the receiver is at 0 latitude.\n    sine_cosine_encoding: If True, we will transform the node/edge features\n        with sine and cosine functions, similar to NERF.\n    encoding_num_freqs: frequency parameter\n    encoding_multiplicative_factor: used for calculating the frequency.\n\n  Returns:\n    Arrays of shape: [num_nodes, num_features] and [num_edges, num_features].\n    with node and edge features.\n\n  \"\"\"\n    num_nodes = node_lat.shape[0]\n    num_edges = senders.shape[0]\n    dtype = node_lat.dtype\n    (node_phi, node_theta) = lat_lon_deg_to_spherical(node_lat, node_lon)\n    node_features = []\n    if add_node_positions:\n        node_features.extend(spherical_to_cartesian(node_phi, node_theta))\n    if add_node_latitude:\n        node_features.append(np.cos(node_theta))\n    if add_node_longitude:\n        node_features.append(np.cos(node_phi))\n        node_features.append(np.sin(node_phi))\n    if not node_features:\n        node_features = np.zeros([num_nodes, 0], dtype=dtype)\n    else:\n        node_features = np.stack(node_features, axis=-1)\n    edge_features = []\n    if add_relative_positions:\n        relative_position = get_relative_position_in_receiver_local_coordinates(node_phi=node_phi, node_theta=node_theta, senders=senders, receivers=receivers, latitude_local_coordinates=relative_latitude_local_coordinates, longitude_local_coordinates=relative_longitude_local_coordinates)\n        relative_edge_distances = np.linalg.norm(relative_position, axis=-1, keepdims=True)\n        max_edge_distance = relative_edge_distances.max()\n        edge_features.append(relative_edge_distances / max_edge_distance)\n        edge_features.append(relative_position / max_edge_distance)\n    if not edge_features:\n        edge_features = np.zeros([num_edges, 0], dtype=dtype)\n    else:\n        edge_features = np.concatenate(edge_features, axis=-1)\n    if sine_cosine_encoding:\n\n        def sine_cosine_transform(x: np.ndarray) -> np.ndarray:\n            freqs = encoding_multiplicative_factor ** np.arange(encoding_num_freqs)\n            phases = freqs * x[..., None]\n            x_sin = np.sin(phases)\n            x_cos = np.cos(phases)\n            x_cat = np.concatenate([x_sin, x_cos], axis=-1)\n            return x_cat.reshape([x.shape[0], -1])\n        node_features = sine_cosine_transform(node_features)\n        edge_features = sine_cosine_transform(edge_features)\n    return (node_features, edge_features)",
    "163": "def sine_cosine_transform(x: np.ndarray) -> np.ndarray:\n    freqs = encoding_multiplicative_factor ** np.arange(encoding_num_freqs)\n    phases = freqs * x[..., None]\n    x_sin = np.sin(phases)\n    x_cos = np.cos(phases)\n    x_cat = np.concatenate([x_sin, x_cos], axis=-1)\n    return x_cat.reshape([x.shape[0], -1])",
    "165": "def restore_leading_axes(grid_xarray: xarray.DataArray) -> xarray.DataArray:\n    \"\"\"Reorders xarray so batch/time/level axes come first (if present).\"\"\"\n    input_dims = list(grid_xarray.dims)\n    output_dims = list(input_dims)\n    for leading_key in ['level', 'time', 'batch']:\n        if leading_key in input_dims:\n            output_dims.remove(leading_key)\n            output_dims.insert(0, leading_key)\n    return grid_xarray.transpose(*output_dims)",
    "167": "def spherical_to_lat_lon(phi: np.ndarray, theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    lon = np.mod(np.rad2deg(phi), 360)\n    lat = 90 - np.rad2deg(theta)\n    return (lat, lon)",
    "169": "def spherical_to_cartesian(phi: np.ndarray, theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    return (np.cos(phi) * np.sin(theta), np.sin(phi) * np.sin(theta), np.cos(theta))",
    "171": "def get_rotation_matrices_to_local_coordinates(reference_phi: np.ndarray, reference_theta: np.ndarray, rotate_latitude: bool, rotate_longitude: bool) -> np.ndarray:\n    \"\"\"Returns a rotation matrix to rotate to a point based on a reference vector.\n\n  The rotation matrix is build such that, a vector in the\n  same coordinate system at the reference point that points towards the pole\n  before the rotation, continues to point towards the pole after the rotation.\n\n  Args:\n    reference_phi: [leading_axis] Polar angles of the reference.\n    reference_theta: [leading_axis] Azimuthal angles of the reference.\n    rotate_latitude: Whether to produce a rotation matrix that would rotate\n        R^3 vectors to zero latitude.\n    rotate_longitude: Whether to produce a rotation matrix that would rotate\n        R^3 vectors to zero longitude.\n\n  Returns:\n    Matrices of shape [leading_axis] such that when applied to the reference\n        position with `rotate_with_matrices(rotation_matrices, reference_pos)`\n\n        * phi goes to 0. if \"rotate_longitude\" is True.\n\n        * theta goes to np.pi / 2 if \"rotate_latitude\" is True.\n\n        The rotation consists of:\n        * rotate_latitude = False, rotate_longitude = True:\n            Latitude preserving rotation.\n        * rotate_latitude = True, rotate_longitude = True:\n            Latitude preserving rotation, followed by longitude preserving\n            rotation.\n        * rotate_latitude = True, rotate_longitude = False:\n            Latitude preserving rotation, followed by longitude preserving\n            rotation, and the inverse of the latitude preserving rotation. Note\n            this is computationally different from rotating the longitude only\n            and is. We do it like this, so the polar geodesic curve, continues\n            to be aligned with one of the axis after the rotation.\n\n  \"\"\"\n    if rotate_longitude and rotate_latitude:\n        azimuthal_rotation = -reference_phi\n        polar_rotation = -reference_theta + np.pi / 2\n        return transform.Rotation.from_euler('zy', np.stack([azimuthal_rotation, polar_rotation], axis=1)).as_matrix()\n    elif rotate_longitude:\n        azimuthal_rotation = -reference_phi\n        return transform.Rotation.from_euler('z', -reference_phi).as_matrix()\n    elif rotate_latitude:\n        azimuthal_rotation = -reference_phi\n        polar_rotation = -reference_theta + np.pi / 2\n        return transform.Rotation.from_euler('zyz', np.stack([azimuthal_rotation, polar_rotation, -azimuthal_rotation], axis=1)).as_matrix()\n    else:\n        raise ValueError('At least one of longitude and latitude should be rotated.')",
    "174": "def get_bipartite_relative_position_in_receiver_local_coordinates(senders_node_phi: np.ndarray, senders_node_theta: np.ndarray, senders: np.ndarray, receivers_node_phi: np.ndarray, receivers_node_theta: np.ndarray, receivers: np.ndarray, latitude_local_coordinates: bool, longitude_local_coordinates: bool) -> np.ndarray:\n    \"\"\"Returns relative position features for the edges.\n\n  This function is equivalent to\n  `get_relative_position_in_receiver_local_coordinates`, but adapted to work\n  with bipartite typed graphs.\n\n  The relative positions will be computed in a rotated space for a local\n  coordinate system as defined by the receiver. The relative positions are\n  simply obtained by subtracting sender position minues receiver position in\n  that local coordinate system after the rotation in R^3.\n\n  Args:\n    senders_node_phi: [num_sender_nodes] with polar angles.\n    senders_node_theta: [num_sender_nodes] with azimuthal angles.\n    senders: [num_edges] with indices into sender nodes.\n    receivers_node_phi: [num_sender_nodes] with polar angles.\n    receivers_node_theta: [num_sender_nodes] with azimuthal angles.\n    receivers: [num_edges] with indices into receiver nodes.\n    latitude_local_coordinates: Whether to rotate edges such that in the\n      positions are computed such that the receiver is always at latitude 0.\n    longitude_local_coordinates: Whether to rotate edges such that in the\n      positions are computed such that the receiver is always at longitude 0.\n\n  Returns:\n    Array of relative positions in R3 [num_edges, 3]\n  \"\"\"\n    senders_node_pos = np.stack(spherical_to_cartesian(senders_node_phi, senders_node_theta), axis=-1)\n    receivers_node_pos = np.stack(spherical_to_cartesian(receivers_node_phi, receivers_node_theta), axis=-1)\n    if not (latitude_local_coordinates or longitude_local_coordinates):\n        return senders_node_pos[senders] - receivers_node_pos[receivers]\n    receiver_rotation_matrices = get_rotation_matrices_to_local_coordinates(reference_phi=receivers_node_phi, reference_theta=receivers_node_theta, rotate_latitude=latitude_local_coordinates, rotate_longitude=longitude_local_coordinates)\n    edge_rotation_matrices = receiver_rotation_matrices[receivers]\n    receiver_pos_in_rotated_space = rotate_with_matrices(edge_rotation_matrices, receivers_node_pos[receivers])\n    sender_pos_in_in_rotated_space = rotate_with_matrices(edge_rotation_matrices, senders_node_pos[senders])\n    return sender_pos_in_in_rotated_space - receiver_pos_in_rotated_space",
    "175": "def variable_to_stacked(variable: xarray.Variable, sizes: Mapping[str, int], preserved_dims: Tuple[str, ...]=('batch', 'lat', 'lon')) -> xarray.Variable:\n    \"\"\"Converts an xarray.Variable to preserved_dims + (\"channels\",).\n\n  Any dimensions other than those included in preserved_dims get stacked into a\n  final \"channels\" dimension. If any of the preserved_dims are missing then they\n  are added, with the data broadcast/tiled to match the sizes specified in\n  `sizes`.\n\n  Args:\n    variable: An xarray.Variable.\n    sizes: Mapping including sizes for any dimensions which are not present in\n      `variable` but are needed for the output. This may be needed for example\n      for a static variable with only (\"lat\", \"lon\") dims, or if you want to\n      encode just the latitude coordinates (a variable with dims (\"lat\",)).\n    preserved_dims: dimensions of variable to not be folded in channels.\n\n  Returns:\n    An xarray.Variable with dimensions preserved_dims + (\"channels\",).\n  \"\"\"\n    stack_to_channels_dims = [d for d in variable.dims if d not in preserved_dims]\n    if stack_to_channels_dims:\n        variable = variable.stack(channels=stack_to_channels_dims)\n    dims = {dim: variable.sizes.get(dim) or sizes[dim] for dim in preserved_dims}\n    dims['channels'] = variable.sizes.get('channels', 1)\n    return variable.set_dims(dims)",
    "176": "def dataset_to_stacked(dataset: xarray.Dataset, sizes: Optional[Mapping[str, int]]=None, preserved_dims: Tuple[str, ...]=('batch', 'lat', 'lon')) -> xarray.DataArray:\n    \"\"\"Converts an xarray.Dataset to a single stacked array.\n\n  This takes each consistuent data_var, converts it into BHWC layout\n  using `variable_to_stacked`, then concats them all along the channels axis.\n\n  Args:\n    dataset: An xarray.Dataset.\n    sizes: Mapping including sizes for any dimensions which are not present in\n      the `dataset` but are needed for the output. See variable_to_stacked.\n    preserved_dims: dimensions from the dataset that should not be folded in\n      the predictions channels.\n\n  Returns:\n    An xarray.DataArray with dimensions preserved_dims + (\"channels\",).\n    Existing coordinates for preserved_dims axes will be preserved, however\n    there will be no coordinates for \"channels\".\n  \"\"\"\n    data_vars = [variable_to_stacked(dataset.variables[name], sizes or dataset.sizes, preserved_dims) for name in sorted(dataset.data_vars.keys())]\n    coords = {dim: coord for (dim, coord) in dataset.coords.items() if dim in preserved_dims}\n    return xarray.DataArray(data=xarray.Variable.concat(data_vars, dim='channels'), coords=coords)",
    "178": "def normalize(values: xarray.Dataset, scales: xarray.Dataset, locations: Optional[xarray.Dataset]) -> xarray.Dataset:\n    \"\"\"Normalize variables using the given scales and (optionally) locations.\"\"\"\n\n    def normalize_array(array):\n        if array.name is None:\n            raise ValueError(\"Can't look up normalization constants because array has no name.\")\n        if locations is not None:\n            if array.name in locations:\n                array = array - locations[array.name].astype(array.dtype)\n            else:\n                logging.warning('No normalization location found for %s', array.name)\n        if array.name in scales:\n            array = array / scales[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization scale found for %s', array.name)\n        return array\n    return xarray_tree.map_structure(normalize_array, values)",
    "179": "def normalize_array(array):\n    if array.name is None:\n        raise ValueError(\"Can't look up normalization constants because array has no name.\")\n    if locations is not None:\n        if array.name in locations:\n            array = array - locations[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization location found for %s', array.name)\n    if array.name in scales:\n        array = array / scales[array.name].astype(array.dtype)\n    else:\n        logging.warning('No normalization scale found for %s', array.name)\n    return array",
    "180": "def unnormalize(values: xarray.Dataset, scales: xarray.Dataset, locations: Optional[xarray.Dataset]) -> xarray.Dataset:\n    \"\"\"Unnormalize variables using the given scales and (optionally) locations.\"\"\"\n\n    def unnormalize_array(array):\n        if array.name is None:\n            raise ValueError(\"Can't look up normalization constants because array has no name.\")\n        if array.name in scales:\n            array = array * scales[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization scale found for %s', array.name)\n        if locations is not None:\n            if array.name in locations:\n                array = array + locations[array.name].astype(array.dtype)\n            else:\n                logging.warning('No normalization location found for %s', array.name)\n        return array\n    return xarray_tree.map_structure(unnormalize_array, values)",
    "181": "def unnormalize_array(array):\n    if array.name is None:\n        raise ValueError(\"Can't look up normalization constants because array has no name.\")\n    if array.name in scales:\n        array = array * scales[array.name].astype(array.dtype)\n    else:\n        logging.warning('No normalization scale found for %s', array.name)\n    if locations is not None:\n        if array.name in locations:\n            array = array + locations[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization location found for %s', array.name)\n    return array",
    "183": "def __init__(self, predictor: predictor_base.Predictor, stddev_by_level: xarray.Dataset, mean_by_level: xarray.Dataset, diffs_stddev_by_level: xarray.Dataset):\n    self._predictor = predictor\n    self._scales = stddev_by_level\n    self._locations = mean_by_level\n    self._residual_scales = diffs_stddev_by_level\n    self._residual_locations = None",
    "184": "def _unnormalize_prediction_and_add_input(self, inputs, norm_prediction):\n    if norm_prediction.sizes.get('time') != 1:\n        raise ValueError('normalization.InputsAndResiduals only supports predicting a single timestep.')\n    if norm_prediction.name in inputs:\n        prediction = unnormalize(norm_prediction, self._residual_scales, self._residual_locations)\n        last_input = inputs[norm_prediction.name].isel(time=-1)\n        prediction = prediction + last_input\n        return prediction\n    else:\n        return unnormalize(norm_prediction, self._scales, self._locations)",
    "185": "def _subtract_input_and_normalize_target(self, inputs, target):\n    if target.sizes.get('time') != 1:\n        raise ValueError('normalization.InputsAndResiduals only supports wrapping predictorsthat predict a single timestep.')\n    if target.name in inputs:\n        target_residual = target\n        last_input = inputs[target.name].isel(time=-1)\n        target_residual = target_residual - last_input\n        return normalize(target_residual, self._residual_scales, self._residual_locations)\n    else:\n        return normalize(target, self._scales, self._locations)",
    "186": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_predictions = self._predictor(norm_inputs, targets_template, forcings=norm_forcings, **kwargs)\n    return xarray_tree.map_structure(lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred), norm_predictions)",
    "187": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n    \"\"\"Returns the loss computed on normalized inputs and targets.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(lambda t: self._subtract_input_and_normalize_target(inputs, t), targets)\n    return self._predictor.loss(norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)",
    "188": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    \"\"\"The loss computed on normalized data, with unnormalized predictions.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(lambda t: self._subtract_input_and_normalize_target(inputs, t), targets)\n    ((loss, scalars), norm_predictions) = self._predictor.loss_and_predictions(norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n    predictions = xarray_tree.map_structure(lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred), norm_predictions)\n    return ((loss, scalars), predictions)",
    "189": "class Predictor(abc.ABC):\n    \"\"\"A possibly-trainable predictor of weather, exposing an xarray-based API.\n\n  Typically wraps an underlying JAX model and handles translating the xarray\n  Dataset values to and from plain JAX arrays that are convenient for input to\n  (and output from) the underlying model.\n\n  Different subclasses may exist to wrap different kinds of underlying model,\n  e.g. models taking stacked inputs/outputs, models taking separate 2D and 3D\n  inputs/outputs, autoregressive models.\n\n  You can also implement a specific model directly as a Predictor if you want,\n  for example if it has quite specific/unique requirements for its input/output\n  or loss function, or if it's convenient to implement directly using xarray.\n  \"\"\"\n\n    @abc.abstractmethod\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> xarray.Dataset:\n        \"\"\"Makes predictions.\n\n    This is only used by the Experiment for inference / evaluation, with\n    training going via the .loss method. So it should default to making\n    predictions for evaluation, although you can also support making predictions\n    for use in the loss via an is_training argument -- see\n    LossFunctionPredictor which helps with that.\n\n    Args:\n      inputs: An xarray.Dataset of inputs.\n      targets_template: An xarray.Dataset or other mapping of xarray.DataArrays,\n        with the same shape as the targets, to demonstrate what kind of\n        predictions are required. You can use this to determine which variables,\n        levels and lead times must be predicted.\n        You are free to raise an error if you don't support predicting what is\n        requested.\n      forcings: An xarray.Dataset of forcings terms. Forcings are variables\n        that can be fed to the model, but do not need to be predicted. This is\n        often because this variable can be computed analytically (e.g. the toa\n        radiation of the sun is mostly a function of geometry) or are considered\n        to be controlled for the experiment (e.g., impose a scenario of C02\n        emission into the atmosphere). Unlike `inputs`, the `forcings` can\n        include information \"from the future\", that is, information at target\n        times specified in the `targets_template`.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      Predictions, as an xarray.Dataset or other mapping of DataArrays which\n      is capable of being evaluated against targets with shape given by\n      targets_template.\n      For probabilistic predictors which can return multiple samples from a\n      predictive distribution, these should (by convention) be returned along\n      an additional 'sample' dimension.\n    \"\"\"\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n        \"\"\"Computes a training loss, for predictors that are trainable.\n\n    Why make this the Predictor's responsibility, rather than letting callers\n    compute their own loss function using predictions obtained from\n    Predictor.__call__?\n\n    Doing it this way gives Predictors more control over their training setup.\n    For example, some predictors may wish to train using different targets to\n    the ones they predict at evaluation time -- perhaps different lead times and\n    variables, perhaps training to predict transformed versions of targets\n    where the transform needs to be inverted at evaluation time, etc.\n\n    It's also necessary for generative models (VAEs, GANs, ...) where the\n    training loss is more complex and isn't expressible as a parameter-free\n    function of predictions and targets.\n\n    Args:\n      inputs: An xarray.Dataset.\n      targets: An xarray.Dataset or other mapping of xarray.DataArrays. See\n        docs on __call__ for an explanation about the targets.\n      forcings: xarray.Dataset of forcing terms.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n        You need not include the loss itself in this dict; it will be added for\n        you.\n    \"\"\"\n        del targets, forcings, optional_kwargs\n        batch_size = inputs.sizes['batch']\n        dummy_loss = xarray_jax.DataArray(jnp.zeros(batch_size), dims=('batch',))\n        return (dummy_loss, {})\n\n    def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> Tuple[LossAndDiagnostics, xarray.Dataset]:\n        \"\"\"Like .loss but also returns corresponding predictions.\n\n    Implementing this is optional as it's not used directly by the Experiment,\n    but it is required by autoregressive.Predictor when applying an inner\n    Predictor autoregressively at training time; we need a loss at each step but\n    also predictions to feed back in for the next step.\n\n    Note the loss itself may not be directly regressing the predictions towards\n    targets, the loss may be computed in terms of transformed predictions and\n    targets (or in some other way). For this reason we can't always cleanly\n    separate this into step 1: get predictions, step 2: compute loss from them,\n    hence the need for this combined method.\n\n    Args:\n      inputs:\n      targets:\n      forcings:\n      **optional_kwargs:\n        As for self.loss.\n\n    Returns:\n      (loss, diagnostics)\n        As for self.loss\n      predictions:\n        The predictions which the loss relates to. These should be of the same\n        shape as what you would get from\n        `self.__call__(inputs, targets_template=targets)`, and should be in the\n        same 'domain' as the inputs (i.e. they shouldn't be transformed\n        differently to how the predictor expects its inputs).\n    \"\"\"\n        raise NotImplementedError",
    "191": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n    \"\"\"Computes a training loss, for predictors that are trainable.\n\n    Why make this the Predictor's responsibility, rather than letting callers\n    compute their own loss function using predictions obtained from\n    Predictor.__call__?\n\n    Doing it this way gives Predictors more control over their training setup.\n    For example, some predictors may wish to train using different targets to\n    the ones they predict at evaluation time -- perhaps different lead times and\n    variables, perhaps training to predict transformed versions of targets\n    where the transform needs to be inverted at evaluation time, etc.\n\n    It's also necessary for generative models (VAEs, GANs, ...) where the\n    training loss is more complex and isn't expressible as a parameter-free\n    function of predictions and targets.\n\n    Args:\n      inputs: An xarray.Dataset.\n      targets: An xarray.Dataset or other mapping of xarray.DataArrays. See\n        docs on __call__ for an explanation about the targets.\n      forcings: xarray.Dataset of forcing terms.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n        You need not include the loss itself in this dict; it will be added for\n        you.\n    \"\"\"\n    del targets, forcings, optional_kwargs\n    batch_size = inputs.sizes['batch']\n    dummy_loss = xarray_jax.DataArray(jnp.zeros(batch_size), dims=('batch',))\n    return (dummy_loss, {})",
    "192": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> Tuple[LossAndDiagnostics, xarray.Dataset]:\n    \"\"\"Like .loss but also returns corresponding predictions.\n\n    Implementing this is optional as it's not used directly by the Experiment,\n    but it is required by autoregressive.Predictor when applying an inner\n    Predictor autoregressively at training time; we need a loss at each step but\n    also predictions to feed back in for the next step.\n\n    Note the loss itself may not be directly regressing the predictions towards\n    targets, the loss may be computed in terms of transformed predictions and\n    targets (or in some other way). For this reason we can't always cleanly\n    separate this into step 1: get predictions, step 2: compute loss from them,\n    hence the need for this combined method.\n\n    Args:\n      inputs:\n      targets:\n      forcings:\n      **optional_kwargs:\n        As for self.loss.\n\n    Returns:\n      (loss, diagnostics)\n        As for self.loss\n      predictions:\n        The predictions which the loss relates to. These should be of the same\n        shape as what you would get from\n        `self.__call__(inputs, targets_template=targets)`, and should be in the\n        same 'domain' as the inputs (i.e. they shouldn't be transformed\n        differently to how the predictor expects its inputs).\n    \"\"\"\n    raise NotImplementedError",
    "194": "def __call__(self, rng: chex.PRNGKey, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> xarray.Dataset:\n    ...",
    "195": "def chunked_prediction(predictor_fn: PredictorFn, rng: chex.PRNGKey, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, num_steps_per_chunk: int=1, verbose: bool=False) -> xarray.Dataset:\n    \"\"\"Outputs a long trajectory by iteratively concatenating chunked predictions.\n\n  Args:\n    predictor_fn: Function to use to make predictions for each chunk.\n    rng: Random key.\n    inputs: Inputs for the model.\n    targets_template: Template for the target prediction, requires targets\n        equispaced in time.\n    forcings: Optional forcing for the model.\n    num_steps_per_chunk: How many of the steps in `targets_template` to predict\n        at each call of `predictor_fn`. It must evenly divide the number of\n        steps in `targets_template`.\n    verbose: Whether to log the current chunk being predicted.\n\n  Returns:\n    Predictions for the targets template.\n\n  \"\"\"\n    chunks_list = []\n    for prediction_chunk in chunked_prediction_generator(predictor_fn=predictor_fn, rng=rng, inputs=inputs, targets_template=targets_template, forcings=forcings, num_steps_per_chunk=num_steps_per_chunk, verbose=verbose):\n        chunks_list.append(jax.device_get(prediction_chunk))\n    return xarray.concat(chunks_list, dim='time')",
    "197": "def _get_next_inputs(prev_inputs: xarray.Dataset, next_frame: xarray.Dataset) -> xarray.Dataset:\n    \"\"\"Computes next inputs, from previous inputs and predictions.\"\"\"\n    non_predicted_or_forced_inputs = list(set(prev_inputs.keys()) - set(next_frame.keys()))\n    if 'time' in prev_inputs[non_predicted_or_forced_inputs].dims:\n        raise ValueError('Found an input with a time index that is not predicted or forced.')\n    next_inputs_keys = list(set(next_frame.keys()).intersection(set(prev_inputs.keys())))\n    next_inputs = next_frame[next_inputs_keys]\n    num_inputs = prev_inputs.dims['time']\n    return xarray.concat([prev_inputs, next_inputs], dim='time', data_vars='different').tail(time=num_inputs)",
    "198": "def extend_targets_template(targets_template: xarray.Dataset, required_num_steps: int) -> xarray.Dataset:\n    \"\"\"Extends `targets_template` to `required_num_steps` with lazy arrays.\n\n  It uses lazy dask arrays of zeros, so it does not require instantiating the\n  array in memory.\n\n  Args:\n    targets_template: Input template to extend.\n    required_num_steps: Number of steps required in the returned template.\n\n  Returns:\n    `xarray.Dataset` identical in variables and timestep to `targets_template`\n    full of `dask.array.zeros` such that the time axis has `required_num_steps`.\n\n  \"\"\"\n    time = targets_template.coords['time']\n    timestep = time[0].data\n    if time.shape[0] > 1:\n        assert np.all(timestep == time[1:] - time[:-1])\n    extended_time = (np.arange(required_num_steps) + 1) * timestep\n    if 'datetime' in targets_template.coords:\n        datetime = targets_template.coords['datetime']\n        extended_datetime = datetime[0].data - timestep + extended_time\n    else:\n        extended_datetime = None\n    datetime = targets_template.coords['time']\n\n    def extend_time(data_array: xarray.DataArray) -> xarray.DataArray:\n        dims = data_array.dims\n        shape = list(data_array.shape)\n        shape[dims.index('time')] = required_num_steps\n        dask_data = dask.array.zeros(shape=tuple(shape), chunks=-1, dtype=data_array.dtype)\n        coords = dict(data_array.coords)\n        coords['time'] = extended_time\n        if extended_datetime is not None:\n            coords['datetime'] = ('time', extended_datetime)\n        return xarray.DataArray(dims=dims, data=dask_data, coords=coords)\n    return xarray_tree.map_structure(extend_time, targets_template)",
    "199": "def extend_time(data_array: xarray.DataArray) -> xarray.DataArray:\n    dims = data_array.dims\n    shape = list(data_array.shape)\n    shape[dims.index('time')] = required_num_steps\n    dask_data = dask.array.zeros(shape=tuple(shape), chunks=-1, dtype=data_array.dtype)\n    coords = dict(data_array.coords)\n    coords['time'] = extended_time\n    if extended_datetime is not None:\n        coords['datetime'] = ('time', extended_datetime)\n    return xarray.DataArray(dims=dims, data=dask_data, coords=coords)",
    "200": "def _get_grid_lat_lon_coords(num_lat: int, num_lon: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generates a linear latitude-longitude grid of the given size.\n\n  Args:\n    num_lat: Size of the latitude dimension of the grid.\n    num_lon: Size of the longitude dimension of the grid.\n\n  Returns:\n    A tuple `(lat, lon)` containing 1D arrays with the latitude and longitude\n    coordinates in degrees of the generated grid.\n  \"\"\"\n    lat = np.linspace(-90.0, 90.0, num=num_lat, endpoint=True)\n    lon = np.linspace(0.0, 360.0, num=num_lon, endpoint=False)\n    return (lat, lon)",
    "203": "def test_missing_dim_raises_value_error(self):\n    data = xa.DataArray(np.random.randn(2, 2), coords=[np.array([0.1, 0.2]), np.array([0.0, 0.5])], dims=['lon', 'x'])\n    with self.assertRaisesRegex(ValueError, '.* dimensions are missing in `data_array_like`.'):\n        solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=360)",
    "204": "def test_missing_coordinate_raises_value_error(self):\n    data = xa.Dataset(data_vars={'var1': (['x', 'lat', 'lon'], np.random.randn(2, 3, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2]), 'lon': np.array([0.0, 0.5])})\n    with self.assertRaisesRegex(ValueError, '.* coordinates are missing in `data_array_like`.'):\n        solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=360)",
    "205": "def test_shape_multiple_timestamps(self):\n    data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.random.randn(2, 4, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2, 0.3]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n    actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=2)\n    self.assertEqual(('time', 'lat', 'lon'), actual.dims)\n    self.assertEqual((2, 4, 2), actual.shape)",
    "206": "def test_shape_single_timestamp(self):\n    data = xa.Dataset(data_vars={'var1': (['lat', 'lon'], np.random.randn(4, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2, 0.3]), 'lon': np.array([0.0, 0.5]), 'datetime': np.datetime64(10, 'D')})\n    actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=2)\n    self.assertEqual(('lat', 'lon'), actual.dims)\n    self.assertEqual((4, 2), actual.shape)",
    "207": "@parameterized.named_parameters(dict(testcase_name='one_timestamp_jitted', periods=1, repeats=3, use_jit=True), dict(testcase_name='one_timestamp_non_jitted', periods=1, repeats=3, use_jit=False), dict(testcase_name='ten_timestamps_non_jitted', periods=10, repeats=1, use_jit=False))\ndef test_full_spatial_resolution(self, periods: int, repeats: int, use_jit: bool):\n    timestamps = pd.date_range(start='2023-09-25', periods=periods, freq='6h')\n    (lat, lon) = _get_grid_lat_lon_coords(num_lat=721, num_lon=1440)\n\n    def benchmark() -> None:\n        solar_radiation.get_toa_incident_solar_radiation(timestamps, lat, lon, integration_period='1h', num_integration_bins=360, use_jit=use_jit).block_until_ready()\n    results = timeit.repeat(benchmark, repeat=repeats, number=1)\n    logging.info('Times to compute `tisr` for input of shape `%d, %d, %d` (seconds): %s', len(timestamps), len(lat), len(lon), np.array2string(np.array(results), precision=1))",
    "210": "@parameterized.named_parameters(dict(testcase_name='reference_tsi_data', loader=solar_radiation.reference_tsi_data, expected_tsi=np.array([1361.0])), dict(testcase_name='era5_tsi_data', loader=solar_radiation.era5_tsi_data, expected_tsi=np.array([1360.944])))\ndef test_mid_2020_lookup(self, loader: solar_radiation.TsiDataLoader, expected_tsi: np.ndarray):\n    tsi_data = loader()\n    tsi = solar_radiation.get_tsi([np.datetime64('2020-07-02T00:00:00')], tsi_data)\n    np.testing.assert_allclose(expected_tsi, tsi)",
    "211": "@parameterized.named_parameters(dict(testcase_name='beginning_2020_left_boundary', timestamps=[np.datetime64('2020-01-01T00:00:00')], expected_tsi=np.array([1000.0])), dict(testcase_name='mid_2020_exact', timestamps=[np.datetime64('2020-07-02T00:00:00')], expected_tsi=np.array([1000.0])), dict(testcase_name='beginning_2021_interpolated', timestamps=[np.datetime64('2021-01-01T00:00:00')], expected_tsi=np.array([1150.0])), dict(testcase_name='mid_2021_lookup', timestamps=[np.datetime64('2021-07-02T12:00:00')], expected_tsi=np.array([1300.0])), dict(testcase_name='beginning_2022_interpolated', timestamps=[np.datetime64('2022-01-01T00:00:00')], expected_tsi=np.array([1250.0])), dict(testcase_name='mid_2022_lookup', timestamps=[np.datetime64('2022-07-02T12:00:00')], expected_tsi=np.array([1200.0])), dict(testcase_name='beginning_2023_right_boundary', timestamps=[np.datetime64('2023-01-01T00:00:00')], expected_tsi=np.array([1200.0])))\ndef test_interpolation(self, timestamps: Sequence[np.datetime64], expected_tsi: np.ndarray):\n    tsi_data = xa.DataArray(np.array([1000.0, 1300.0, 1200.0]), dims=['time'], coords={'time': np.array([2020.5, 2021.5, 2022.5])})\n    tsi = solar_radiation.get_tsi(timestamps, tsi_data)\n    np.testing.assert_allclose(expected_tsi, tsi)",
    "213": "class EdgesIndices(NamedTuple):\n    \"\"\"Represents indices to nodes adjacent to the edges.\"\"\"\n    senders: ArrayLike\n    receivers: ArrayLike",
    "215": "class Context(NamedTuple):\n    n_graph: ArrayLike\n    features: ArrayLikeTree",
    "217": "class TypedGraph(NamedTuple):\n    \"\"\"A graph with typed nodes and edges.\n\n  A typed graph is made of a context, multiple sets of nodes and multiple\n  sets of edges connecting those nodes (as indicated by the EdgeSetKey).\n  \"\"\"\n    context: Context\n    nodes: Mapping[str, NodeSet]\n    edges: Mapping[EdgeSetKey, EdgeSet]\n\n    def edge_key_by_name(self, name: str) -> EdgeSetKey:\n        found_key = [k for k in self.edges.keys() if k.name == name]\n        if len(found_key) != 1:\n            raise KeyError(\"invalid edge key '{}'. Available edges: [{}]\".format(name, ', '.join((x.name for x in self.edges.keys()))))\n        return found_key[0]\n\n    def edge_by_name(self, name: str) -> EdgeSet:\n        return self.edges[self.edge_key_by_name(name)]",
    "218": "def edge_key_by_name(self, name: str) -> EdgeSetKey:\n    found_key = [k for k in self.edges.keys() if k.name == name]\n    if len(found_key) != 1:\n        raise KeyError(\"invalid edge key '{}'. Available edges: [{}]\".format(name, ', '.join((x.name for x in self.edges.keys()))))\n    return found_key[0]",
    "220": "def GraphNetwork(update_edge_fn: Mapping[str, jraph.GNUpdateEdgeFn], update_node_fn: Mapping[str, GNUpdateNodeFn], update_global_fn: Optional[GNUpdateGlobalFn]=None, aggregate_edges_for_nodes_fn: jraph.AggregateEdgesToNodesFn=jraph.segment_sum, aggregate_nodes_for_globals_fn: jraph.AggregateNodesToGlobalsFn=jraph.segment_sum, aggregate_edges_for_globals_fn: jraph.AggregateEdgesToGlobalsFn=jraph.segment_sum):\n    \"\"\"Returns a method that applies a configured GraphNetwork.\n\n  This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n  extended to Typed Graphs with multiple edge sets and node sets and extended to\n  allow aggregating not only edges received by the nodes, but also edges sent by\n  the nodes.\n\n  Example usage::\n\n    gn = GraphNetwork(update_edge_function,\n    update_node_function, **kwargs)\n    # Conduct multiple rounds of message passing with the same parameters:\n    for _ in range(num_message_passing_steps):\n      graph = gn(graph)\n\n  Args:\n    update_edge_fn: mapping of functions used to update a subset of the edge\n      types, indexed by edge type name.\n    update_node_fn: mapping of functions used to update a subset of the node\n      types, indexed by node type name.\n    update_global_fn: function used to update the globals or None to deactivate\n      globals updates.\n    aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n      node.\n    aggregate_nodes_for_globals_fn: function used to aggregate the nodes for the\n      globals.\n    aggregate_edges_for_globals_fn: function used to aggregate the edges for the\n      globals.\n\n  Returns:\n    A method that applies the configured GraphNetwork.\n  \"\"\"\n\n    def _apply_graph_net(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Applies a configured GraphNetwork to a graph.\n\n    This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n    extended to Typed Graphs with multiple edge sets and node sets and extended\n    to allow aggregating not only edges received by the nodes, but also edges\n    sent by the nodes.\n\n    Args:\n      graph: a `TypedGraph` containing the graph.\n\n    Returns:\n      Updated `TypedGraph`.\n    \"\"\"\n        updated_graph = graph\n        updated_edges = dict(updated_graph.edges)\n        for (edge_set_name, edge_fn) in update_edge_fn.items():\n            edge_set_key = graph.edge_key_by_name(edge_set_name)\n            updated_edges[edge_set_key] = _edge_update(updated_graph, edge_fn, edge_set_key)\n        updated_graph = updated_graph._replace(edges=updated_edges)\n        updated_nodes = dict(updated_graph.nodes)\n        for (node_set_key, node_fn) in update_node_fn.items():\n            updated_nodes[node_set_key] = _node_update(updated_graph, node_fn, node_set_key, aggregate_edges_for_nodes_fn)\n        updated_graph = updated_graph._replace(nodes=updated_nodes)\n        if update_global_fn:\n            updated_context = _global_update(updated_graph, update_global_fn, aggregate_edges_for_globals_fn, aggregate_nodes_for_globals_fn)\n            updated_graph = updated_graph._replace(context=updated_context)\n        return updated_graph\n    return _apply_graph_net",
    "221": "def _apply_graph_net(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Applies a configured GraphNetwork to a graph.\n\n    This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n    extended to Typed Graphs with multiple edge sets and node sets and extended\n    to allow aggregating not only edges received by the nodes, but also edges\n    sent by the nodes.\n\n    Args:\n      graph: a `TypedGraph` containing the graph.\n\n    Returns:\n      Updated `TypedGraph`.\n    \"\"\"\n    updated_graph = graph\n    updated_edges = dict(updated_graph.edges)\n    for (edge_set_name, edge_fn) in update_edge_fn.items():\n        edge_set_key = graph.edge_key_by_name(edge_set_name)\n        updated_edges[edge_set_key] = _edge_update(updated_graph, edge_fn, edge_set_key)\n    updated_graph = updated_graph._replace(edges=updated_edges)\n    updated_nodes = dict(updated_graph.nodes)\n    for (node_set_key, node_fn) in update_node_fn.items():\n        updated_nodes[node_set_key] = _node_update(updated_graph, node_fn, node_set_key, aggregate_edges_for_nodes_fn)\n    updated_graph = updated_graph._replace(nodes=updated_nodes)\n    if update_global_fn:\n        updated_context = _global_update(updated_graph, update_global_fn, aggregate_edges_for_globals_fn, aggregate_nodes_for_globals_fn)\n        updated_graph = updated_graph._replace(context=updated_context)\n    return updated_graph",
    "222": "def _edge_update(graph, edge_fn, edge_set_key):\n    \"\"\"Updates an edge set of a given key.\"\"\"\n    sender_nodes = graph.nodes[edge_set_key.node_sets[0]]\n    receiver_nodes = graph.nodes[edge_set_key.node_sets[1]]\n    edge_set = graph.edges[edge_set_key]\n    senders = edge_set.indices.senders\n    receivers = edge_set.indices.receivers\n    sent_attributes = tree.tree_map(lambda n: n[senders], sender_nodes.features)\n    received_attributes = tree.tree_map(lambda n: n[receivers], receiver_nodes.features)\n    n_edge = edge_set.n_edge\n    sum_n_edge = senders.shape[0]\n    global_features = tree.tree_map(lambda g: jnp.repeat(g, n_edge, axis=0, total_repeat_length=sum_n_edge), graph.context.features)\n    new_features = edge_fn(edge_set.features, sent_attributes, received_attributes, global_features)\n    return edge_set._replace(features=new_features)",
    "223": "def _node_update(graph, node_fn, node_set_key, aggregation_fn):\n    \"\"\"Updates an edge set of a given key.\"\"\"\n    node_set = graph.nodes[node_set_key]\n    sum_n_node = tree.tree_leaves(node_set.features)[0].shape[0]\n    sent_features = {}\n    for (edge_set_key, edge_set) in graph.edges.items():\n        sender_node_set_key = edge_set_key.node_sets[0]\n        if sender_node_set_key == node_set_key:\n            assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n            senders = edge_set.indices.senders\n            sent_features[edge_set_key.name] = tree.tree_map(lambda e: aggregation_fn(e, senders, sum_n_node), edge_set.features)\n    received_features = {}\n    for (edge_set_key, edge_set) in graph.edges.items():\n        receiver_node_set_key = edge_set_key.node_sets[1]\n        if receiver_node_set_key == node_set_key:\n            assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n            receivers = edge_set.indices.receivers\n            received_features[edge_set_key.name] = tree.tree_map(lambda e: aggregation_fn(e, receivers, sum_n_node), edge_set.features)\n    n_node = node_set.n_node\n    global_features = tree.tree_map(lambda g: jnp.repeat(g, n_node, axis=0, total_repeat_length=sum_n_node), graph.context.features)\n    new_features = node_fn(node_set.features, sent_features, received_features, global_features)\n    return node_set._replace(features=new_features)",
    "224": "def _global_update(graph, global_fn, edge_aggregation_fn, node_aggregation_fn):\n    \"\"\"Updates an edge set of a given key.\"\"\"\n    n_graph = graph.context.n_graph.shape[0]\n    graph_idx = jnp.arange(n_graph)\n    edge_features = {}\n    for (edge_set_key, edge_set) in graph.edges.items():\n        assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n        sum_n_edge = edge_set.indices.senders.shape[0]\n        edge_gr_idx = jnp.repeat(graph_idx, edge_set.n_edge, axis=0, total_repeat_length=sum_n_edge)\n        edge_features[edge_set_key.name] = tree.tree_map(lambda e: edge_aggregation_fn(e, edge_gr_idx, n_graph), edge_set.features)\n    node_features = {}\n    for (node_set_key, node_set) in graph.nodes.items():\n        sum_n_node = tree.tree_leaves(node_set.features)[0].shape[0]\n        node_gr_idx = jnp.repeat(graph_idx, node_set.n_node, axis=0, total_repeat_length=sum_n_node)\n        node_features[node_set_key] = tree.tree_map(lambda n: node_aggregation_fn(n, node_gr_idx, n_graph), node_set.features)\n    new_features = global_fn(node_features, edge_features, graph.context.features)\n    return graph.context._replace(features=new_features)",
    "226": "def GraphMapFeatures(embed_edge_fn: Optional[Mapping[str, jraph.EmbedEdgeFn]]=None, embed_node_fn: Optional[Mapping[str, jraph.EmbedNodeFn]]=None, embed_global_fn: Optional[jraph.EmbedGlobalFn]=None):\n    \"\"\"Returns function which embeds the components of a graph independently.\n\n  Args:\n    embed_edge_fn: mapping of functions used to embed each edge type,\n      indexed by edge type name.\n    embed_node_fn: mapping of functions used to embed each node type,\n      indexed by node type name.\n    embed_global_fn: function used to embed the globals.\n  \"\"\"\n\n    def _embed(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        updated_edges = dict(graph.edges)\n        if embed_edge_fn:\n            for (edge_set_name, embed_fn) in embed_edge_fn.items():\n                edge_set_key = graph.edge_key_by_name(edge_set_name)\n                edge_set = graph.edges[edge_set_key]\n                updated_edges[edge_set_key] = edge_set._replace(features=embed_fn(edge_set.features))\n        updated_nodes = dict(graph.nodes)\n        if embed_node_fn:\n            for (node_set_key, embed_fn) in embed_node_fn.items():\n                node_set = graph.nodes[node_set_key]\n                updated_nodes[node_set_key] = node_set._replace(features=embed_fn(node_set.features))\n        updated_context = graph.context\n        if embed_global_fn:\n            updated_context = updated_context._replace(features=embed_global_fn(updated_context.features))\n        return graph._replace(edges=updated_edges, nodes=updated_nodes, context=updated_context)\n    return _embed",
    "227": "def _embed(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    updated_edges = dict(graph.edges)\n    if embed_edge_fn:\n        for (edge_set_name, embed_fn) in embed_edge_fn.items():\n            edge_set_key = graph.edge_key_by_name(edge_set_name)\n            edge_set = graph.edges[edge_set_key]\n            updated_edges[edge_set_key] = edge_set._replace(features=embed_fn(edge_set.features))\n    updated_nodes = dict(graph.nodes)\n    if embed_node_fn:\n        for (node_set_key, embed_fn) in embed_node_fn.items():\n            node_set = graph.nodes[node_set_key]\n            updated_nodes[node_set_key] = node_set._replace(features=embed_fn(node_set.features))\n    updated_context = graph.context\n    if embed_global_fn:\n        updated_context = updated_context._replace(features=embed_global_fn(updated_context.features))\n    return graph._replace(edges=updated_edges, nodes=updated_nodes, context=updated_context)",
    "229": "def DataArray(data, coords=None, dims=None, name=None, attrs=None, jax_coords=None) -> xarray.DataArray:\n    \"\"\"Like xarray.DataArray, but supports using JAX arrays.\n\n  Args:\n    data: As for xarray.DataArray, except jax arrays are also supported.\n    coords: Coordinates for the array, see xarray.DataArray. These coordinates\n      must be based on plain numpy arrays or something convertible to plain\n      numpy arrays. Their values will form a static part of the data structure\n      from the point of view of jax.tree_util. In particular this means these\n      coordinates will be passed as plain numpy arrays even inside a JIT'd\n      function, and the JIT'd function will be recompiled under the hood if the\n      coordinates of DataArrays passed into it change.\n      If this is not convenient for you, see also jax_coords below.\n    dims: See xarray.DataArray.\n    name: See xarray.DataArray.\n    attrs: See xarray.DataArray.\n    jax_coords: Additional coordinates, which *can* use JAX arrays. These\n      coordinates will be treated as JAX data from the point of view of\n      jax.tree_util, that means when JIT'ing they will be passed as tracers and\n      computation involving them will be JIT'd.\n      Unfortunately a side-effect of this is that they can't be used as index\n      coordinates (because xarray's indexing logic is not JIT-able). If you\n      specify a coordinate with the same name as a dimension here, it will not\n      be set as an index coordinate; this behaviour is different to the default\n      for `coords`, and it means that things like `.sel` based on the jax\n      coordinate will not work.\n      Note we require `jax_coords` to be explicitly specified via a different\n      constructor argument to `coords`, rather than just looking for jax arrays\n      within the `coords` and treating them differently. This is because it\n      affects the way jax.tree_util treats them, which is somewhat orthogonal to\n      whether the value is passed in as numpy or not, and generally needs to be\n      handled consistently so is something we encourage explicit control over.\n\n  Returns:\n    An instance of xarray.DataArray. Where JAX arrays are used as data or\n    coords, they will be wrapped with JaxArrayWrapper and can be unwrapped via\n    `unwrap` and `unwrap_data`.\n  \"\"\"\n    result = xarray.DataArray(wrap(data), dims=dims, name=name, attrs=attrs or {})\n    return assign_coords(result, coords=coords, jax_coords=jax_coords)",
    "231": "def assign_coords(x: DatasetOrDataArray, *, coords: Optional[Mapping[Hashable, Any]]=None, jax_coords: Optional[Mapping[Hashable, Any]]=None) -> DatasetOrDataArray:\n    \"\"\"Replacement for assign_coords which works in presence of jax_coords.\n\n  `jax_coords` allow certain specified coordinates to have their data passed as\n  JAX arrays (including through jax.jit boundaries). The compromise in return is\n  that they are not created as index coordinates and cannot be used for .sel\n  and other coordinate-based indexing operations. See docs for `jax_coords` on\n  xarray_jax.Dataset and xarray_jax.DataArray for more information.\n\n  This function can be used to set jax_coords on an existing DataArray or\n  Dataset, and also to set a mix of jax and non-jax coordinates. It implements\n  some workarounds to prevent xarray trying and failing to create IndexVariables\n  from jax arrays under the hood.\n\n  If you have any jax_coords with the same name as a dimension, you'll need to\n  use this function instead of data_array.assign_coords or dataset.assign_coords\n  in general, to avoid an xarray bug where it tries (and in our case fails) to\n  create indexes for existing jax coords. See\n  https://github.com/pydata/xarray/issues/7885.\n\n  Args:\n    x: An xarray Dataset or DataArray.\n    coords: Dict of (non-JAX) coords, or None if not assigning any.\n    jax_coords: Dict of JAX coords, or None if not assigning any. See docs for\n      xarray_jax.Dataset / DataArray for more information on jax_coords.\n\n  Returns:\n    The Dataset or DataArray with coordinates assigned, similarly to\n    Dataset.assign_coords / DataArray.assign_coords.\n  \"\"\"\n    coords = {} if coords is None else dict(coords)\n    jax_coords = {} if jax_coords is None else dict(jax_coords)\n    existing_jax_coords = get_jax_coords(x)\n    jax_coords = existing_jax_coords | jax_coords\n    x = x.drop_vars(existing_jax_coords.keys())\n    renamed_jax_coords = {}\n    for (name, coord) in jax_coords.items():\n        if isinstance(coord, xarray.DataArray):\n            coord = coord.variable\n        if isinstance(coord, xarray.Variable):\n            coord = coord.copy(deep=False)\n        else:\n            coord = Variable((name,), coord)\n        coord.attrs[_JAX_COORD_ATTR_NAME] = True\n        renamed_jax_coords[f'__NONINDEX_{name}'] = coord\n    x = x.assign_coords(coords=coords | renamed_jax_coords)\n    rename_back_mapping = {f'__NONINDEX_{name}': name for name in jax_coords}\n    if isinstance(x, xarray.Dataset):\n        return x.rename_vars(rename_back_mapping)\n    else:\n        return x.rename(rename_back_mapping)",
    "233": "def assign_jax_coords(x: DatasetOrDataArray, jax_coords: Optional[Mapping[Hashable, Any]]=None, **jax_coords_kwargs) -> DatasetOrDataArray:\n    \"\"\"Assigns only jax_coords, with same API as xarray's assign_coords.\"\"\"\n    return assign_coords(x, jax_coords=jax_coords or jax_coords_kwargs)",
    "235": "def unwrap(value, require_jax=False):\n    \"\"\"Unwraps wrapped JAX arrays used in xarray, passing through other values.\"\"\"\n    if isinstance(value, JaxArrayWrapper):\n        return value.jax_array\n    elif isinstance(value, jax.Array):\n        return value\n    elif require_jax:\n        raise TypeError(f'Expected JAX array, found {type(value)}.')\n    else:\n        return value",
    "236": "def _wrapped(func):\n    \"\"\"Surrounds a function with JAX array unwrapping/wrapping.\"\"\"\n\n    def wrapped_func(*args, **kwargs):\n        (args, kwargs) = tree.map_structure(unwrap, (args, kwargs))\n        result = func(*args, **kwargs)\n        return tree.map_structure(wrap, result)\n    return wrapped_func",
    "238": "def unwrap_data(value: Union[xarray.Variable, xarray.DataArray], require_jax: bool=False) -> Union[jax.Array, np.ndarray]:\n    \"\"\"The unwrapped (see unwrap) data of a an xarray.Variable or DataArray.\"\"\"\n    return unwrap(value.data, require_jax=require_jax)",
    "239": "def unwrap_vars(dataset: Mapping[Hashable, xarray.DataArray], require_jax: bool=False) -> Mapping[str, Union[jax.Array, np.ndarray]]:\n    \"\"\"The unwrapped data (see unwrap) of the variables in a dataset.\"\"\"\n    return {str(name): unwrap_data(var, require_jax=require_jax) for (name, var) in dataset.items()}",
    "240": "def unwrap_coords(dataset: Union[xarray.Dataset, xarray.DataArray], require_jax: bool=False) -> Mapping[str, Union[jax.Array, np.ndarray]]:\n    \"\"\"The unwrapped data (see unwrap) of the coords in a Dataset or DataArray.\"\"\"\n    return {str(name): unwrap_data(var, require_jax=require_jax) for (name, var) in dataset.coords.items()}",
    "242": "def jax_vars(dataset: Mapping[Hashable, xarray.DataArray]) -> Mapping[str, jax.Array]:\n    \"\"\"Like unwrap_vars, but will complain if vars are not all jax arrays.\"\"\"\n    return cast(Mapping[str, jax.Array], unwrap_vars(dataset, require_jax=True))",
    "245": "def __array_ufunc__(self, ufunc, method, *args, **kwargs):\n    for x in args:\n        if not isinstance(x, (jax.typing.ArrayLike, type(self))):\n            return NotImplemented\n    if method != '__call__':\n        return NotImplemented\n    try:\n        func = getattr(jnp, ufunc.__name__)\n    except AttributeError:\n        return NotImplemented\n    kwargs.pop('out', None)\n    return _wrapped(func)(*args, **kwargs)",
    "247": "def __repr__(self):\n    return f'xarray_jax.JaxArrayWrapper({repr(self.jax_array)})'",
    "249": "@property\ndef dtype(self):\n    return self.jax_array.dtype",
    "251": "@property\ndef size(self):\n    return self.jax_array.size",
    "253": "@property\ndef imag(self):\n    return self.jax_array.imag",
    "255": "def apply_ufunc(func, *args, require_jax=False, **apply_ufunc_kwargs):\n    \"\"\"Like xarray.apply_ufunc but for jax-specific ufuncs.\n\n  Many numpy ufuncs will work fine out of the box with xarray_jax and\n  JaxArrayWrapper, since JaxArrayWrapper quacks (mostly) like a numpy array and\n  will convert many numpy operations to jax ops under the hood. For these\n  situations, xarray.apply_ufunc should work fine.\n\n  But sometimes you need a jax-specific ufunc which needs to be given a\n  jax array as input or return a jax array as output. In that case you should\n  use this helper as it will remove any JaxArrayWrapper before calling the func,\n  and wrap the result afterwards before handing it back to xarray.\n\n  Args:\n    func: A function that works with jax arrays (e.g. using functions from\n      jax.numpy) but otherwise meets the spec for the func argument to\n      xarray.apply_ufunc.\n    *args: xarray arguments to be mapped to arguments for func\n      (see xarray.apply_ufunc).\n    require_jax: Whether to require that inputs are based on jax arrays or allow\n      those based on plain numpy arrays too.\n    **apply_ufunc_kwargs: See xarray.apply_ufunc.\n\n  Returns:\n    Corresponding xarray results (see xarray.apply_ufunc).\n  \"\"\"\n\n    def wrapped_func(*maybe_wrapped_args):\n        unwrapped_args = [unwrap(a, require_jax) for a in maybe_wrapped_args]\n        result = func(*unwrapped_args)\n        return jax.tree_util.tree_map(wrap, result)\n    return xarray.apply_ufunc(wrapped_func, *args, **apply_ufunc_kwargs)",
    "258": "def fn_passed_to_pmap(*flat_args):\n    assert input_treedef is not None\n\n    def check_and_remove_leading_dim(dims):\n        try:\n            index = dims.index(dim)\n        except ValueError:\n            index = None\n        if index != 0:\n            raise ValueError(f'Expected dim {dim} at index 0, found at {index}.')\n        return dims[1:]\n    with dims_change_on_unflatten(check_and_remove_leading_dim):\n        args = jax.tree_util.tree_unflatten(input_treedef, flat_args)\n    result = fn(*args)\n    nonlocal output_treedef\n    (flat_result, output_treedef) = jax.tree_util.tree_flatten(result)\n    return flat_result",
    "260": "def result_fn(*args):\n    nonlocal input_treedef\n    (flat_args, input_treedef) = jax.tree_util.tree_flatten(args)\n    flat_result = pmapped_fn(*flat_args)\n    assert output_treedef is not None\n    with dims_change_on_unflatten(lambda dims: (dim,) + dims):\n        return jax.tree_util.tree_unflatten(output_treedef, flat_result)",
    "261": "@contextlib.contextmanager\ndef dims_change_on_unflatten(dims_change_fn: DimsChangeFn):\n    \"\"\"Can be used to change the dims used when unflattening arrays into xarrays.\n\n  This is useful when some axes were added to / removed from the underlying jax\n  arrays after they were flattened using jax.tree_util.tree_flatten, and you\n  want to unflatten them again afterwards using the original treedef but\n  adjusted for the added/removed dimensions.\n\n  It can also be used with jax.tree_util.tree_map, when it's called with a\n  function that adds/removes axes or otherwise changes the axis order.\n\n  When dimensions are removed, any coordinates using those removed dimensions\n  will also be removed on unflatten.\n\n  This is implemented as a context manager that sets some thread-local state\n  affecting the behaviour of our unflatten functions, because it's not possible\n  to directly modify the treedef to change the dims/coords in it (and with\n  tree_map, the treedef isn't exposed to you anyway).\n\n  Args:\n    dims_change_fn: Maps a tuple of dimension names for the original\n      Variable/DataArray/Dataset that was flattened, to an updated tuple of\n      dimensions which should be used when unflattening.\n\n  Yields:\n    To a context manager in whose scope jax.tree_util.tree_unflatten and\n    jax.tree_util.tree_map will apply the dims_change_fn before reconstructing\n    xarrays from jax arrays.\n  \"\"\"\n    token = _DIMS_CHANGE_ON_UNFLATTEN_FN.set(dims_change_fn)\n    try:\n        yield\n    finally:\n        _DIMS_CHANGE_ON_UNFLATTEN_FN.reset(token)",
    "263": "def _unflatten_variable(aux: Tuple[Hashable, ...], children: Tuple[jax.typing.ArrayLike]) -> xarray.Variable:\n    \"\"\"Unflattens a Variable for jax.tree_util.\"\"\"\n    dims = aux\n    dims_change_fn = _DIMS_CHANGE_ON_UNFLATTEN_FN.get(None)\n    if dims_change_fn:\n        dims = dims_change_fn(dims)\n    return Variable(dims=dims, data=children[0])",
    "264": "def _split_static_and_jax_coords(coords: xarray.core.coordinates.Coordinates) -> Tuple[Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]]:\n    static_coord_vars = {}\n    jax_coord_vars = {}\n    for (name, coord) in coords.items():\n        if coord.attrs.get(_JAX_COORD_ATTR_NAME, False):\n            jax_coord_vars[name] = coord.variable\n        else:\n            assert not isinstance(coord, (jax.Array, JaxArrayWrapper))\n            static_coord_vars[name] = coord.variable\n    return (static_coord_vars, jax_coord_vars)",
    "266": "class _HashableCoords(collections.abc.Mapping):\n    \"\"\"Wraps a dict of xarray Variables as hashable, used for static coordinates.\n\n  This needs to be hashable so that when an xarray.Dataset is passed to a\n  jax.jit'ed function, jax can check whether it's seen an array with the\n  same static coordinates(*) before or whether it needs to recompile the\n  function for the new values of the static coordinates.\n\n  (*) note jax_coords are not included in this; their value can be different\n  on different calls without triggering a recompile.\n  \"\"\"\n\n    def __init__(self, coord_vars: Mapping[Hashable, xarray.Variable]):\n        self._variables = coord_vars\n\n    def __repr__(self) -> str:\n        return f'_HashableCoords({repr(self._variables)})'\n\n    def __getitem__(self, key: Hashable) -> xarray.Variable:\n        return self._variables[key]\n\n    def __len__(self) -> int:\n        return len(self._variables)\n\n    def __iter__(self) -> Iterator[Hashable]:\n        return iter(self._variables)\n\n    def __hash__(self):\n        if not hasattr(self, '_hash'):\n            self._hash = hash(frozenset(((name, var.data.tobytes()) for (name, var) in self._variables.items())))\n        return self._hash\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        elif not isinstance(other, type(self)):\n            return NotImplemented\n        elif self._variables is other._variables:\n            return True\n        else:\n            return self._variables.keys() == other._variables.keys() and all((variable.equals(other._variables[name]) for (name, variable) in self._variables.items()))",
    "268": "def __repr__(self) -> str:\n    return f'_HashableCoords({repr(self._variables)})'",
    "270": "def __len__(self) -> int:\n    return len(self._variables)",
    "272": "def __hash__(self):\n    if not hasattr(self, '_hash'):\n        self._hash = hash(frozenset(((name, var.data.tobytes()) for (name, var) in self._variables.items())))\n    return self._hash",
    "273": "def __eq__(self, other):\n    if self is other:\n        return True\n    elif not isinstance(other, type(self)):\n        return NotImplemented\n    elif self._variables is other._variables:\n        return True\n    else:\n        return self._variables.keys() == other._variables.keys() and all((variable.equals(other._variables[name]) for (name, variable) in self._variables.items()))",
    "274": "def _flatten_data_array(v: xarray.DataArray) -> Tuple[Tuple[xarray.Variable, Mapping[Hashable, xarray.Variable]], Tuple[Optional[Hashable], _HashableCoords]]:\n    \"\"\"Flattens a DataArray for jax.tree_util.\"\"\"\n    (static_coord_vars, jax_coord_vars) = _split_static_and_jax_coords(v.coords)\n    children = (v.variable, jax_coord_vars)\n    aux = (v.name, _HashableCoords(static_coord_vars))\n    return (children, aux)",
    "275": "def _unflatten_data_array(aux: Tuple[Optional[Hashable], _HashableCoords], children: Tuple[xarray.Variable, Mapping[Hashable, xarray.Variable]]) -> xarray.DataArray:\n    \"\"\"Unflattens a DataArray for jax.tree_util.\"\"\"\n    (variable, jax_coord_vars) = children\n    (name, static_coord_vars) = aux\n    static_coord_vars = _drop_with_none_of_dims(static_coord_vars, variable.dims)\n    return DataArray(variable, name=name, coords=static_coord_vars, jax_coords=jax_coord_vars)",
    "276": "def _flatten_dataset(dataset: xarray.Dataset) -> Tuple[Tuple[Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]], _HashableCoords]:\n    \"\"\"Flattens a Dataset for jax.tree_util.\"\"\"\n    variables = {name: data_array.variable for (name, data_array) in dataset.data_vars.items()}\n    (static_coord_vars, jax_coord_vars) = _split_static_and_jax_coords(dataset.coords)\n    children = (variables, jax_coord_vars)\n    aux = _HashableCoords(static_coord_vars)\n    return (children, aux)",
    "277": "def _unflatten_dataset(aux: _HashableCoords, children: Tuple[Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]]) -> xarray.Dataset:\n    \"\"\"Unflattens a Dataset for jax.tree_util.\"\"\"\n    (data_vars, jax_coord_vars) = children\n    static_coord_vars = aux\n    dataset = xarray.Dataset(data_vars)\n    static_coord_vars = _drop_with_none_of_dims(static_coord_vars, dataset.dims)\n    return assign_coords(dataset, coords=static_coord_vars, jax_coords=jax_coord_vars)",
    "278": "class XarrayJaxTest(absltest.TestCase):\n\n    def test_jax_array_wrapper_with_numpy_api(self):\n        ones = jnp.ones((3, 4), dtype=np.float32)\n        x = xarray_jax.JaxArrayWrapper(ones)\n        x = np.abs((x + 2) * (x - 3))\n        x = x[:-1, 1:3]\n        x = np.concatenate([x, x + 1], axis=0)\n        x = np.transpose(x, (1, 0))\n        x = np.reshape(x, (-1,))\n        x = x.astype(np.int32)\n        self.assertIsInstance(x, xarray_jax.JaxArrayWrapper)\n        self.assertIsInstance(np.asarray(x), np.ndarray)\n\n    def test_jax_xarray_variable(self):\n\n        def ops_via_xarray(inputs):\n            x = xarray_jax.Variable(('lat', 'lon'), inputs)\n            x = np.abs((x + 2) * (x - 3))\n            x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n            x = xarray.Variable.concat([x, x + 1], dim='lat')\n            x = x.transpose('lon', 'lat')\n            x = x.stack(channels=('lon', 'lat'))\n            x = x.sum()\n            return xarray_jax.jax_data(x)\n        ones = jnp.ones((3, 4), dtype=np.float32)\n        result = ops_via_xarray(ones)\n        self.assertIsInstance(result, jax.Array)\n        jax.jit(ops_via_xarray)(ones)\n        jax.grad(ops_via_xarray)(ones)\n\n    def test_jax_xarray_data_array(self):\n\n        def ops_via_xarray(inputs):\n            x = xarray_jax.DataArray(dims=('lat', 'lon'), data=inputs, coords={'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n            x = np.abs((x + 2) * (x - 3))\n            x = x.sel({'lat': slice(0, 20)})\n            y = xarray_jax.DataArray(dims=('lat', 'lon'), data=ones, coords={'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n            x = xarray.concat([x, y], dim='lat')\n            x = x.transpose('lon', 'lat')\n            x = x.stack(channels=('lon', 'lat'))\n            x = x.unstack()\n            x = x.sum()\n            return xarray_jax.jax_data(x)\n        ones = jnp.ones((3, 4), dtype=np.float32)\n        result = ops_via_xarray(ones)\n        self.assertIsInstance(result, jax.Array)\n        jax.jit(ops_via_xarray)(ones)\n        jax.grad(ops_via_xarray)(ones)\n\n    def test_jax_xarray_dataset(self):\n\n        def ops_via_xarray(foo, bar):\n            x = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n            x = np.abs((x + 2) * (x - 3))\n            x = x.sel({'lat': slice(0, 20)})\n            y = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n            x = xarray.concat([x, y], dim='lat')\n            x = x.transpose('lon', 'lat', 'time')\n            x = x.stack(channels=('lon', 'lat'))\n            x = (x.foo + x.bar).sum()\n            return xarray_jax.jax_data(x)\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        result = ops_via_xarray(foo, bar)\n        self.assertIsInstance(result, jax.Array)\n        jax.jit(ops_via_xarray)(foo, bar)\n        jax.grad(ops_via_xarray)(foo, bar)\n\n    def test_jit_function_with_xarray_variable_arguments_and_return(self):\n        function = jax.jit(lambda v: v + 1)\n        with self.subTest('jax input'):\n            inputs = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n            _ = function(inputs)\n            outputs = function(inputs)\n            self.assertEqual(outputs.dims, inputs.dims)\n        with self.subTest('numpy input'):\n            inputs = xarray.Variable(('lat', 'lon'), np.ones((3, 4), dtype=np.float32))\n            _ = function(inputs)\n            outputs = function(inputs)\n            self.assertEqual(outputs.dims, inputs.dims)\n\n    def test_jit_problem_if_convert_to_plain_numpy_array(self):\n        inputs = xarray_jax.DataArray(data=jnp.ones((2,), dtype=np.float32), dims=('foo',))\n        with self.assertRaises(jax.errors.TracerArrayConversionError):\n            jax.jit(lambda data_array: data_array.values)(inputs)\n\n    def test_grad_function_with_xarray_variable_arguments(self):\n        x = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n        jax.grad(lambda v: xarray_jax.jax_data(v.sum()))(x)\n\n    def test_jit_function_with_xarray_data_array_arguments_and_return(self):\n        inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n        fn = jax.jit(lambda v: v + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertEqual(outputs.dims, inputs.dims)\n        chex.assert_trees_all_equal(outputs.coords, inputs.coords)\n\n    def test_jit_function_with_data_array_and_jax_coords(self):\n        inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': jnp.arange(4) * 10})\n        self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', inputs.indexes)\n\n        @jax.jit\n        def fn(v):\n            self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n            self.assertIn('lat', v.indexes)\n            self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n            self.assertNotIn('lon', v.indexes)\n            v = v + v.coords['lon']\n            return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', outputs.indexes)\n        self.assertEqual(outputs.dims, inputs.dims)\n        chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n        chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n        chex.assert_trees_all_equal(outputs.data, (inputs + inputs.coords['lon']).data)\n\n    def test_jit_function_with_xarray_dataset_arguments_and_return(self):\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        fn = jax.jit(lambda v: v + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertEqual({'foo', 'bar'}, outputs.data_vars.keys())\n        self.assertEqual(inputs.foo.dims, outputs.foo.dims)\n        self.assertEqual(inputs.bar.dims, outputs.bar.dims)\n        chex.assert_trees_all_equal(outputs.coords, inputs.coords)\n\n    def test_jit_function_with_dataset_and_jax_coords(self):\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': jnp.arange(4) * 10})\n        self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', inputs.indexes)\n\n        @jax.jit\n        def fn(v):\n            self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n            self.assertIn('lat', v.indexes)\n            self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n            self.assertNotIn('lon', v.indexes)\n            v = v + v.coords['lon']\n            return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', outputs.indexes)\n        self.assertEqual(outputs.dims, inputs.dims)\n        chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n        chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n        outputs_dict = {key: outputs[key].data for key in outputs}\n        inputs_and_inputs_coords_dict = {key: (inputs + inputs.coords['lon'])[key].data for key in inputs + inputs.coords['lon']}\n        chex.assert_trees_all_equal(outputs_dict, inputs_and_inputs_coords_dict)\n\n    def test_flatten_unflatten_variable(self):\n        variable = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n        (children, aux) = xarray_jax._flatten_variable(variable)\n        hash(aux)\n        self.assertEqual(aux, aux)\n        roundtrip = xarray_jax._unflatten_variable(aux, children)\n        self.assertTrue(variable.equals(roundtrip))\n\n    def test_flatten_unflatten_data_array(self):\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': np.arange(4) * 10})\n        (children, aux) = xarray_jax._flatten_data_array(data_array)\n        hash(aux)\n        self.assertEqual(aux, aux)\n        roundtrip = xarray_jax._unflatten_data_array(aux, children)\n        self.assertTrue(data_array.equals(roundtrip))\n\n    def test_flatten_unflatten_dataset(self):\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        dataset = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': np.arange(4) * 10})\n        (children, aux) = xarray_jax._flatten_dataset(dataset)\n        hash(aux)\n        self.assertEqual(aux, aux)\n        roundtrip = xarray_jax._unflatten_dataset(aux, children)\n        self.assertTrue(dataset.equals(roundtrip))\n\n    def test_flatten_unflatten_added_dim(self):\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n        (leaves, treedef) = jax.tree_util.tree_flatten(data_array)\n        leaves = [jnp.expand_dims(x, 0) for x in leaves]\n        with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n            with_new_dim = jax.tree_util.tree_unflatten(treedef, leaves)\n        self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n        xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))\n\n    def test_map_added_dim(self):\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n        with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n            with_new_dim = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, 0), data_array)\n        self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n        xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))\n\n    def test_map_remove_dim(self):\n        foo = jnp.ones((1, 3, 4), dtype=np.float32)\n        bar = jnp.ones((1, 2, 3, 4), dtype=np.float32)\n        dataset = xarray_jax.Dataset(data_vars={'foo': (('batch', 'lat', 'lon'), foo), 'bar': (('batch', 'time', 'lat', 'lon'), bar)}, coords={'batch': np.array([123]), 'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        with xarray_jax.dims_change_on_unflatten(lambda dims: dims[1:]):\n            with_removed_dim = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, 0), dataset)\n        self.assertEqual(('lat', 'lon'), with_removed_dim['foo'].dims)\n        self.assertEqual(('time', 'lat', 'lon'), with_removed_dim['bar'].dims)\n        self.assertNotIn('batch', with_removed_dim.dims)\n        self.assertNotIn('batch', with_removed_dim.coords)\n        xarray.testing.assert_identical(jax.device_get(dataset.isel(batch=0, drop=True)), jax.device_get(with_removed_dim))\n\n    def test_pmap(self):\n        devices = jax.local_device_count()\n        foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n        bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n        dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)})\n\n        def func(d):\n            self.assertNotIn('device', d.dims)\n            return d + 1\n        func = xarray_jax.pmap(func, dim='device')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n        dataset = dataset.drop_vars('foo')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n\n    def test_pmap_with_jax_coords(self):\n        devices = jax.local_device_count()\n        foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n        bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n        time = jnp.zeros((devices, 2), dtype=np.float32)\n        dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)}, coords={'lat': np.arange(3), 'lon': np.arange(4)}, jax_coords={'time': xarray_jax.Variable(('device', 'time'), time)})\n\n        def func(d):\n            self.assertNotIn('device', d.dims)\n            self.assertNotIn('device', d.coords['time'].dims)\n            self.assertIsInstance(d.coords['time'].data, xarray_jax.JaxArrayWrapper)\n            self.assertNotIn('time', d.indexes)\n            return d + 1\n        func = xarray_jax.pmap(func, dim='device')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n        dataset = dataset.drop_vars('foo')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n\n    def test_pmap_with_tree_mix_of_xarray_and_jax_array(self):\n        devices = jax.local_device_count()\n        data_array = xarray_jax.DataArray(data=jnp.ones((devices, 3, 4), dtype=np.float32), dims=('device', 'lat', 'lon'))\n        plain_array = jnp.ones((devices, 2), dtype=np.float32)\n        inputs = {'foo': data_array, 'bar': plain_array}\n\n        def func(x):\n            return (x['foo'] + 1, x['bar'] + 1)\n        func = xarray_jax.pmap(func, dim='device')\n        (result_foo, result_bar) = func(inputs)\n        xarray.testing.assert_identical(jax.device_get(inputs['foo'] + 1), jax.device_get(result_foo))\n        np.testing.assert_array_equal(jax.device_get(inputs['bar'] + 1), jax.device_get(result_bar))\n\n    def test_pmap_complains_when_dim_not_first(self):\n        devices = jax.local_device_count()\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, devices, 4), dtype=np.float32), dims=('lat', 'device', 'lon'))\n        func = xarray_jax.pmap(lambda x: x + 1, dim='device')\n        with self.assertRaisesRegex(ValueError, 'Expected dim device at index 0, found at 1'):\n            func(data_array)\n\n    def test_apply_ufunc(self):\n        inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 2], [3, 4]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n        result = xarray_jax.apply_ufunc(lambda x: jnp.sum(x, axis=-1), inputs, input_core_dims=[['x']])\n        expected_result = xarray_jax.DataArray(data=[4, 6], dims=('y',), coords={'y': [2, 3]})\n        xarray.testing.assert_identical(expected_result, jax.device_get(result))\n\n    def test_apply_ufunc_multiple_return_values(self):\n\n        def ufunc(array):\n            return (jnp.min(array, axis=-1), jnp.max(array, axis=-1))\n        inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 4], [3, 2]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n        result = xarray_jax.apply_ufunc(ufunc, inputs, input_core_dims=[['x']], output_core_dims=[[], []])\n        expected = (xarray_jax.DataArray(data=[1, 2], dims=('y',), coords={'y': [2, 3]}), xarray_jax.DataArray(data=[3, 4], dims=('y',), coords={'y': [2, 3]}))\n        xarray.testing.assert_identical(expected[0], jax.device_get(result[0]))\n        xarray.testing.assert_identical(expected[1], jax.device_get(result[1]))",
    "279": "def test_jax_array_wrapper_with_numpy_api(self):\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    x = xarray_jax.JaxArrayWrapper(ones)\n    x = np.abs((x + 2) * (x - 3))\n    x = x[:-1, 1:3]\n    x = np.concatenate([x, x + 1], axis=0)\n    x = np.transpose(x, (1, 0))\n    x = np.reshape(x, (-1,))\n    x = x.astype(np.int32)\n    self.assertIsInstance(x, xarray_jax.JaxArrayWrapper)\n    self.assertIsInstance(np.asarray(x), np.ndarray)",
    "280": "def test_jax_xarray_variable(self):\n\n    def ops_via_xarray(inputs):\n        x = xarray_jax.Variable(('lat', 'lon'), inputs)\n        x = np.abs((x + 2) * (x - 3))\n        x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n        x = xarray.Variable.concat([x, x + 1], dim='lat')\n        x = x.transpose('lon', 'lat')\n        x = x.stack(channels=('lon', 'lat'))\n        x = x.sum()\n        return xarray_jax.jax_data(x)\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    result = ops_via_xarray(ones)\n    self.assertIsInstance(result, jax.Array)\n    jax.jit(ops_via_xarray)(ones)\n    jax.grad(ops_via_xarray)(ones)",
    "281": "def ops_via_xarray(inputs):\n    x = xarray_jax.Variable(('lat', 'lon'), inputs)\n    x = np.abs((x + 2) * (x - 3))\n    x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n    x = xarray.Variable.concat([x, x + 1], dim='lat')\n    x = x.transpose('lon', 'lat')\n    x = x.stack(channels=('lon', 'lat'))\n    x = x.sum()\n    return xarray_jax.jax_data(x)",
    "282": "def test_jax_xarray_data_array(self):\n\n    def ops_via_xarray(inputs):\n        x = xarray_jax.DataArray(dims=('lat', 'lon'), data=inputs, coords={'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        x = np.abs((x + 2) * (x - 3))\n        x = x.sel({'lat': slice(0, 20)})\n        y = xarray_jax.DataArray(dims=('lat', 'lon'), data=ones, coords={'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n        x = xarray.concat([x, y], dim='lat')\n        x = x.transpose('lon', 'lat')\n        x = x.stack(channels=('lon', 'lat'))\n        x = x.unstack()\n        x = x.sum()\n        return xarray_jax.jax_data(x)\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    result = ops_via_xarray(ones)\n    self.assertIsInstance(result, jax.Array)\n    jax.jit(ops_via_xarray)(ones)\n    jax.grad(ops_via_xarray)(ones)",
    "283": "def ops_via_xarray(inputs):\n    x = xarray_jax.DataArray(dims=('lat', 'lon'), data=inputs, coords={'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    x = np.abs((x + 2) * (x - 3))\n    x = x.sel({'lat': slice(0, 20)})\n    y = xarray_jax.DataArray(dims=('lat', 'lon'), data=ones, coords={'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n    x = xarray.concat([x, y], dim='lat')\n    x = x.transpose('lon', 'lat')\n    x = x.stack(channels=('lon', 'lat'))\n    x = x.unstack()\n    x = x.sum()\n    return xarray_jax.jax_data(x)",
    "284": "def test_jax_xarray_dataset(self):\n\n    def ops_via_xarray(foo, bar):\n        x = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        x = np.abs((x + 2) * (x - 3))\n        x = x.sel({'lat': slice(0, 20)})\n        y = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n        x = xarray.concat([x, y], dim='lat')\n        x = x.transpose('lon', 'lat', 'time')\n        x = x.stack(channels=('lon', 'lat'))\n        x = (x.foo + x.bar).sum()\n        return xarray_jax.jax_data(x)\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    result = ops_via_xarray(foo, bar)\n    self.assertIsInstance(result, jax.Array)\n    jax.jit(ops_via_xarray)(foo, bar)\n    jax.grad(ops_via_xarray)(foo, bar)",
    "285": "def ops_via_xarray(foo, bar):\n    x = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    x = np.abs((x + 2) * (x - 3))\n    x = x.sel({'lat': slice(0, 20)})\n    y = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n    x = xarray.concat([x, y], dim='lat')\n    x = x.transpose('lon', 'lat', 'time')\n    x = x.stack(channels=('lon', 'lat'))\n    x = (x.foo + x.bar).sum()\n    return xarray_jax.jax_data(x)",
    "286": "def test_jit_function_with_xarray_variable_arguments_and_return(self):\n    function = jax.jit(lambda v: v + 1)\n    with self.subTest('jax input'):\n        inputs = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n        _ = function(inputs)\n        outputs = function(inputs)\n        self.assertEqual(outputs.dims, inputs.dims)\n    with self.subTest('numpy input'):\n        inputs = xarray.Variable(('lat', 'lon'), np.ones((3, 4), dtype=np.float32))\n        _ = function(inputs)\n        outputs = function(inputs)\n        self.assertEqual(outputs.dims, inputs.dims)",
    "288": "def test_grad_function_with_xarray_variable_arguments(self):\n    x = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n    jax.grad(lambda v: xarray_jax.jax_data(v.sum()))(x)",
    "289": "def test_jit_function_with_xarray_data_array_arguments_and_return(self):\n    inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n    fn = jax.jit(lambda v: v + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords, inputs.coords)",
    "290": "def test_jit_function_with_data_array_and_jax_coords(self):\n    inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': jnp.arange(4) * 10})\n    self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', inputs.indexes)\n\n    @jax.jit\n    def fn(v):\n        self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n        self.assertIn('lat', v.indexes)\n        self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', v.indexes)\n        v = v + v.coords['lon']\n        return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', outputs.indexes)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n    chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n    chex.assert_trees_all_equal(outputs.data, (inputs + inputs.coords['lon']).data)",
    "291": "@jax.jit\ndef fn(v):\n    self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n    self.assertIn('lat', v.indexes)\n    self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', v.indexes)\n    v = v + v.coords['lon']\n    return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)",
    "292": "def test_jit_function_with_xarray_dataset_arguments_and_return(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    fn = jax.jit(lambda v: v + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertEqual({'foo', 'bar'}, outputs.data_vars.keys())\n    self.assertEqual(inputs.foo.dims, outputs.foo.dims)\n    self.assertEqual(inputs.bar.dims, outputs.bar.dims)\n    chex.assert_trees_all_equal(outputs.coords, inputs.coords)",
    "293": "def test_jit_function_with_dataset_and_jax_coords(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': jnp.arange(4) * 10})\n    self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', inputs.indexes)\n\n    @jax.jit\n    def fn(v):\n        self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n        self.assertIn('lat', v.indexes)\n        self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', v.indexes)\n        v = v + v.coords['lon']\n        return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', outputs.indexes)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n    chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n    outputs_dict = {key: outputs[key].data for key in outputs}\n    inputs_and_inputs_coords_dict = {key: (inputs + inputs.coords['lon'])[key].data for key in inputs + inputs.coords['lon']}\n    chex.assert_trees_all_equal(outputs_dict, inputs_and_inputs_coords_dict)",
    "294": "@jax.jit\ndef fn(v):\n    self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n    self.assertIn('lat', v.indexes)\n    self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', v.indexes)\n    v = v + v.coords['lon']\n    return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)",
    "295": "def test_flatten_unflatten_variable(self):\n    variable = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n    (children, aux) = xarray_jax._flatten_variable(variable)\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_variable(aux, children)\n    self.assertTrue(variable.equals(roundtrip))",
    "296": "def test_flatten_unflatten_data_array(self):\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': np.arange(4) * 10})\n    (children, aux) = xarray_jax._flatten_data_array(data_array)\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_data_array(aux, children)\n    self.assertTrue(data_array.equals(roundtrip))",
    "297": "def test_flatten_unflatten_dataset(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': np.arange(4) * 10})\n    (children, aux) = xarray_jax._flatten_dataset(dataset)\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_dataset(aux, children)\n    self.assertTrue(dataset.equals(roundtrip))",
    "298": "def test_flatten_unflatten_added_dim(self):\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n    (leaves, treedef) = jax.tree_util.tree_flatten(data_array)\n    leaves = [jnp.expand_dims(x, 0) for x in leaves]\n    with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n        with_new_dim = jax.tree_util.tree_unflatten(treedef, leaves)\n    self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n    xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))",
    "299": "def test_map_added_dim(self):\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n    with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n        with_new_dim = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, 0), data_array)\n    self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n    xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))",
    "300": "def test_map_remove_dim(self):\n    foo = jnp.ones((1, 3, 4), dtype=np.float32)\n    bar = jnp.ones((1, 2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset(data_vars={'foo': (('batch', 'lat', 'lon'), foo), 'bar': (('batch', 'time', 'lat', 'lon'), bar)}, coords={'batch': np.array([123]), 'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    with xarray_jax.dims_change_on_unflatten(lambda dims: dims[1:]):\n        with_removed_dim = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, 0), dataset)\n    self.assertEqual(('lat', 'lon'), with_removed_dim['foo'].dims)\n    self.assertEqual(('time', 'lat', 'lon'), with_removed_dim['bar'].dims)\n    self.assertNotIn('batch', with_removed_dim.dims)\n    self.assertNotIn('batch', with_removed_dim.coords)\n    xarray.testing.assert_identical(jax.device_get(dataset.isel(batch=0, drop=True)), jax.device_get(with_removed_dim))",
    "301": "def test_pmap(self):\n    devices = jax.local_device_count()\n    foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n    bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)})\n\n    def func(d):\n        self.assertNotIn('device', d.dims)\n        return d + 1\n    func = xarray_jax.pmap(func, dim='device')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n    dataset = dataset.drop_vars('foo')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))",
    "303": "def test_pmap_with_jax_coords(self):\n    devices = jax.local_device_count()\n    foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n    bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n    time = jnp.zeros((devices, 2), dtype=np.float32)\n    dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)}, coords={'lat': np.arange(3), 'lon': np.arange(4)}, jax_coords={'time': xarray_jax.Variable(('device', 'time'), time)})\n\n    def func(d):\n        self.assertNotIn('device', d.dims)\n        self.assertNotIn('device', d.coords['time'].dims)\n        self.assertIsInstance(d.coords['time'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('time', d.indexes)\n        return d + 1\n    func = xarray_jax.pmap(func, dim='device')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n    dataset = dataset.drop_vars('foo')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))",
    "305": "def test_pmap_with_tree_mix_of_xarray_and_jax_array(self):\n    devices = jax.local_device_count()\n    data_array = xarray_jax.DataArray(data=jnp.ones((devices, 3, 4), dtype=np.float32), dims=('device', 'lat', 'lon'))\n    plain_array = jnp.ones((devices, 2), dtype=np.float32)\n    inputs = {'foo': data_array, 'bar': plain_array}\n\n    def func(x):\n        return (x['foo'] + 1, x['bar'] + 1)\n    func = xarray_jax.pmap(func, dim='device')\n    (result_foo, result_bar) = func(inputs)\n    xarray.testing.assert_identical(jax.device_get(inputs['foo'] + 1), jax.device_get(result_foo))\n    np.testing.assert_array_equal(jax.device_get(inputs['bar'] + 1), jax.device_get(result_bar))",
    "307": "def test_pmap_complains_when_dim_not_first(self):\n    devices = jax.local_device_count()\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, devices, 4), dtype=np.float32), dims=('lat', 'device', 'lon'))\n    func = xarray_jax.pmap(lambda x: x + 1, dim='device')\n    with self.assertRaisesRegex(ValueError, 'Expected dim device at index 0, found at 1'):\n        func(data_array)",
    "308": "def test_apply_ufunc(self):\n    inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 2], [3, 4]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n    result = xarray_jax.apply_ufunc(lambda x: jnp.sum(x, axis=-1), inputs, input_core_dims=[['x']])\n    expected_result = xarray_jax.DataArray(data=[4, 6], dims=('y',), coords={'y': [2, 3]})\n    xarray.testing.assert_identical(expected_result, jax.device_get(result))",
    "309": "def test_apply_ufunc_multiple_return_values(self):\n\n    def ufunc(array):\n        return (jnp.min(array, axis=-1), jnp.max(array, axis=-1))\n    inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 4], [3, 2]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n    result = xarray_jax.apply_ufunc(ufunc, inputs, input_core_dims=[['x']], output_core_dims=[[], []])\n    expected = (xarray_jax.DataArray(data=[1, 2], dims=('y',), coords={'y': [2, 3]}), xarray_jax.DataArray(data=[3, 4], dims=('y',), coords={'y': [2, 3]}))\n    xarray.testing.assert_identical(expected[0], jax.device_get(result[0]))\n    xarray.testing.assert_identical(expected[1], jax.device_get(result[1]))",
    "311": "def map_structure(func: Callable[..., Any], *structures: Any) -> Any:\n    \"\"\"Maps func through given structures with xarrays. See tree.map_structure.\"\"\"\n    if not callable(func):\n        raise TypeError(f'func must be callable, got: {func}')\n    if not structures:\n        raise ValueError('Must provide at least one structure')\n    first = structures[0]\n    if isinstance(first, xarray.Dataset):\n        data = {k: func(*[s[k] for s in structures]) for k in first.keys()}\n        if all((isinstance(a, (type(None), xarray.DataArray)) for a in data.values())):\n            data_arrays = [v.rename(k) for (k, v) in data.items() if v is not None]\n            try:\n                return xarray.merge(data_arrays, join='exact')\n            except ValueError:\n                pass\n        return data\n    if isinstance(first, dict):\n        return {k: map_structure(func, *[s[k] for s in structures]) for k in first.keys()}\n    if isinstance(first, (list, tuple, set)):\n        return type(first)((map_structure(func, *s) for s in zip(*structures)))\n    return func(*structures)",
    "312": "class XarrayTreeTest(absltest.TestCase):\n\n    def test_map_structure_maps_over_leaves_but_preserves_dataset_type(self):\n\n        def fn(leaf):\n            self.assertIsInstance(leaf, xarray.DataArray)\n            result = leaf + 1\n            result = result.rename(None)\n            return result\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertIsInstance(result, xarray.Dataset)\n        self.assertSameElements({'foo', 'bar'}, result.keys())\n\n    def test_map_structure_on_data_arrays(self):\n        data_arrays = dict(TEST_DATASET)\n        result = xarray_tree.map_structure(lambda x: x + 1, data_arrays)\n        self.assertIsInstance(result, dict)\n        self.assertSameElements({'foo', 'bar'}, result.keys())\n\n    def test_map_structure_on_dataset_plain_dict_when_coords_incompatible(self):\n\n        def fn(leaf):\n            if leaf.name == 'foo':\n                return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [1, 2]})\n            else:\n                return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [3, 4]})\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertIsInstance(result, dict)\n        self.assertSameElements({'foo', 'bar'}, result.keys())\n\n    def test_map_structure_on_dataset_drops_vars_with_none_return_values(self):\n\n        def fn(leaf):\n            return leaf if leaf.name == 'foo' else None\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertIsInstance(result, xarray.Dataset)\n        self.assertSameElements({'foo'}, result.keys())\n\n    def test_map_structure_on_dataset_returns_plain_dict_other_return_types(self):\n\n        def fn(leaf):\n            self.assertIsInstance(leaf, xarray.DataArray)\n            return 'not a DataArray'\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertEqual({'foo': 'not a DataArray', 'bar': 'not a DataArray'}, result)\n\n    def test_map_structure_two_args_different_variable_orders(self):\n        dataset_different_order = TEST_DATASET[['bar', 'foo']]\n\n        def fn(arg1, arg2):\n            self.assertEqual(arg1.name, arg2.name)\n        xarray_tree.map_structure(fn, TEST_DATASET, dataset_different_order)",
    "313": "def test_map_structure_maps_over_leaves_but_preserves_dataset_type(self):\n\n    def fn(leaf):\n        self.assertIsInstance(leaf, xarray.DataArray)\n        result = leaf + 1\n        result = result.rename(None)\n        return result\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, xarray.Dataset)\n    self.assertSameElements({'foo', 'bar'}, result.keys())",
    "315": "def test_map_structure_on_data_arrays(self):\n    data_arrays = dict(TEST_DATASET)\n    result = xarray_tree.map_structure(lambda x: x + 1, data_arrays)\n    self.assertIsInstance(result, dict)\n    self.assertSameElements({'foo', 'bar'}, result.keys())",
    "316": "def test_map_structure_on_dataset_plain_dict_when_coords_incompatible(self):\n\n    def fn(leaf):\n        if leaf.name == 'foo':\n            return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [1, 2]})\n        else:\n            return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [3, 4]})\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, dict)\n    self.assertSameElements({'foo', 'bar'}, result.keys())",
    "318": "def test_map_structure_on_dataset_drops_vars_with_none_return_values(self):\n\n    def fn(leaf):\n        return leaf if leaf.name == 'foo' else None\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, xarray.Dataset)\n    self.assertSameElements({'foo'}, result.keys())",
    "320": "def test_map_structure_on_dataset_returns_plain_dict_other_return_types(self):\n\n    def fn(leaf):\n        self.assertIsInstance(leaf, xarray.DataArray)\n        return 'not a DataArray'\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertEqual({'foo': 'not a DataArray', 'bar': 'not a DataArray'}, result)",
    "322": "def test_map_structure_two_args_different_variable_orders(self):\n    dataset_different_order = TEST_DATASET[['bar', 'foo']]\n\n    def fn(arg1, arg2):\n        self.assertEqual(arg1.name, arg2.name)\n    xarray_tree.map_structure(fn, TEST_DATASET, dataset_different_order)"
}