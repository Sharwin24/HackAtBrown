{
    "0": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Module setuptools script.\"\"\"\n\nfrom setuptools import setup\n\ndescription = (\n    \"GraphCast: Learning skillful medium-range global weather forecasting\"\n)\n\nsetup(\n    name=\"graphcast\",\n    version=\"0.1\",\n    description=description,\n    long_description=description,\n    author=\"DeepMind\",\n    license=\"Apache License, Version 2.0\",\n    keywords=\"GraphCast Weather Prediction\",\n    url=\"https://github.com/deepmind/graphcast\",\n    packages=[\"graphcast\"],\n    install_requires=[\n        \"cartopy\",\n        \"chex\",\n        \"colabtools\",\n        \"dask\",\n        \"dm-haiku\",\n        \"dm-tree\",\n        \"jax\",\n        \"jraph\",\n        \"matplotlib\",\n        \"numpy\",\n        \"pandas\",\n        \"rtree\",\n        \"scipy\",\n        \"trimesh\",\n        \"typing_extensions\",\n        \"xarray\",\n    ],\n    classifiers=[\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Science/Research\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Operating System :: POSIX :: Linux\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Atmospheric Science\",\n        \"Topic :: Scientific/Engineering :: Physics\",\n    ],\n)\n",
    "1": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"A Predictor wrapping a one-step Predictor to make autoregressive predictions.\n\"\"\"\n\nfrom typing import Optional, cast\n\nfrom absl import logging\nfrom graphcast import predictor_base\nfrom graphcast import xarray_jax\nfrom graphcast import xarray_tree\nimport haiku as hk\nimport jax\nimport xarray\n\n\ndef _unflatten_and_expand_time(flat_variables, tree_def, time_coords):\n  variables = jax.tree_util.tree_unflatten(tree_def, flat_variables)\n  return variables.expand_dims(time=time_coords, axis=0)\n\n\ndef _get_flat_arrays_and_single_timestep_treedef(variables):\n  flat_arrays = jax.tree_util.tree_leaves(variables.transpose('time', ...))\n  _, treedef = jax.tree_util.tree_flatten(variables.isel(time=0, drop=True))\n  return flat_arrays, treedef\n\n\nclass Predictor(predictor_base.Predictor):\n  \"\"\"Wraps a one-step Predictor to make multi-step predictions autoregressively.\n\n  The wrapped Predictor will be used to predict a single timestep conditional\n  on the inputs passed to the outer Predictor. Its predictions are then\n  passed back in as inputs at the next timestep, for as many timesteps as are\n  requested in the targets_template. (When multiple timesteps of input are\n  used, a rolling window of inputs is maintained with new predictions\n  concatenated onto the end).\n\n  You may ask for additional variables to be predicted as targets which aren't\n  used as inputs. These will be predicted as output variables only and not fed\n  back in autoregressively. All target variables must be time-dependent however.\n\n  You may also specify static (non-time-dependent) inputs which will be passed\n  in at each timestep but are not predicted.\n\n  At present, any time-dependent inputs must also be present as targets so they\n  can be passed in autoregressively.\n\n  The loss of the wrapped one-step Predictor is averaged over all timesteps to\n  give a loss for the autoregressive Predictor.\n  \"\"\"\n\n  def __init__(\n      self,\n      predictor: predictor_base.Predictor,\n      noise_level: Optional[float] = None,\n      gradient_checkpointing: bool = False,\n      ):\n    \"\"\"Initializes an autoregressive predictor wrapper.\n\n    Args:\n      predictor: A predictor to wrap in an auto-regressive way.\n      noise_level: Optional value that multiplies the standard normal noise\n        added to the time-dependent variables of the predictor inputs. In\n        particular, no noise is added to the predictions that are fed back\n        auto-regressively. Defaults to not adding noise.\n      gradient_checkpointing: If True, gradient checkpointing will be\n        used at each step of the computation to save on memory. Roughtly this\n        should make the backwards pass two times more expensive, and the time\n        per step counting the forward pass, should only increase by about 50%.\n        Note this parameter will be ignored with a warning if the scan sequence\n        length is 1.\n    \"\"\"\n    self._predictor = predictor\n    self._noise_level = noise_level\n    self._gradient_checkpointing = gradient_checkpointing\n\n  def _get_and_validate_constant_inputs(self, inputs, targets, forcings):\n    constant_inputs = inputs.drop_vars(targets.keys(), errors='ignore')\n    constant_inputs = constant_inputs.drop_vars(\n        forcings.keys(), errors='ignore')\n    for name, var in constant_inputs.items():\n      if 'time' in var.dims:\n        raise ValueError(\n            f'Time-dependent input variable {name} must either be a forcing '\n            'variable, or a target variable to allow for auto-regressive '\n            'feedback.')\n    return constant_inputs\n\n  def _validate_targets_and_forcings(self, targets, forcings):\n    for name, var in targets.items():\n      if 'time' not in var.dims:\n        raise ValueError(f'Target variable {name} must be time-dependent.')\n\n    for name, var in forcings.items():\n      if 'time' not in var.dims:\n        raise ValueError(f'Forcing variable {name} must be time-dependent.')\n\n    overlap = forcings.keys() & targets.keys()\n    if overlap:\n      raise ValueError('The following were specified as both targets and '\n                       f'forcings, which isn\\'t allowed: {overlap}')\n\n  def _update_inputs(self, inputs, next_frame):\n    num_inputs = inputs.dims['time']\n\n    predicted_or_forced_inputs = next_frame[list(inputs.keys())]\n\n    # Combining datasets with inputs and target time stamps aligns them.\n    # Only keep the num_inputs trailing frames for use as next inputs.\n    return (xarray.concat([inputs, predicted_or_forced_inputs], dim='time')\n            .tail(time=num_inputs)\n            # Update the time coordinate to reset the lead times for\n            # next AR iteration.\n            .assign_coords(time=inputs.coords['time']))\n\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               **kwargs) -> xarray.Dataset:\n    \"\"\"Calls the Predictor.\n\n    Args:\n      inputs: input variable used to make predictions. Inputs can include both\n        time-dependent and time independent variables. Any time-dependent\n        input variables must also be present in the targets_template or the\n        forcings.\n      targets_template: A target template containing informations about which\n        variables should be predicted and the time alignment of the predictions.\n        All target variables must be time-dependent.\n        The number of time frames is used to set the number of unroll of the AR\n        predictor (e.g. multiple unroll of the inner predictor for one time step\n        in the targets is not supported yet).\n      forcings: Variables that will be fed to the model. The variables\n        should not overlap with the target ones. The time coordinates of the\n        forcing variables should match the target ones.\n        Forcing variables which are also present in the inputs, will be used to\n        supply ground-truth values for those inputs when they are passed to the\n        underlying predictor at timesteps beyond the first timestep.\n      **kwargs: Additional arguments passed along to the inner Predictor.\n\n    Returns:\n      predictions: the model predictions matching the target template.\n\n    Raise:\n      ValueError: if the time coordinates of the inputs and targets are not\n        different by a constant time step.\n    \"\"\"\n\n    constant_inputs = self._get_and_validate_constant_inputs(\n        inputs, targets_template, forcings)\n    self._validate_targets_and_forcings(targets_template, forcings)\n\n    # After the above checks, the remaining inputs must be time-dependent:\n    inputs = inputs.drop_vars(constant_inputs.keys())\n\n    # A predictions template only including the next time to predict.\n    target_template = targets_template.isel(time=[0])\n\n    flat_forcings, forcings_treedef = (\n        _get_flat_arrays_and_single_timestep_treedef(forcings))\n    scan_variables = flat_forcings\n\n    def one_step_prediction(inputs, scan_variables):\n\n      flat_forcings = scan_variables\n      forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef,\n                                            target_template.coords['time'])\n\n      # Add constant inputs:\n      all_inputs = xarray.merge([constant_inputs, inputs])\n      predictions: xarray.Dataset = self._predictor(\n          all_inputs, target_template,\n          forcings=forcings,\n          **kwargs)\n\n      next_frame = xarray.merge([predictions, forcings])\n      next_inputs = self._update_inputs(inputs, next_frame)\n\n      # Drop the length-1 time dimension, since scan will concat all the outputs\n      # for different times along a new leading time dimension:\n      predictions = predictions.squeeze('time', drop=True)\n      # We return the prediction flattened into plain jax arrays, because the\n      # extra leading dimension added by scan prevents the tree_util\n      # registrations in xarray_jax from unflattening them back into an\n      # xarray.Dataset automatically:\n      flat_pred = jax.tree_util.tree_leaves(predictions)\n      return next_inputs, flat_pred\n\n    if self._gradient_checkpointing:\n      scan_length = targets_template.dims['time']\n      if scan_length <= 1:\n        logging.warning(\n            'Skipping gradient checkpointing for sequence length of 1')\n      else:\n        # Just in case we take gradients (e.g. for control), although\n        # in most cases this will just be for a forward pass.\n        one_step_prediction = hk.remat(one_step_prediction)\n\n    # Loop (without unroll) with hk states in cell (jax.lax.scan won't do).\n    _, flat_preds = hk.scan(one_step_prediction, inputs, scan_variables)\n\n    # The result of scan will have an extra leading axis on all arrays,\n    # corresponding to the target times in this case. We need to be prepared for\n    # it when unflattening the arrays back into a Dataset:\n    scan_result_template = (\n        target_template.squeeze('time', drop=True)\n        .expand_dims(time=targets_template.coords['time'], axis=0))\n    _, scan_result_treedef = jax.tree_util.tree_flatten(scan_result_template)\n    predictions = jax.tree_util.tree_unflatten(scan_result_treedef, flat_preds)\n    return predictions\n\n  def loss(self,\n           inputs: xarray.Dataset,\n           targets: xarray.Dataset,\n           forcings: xarray.Dataset,\n           **kwargs\n           ) -> predictor_base.LossAndDiagnostics:\n    \"\"\"The mean of the per-timestep losses of the underlying predictor.\"\"\"\n    if targets.sizes['time'] == 1:\n      # If there is only a single target timestep then we don't need any\n      # autoregressive feedback and can delegate the loss directly to the\n      # underlying single-step predictor. This means the underlying predictor\n      # doesn't need to implement .loss_and_predictions.\n      return self._predictor.loss(inputs, targets, forcings, **kwargs)\n\n    constant_inputs = self._get_and_validate_constant_inputs(\n        inputs, targets, forcings)\n    self._validate_targets_and_forcings(targets, forcings)\n    # After the above checks, the remaining inputs must be time-dependent:\n    inputs = inputs.drop_vars(constant_inputs.keys())\n\n    if self._noise_level:\n      def add_noise(x):\n        return x + self._noise_level * jax.random.normal(\n            hk.next_rng_key(), shape=x.shape)\n      # Add noise to time-dependent variables of the inputs.\n      inputs = jax.tree_map(add_noise, inputs)\n\n    # The per-timestep targets passed by scan to one_step_loss below will have\n    # no leading time axis. We need a treedef without the time axis to use\n    # inside one_step_loss to unflatten it back into a dataset:\n    flat_targets, target_treedef = _get_flat_arrays_and_single_timestep_treedef(\n        targets)\n    scan_variables = flat_targets\n\n    flat_forcings, forcings_treedef = (\n        _get_flat_arrays_and_single_timestep_treedef(forcings))\n    scan_variables = (flat_targets, flat_forcings)\n\n    def one_step_loss(inputs, scan_variables):\n      flat_target, flat_forcings = scan_variables\n      forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef,\n                                            targets.coords['time'][:1])\n\n      target = _unflatten_and_expand_time(flat_target, target_treedef,\n                                          targets.coords['time'][:1])\n\n      # Add constant inputs:\n      all_inputs = xarray.merge([constant_inputs, inputs])\n\n      (loss, diagnostics), predictions = self._predictor.loss_and_predictions(\n          all_inputs,\n          target,\n          forcings=forcings,\n          **kwargs)\n\n      # Unwrap to jax arrays shape (batch,):\n      loss, diagnostics = xarray_tree.map_structure(\n          xarray_jax.unwrap_data, (loss, diagnostics))\n\n      predictions = cast(xarray.Dataset, predictions)  # Keeps pytype happy.\n      next_frame = xarray.merge([predictions, forcings])\n      next_inputs = self._update_inputs(inputs, next_frame)\n\n      return next_inputs, (loss, diagnostics)\n\n    if self._gradient_checkpointing:\n      scan_length = targets.dims['time']\n      if scan_length <= 1:\n        logging.warning(\n            'Skipping gradient checkpointing for sequence length of 1')\n      else:\n        one_step_loss = hk.remat(one_step_loss)\n\n    # We can pass inputs (the initial state of the loop) in directly as a\n    # Dataset because the shape we pass in to scan is the same as the shape scan\n    # passes to the inner function. But, for scan_variables, we must flatten the\n    # targets (and unflatten them inside the inner function) because they are\n    # passed to the inner function per-timestep without the original time axis.\n    # The same apply to the optional forcing.\n    _, (per_timestep_losses, per_timestep_diagnostics) = hk.scan(\n        one_step_loss, inputs, scan_variables)\n\n    # Re-wrap loss and diagnostics as DataArray and average them over time:\n    (loss, diagnostics) = jax.tree_util.tree_map(\n        lambda x: xarray_jax.DataArray(x, dims=('time', 'batch')).mean(  # pylint: disable=g-long-lambda\n            'time', skipna=False),\n        (per_timestep_losses, per_timestep_diagnostics))\n\n    return loss, diagnostics\n",
    "2": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Wrappers that take care of casting.\"\"\"\n\nimport contextlib\nfrom typing import Any, Mapping, Tuple\n\nimport chex\nfrom graphcast import predictor_base\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport xarray\n\n\nPyTree = Any\n\n\nclass Bfloat16Cast(predictor_base.Predictor):\n  \"\"\"Wrapper that casts all inputs to bfloat16 and outputs to targets dtype.\"\"\"\n\n  def __init__(self, predictor: predictor_base.Predictor, enabled: bool = True):\n    \"\"\"Inits the wrapper.\n\n    Args:\n      predictor: predictor being wrapped.\n      enabled: disables the wrapper if False, for simpler hyperparameter scans.\n\n    \"\"\"\n    self._enabled = enabled\n    self._predictor = predictor\n\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               **kwargs\n               ) -> xarray.Dataset:\n    if not self._enabled:\n      return self._predictor(inputs, targets_template, forcings, **kwargs)\n\n    with bfloat16_variable_view():\n      predictions = self._predictor(\n          *_all_inputs_to_bfloat16(inputs, targets_template, forcings),\n          **kwargs,)\n\n    predictions_dtype = infer_floating_dtype(predictions)  # pytype: disable=wrong-arg-types\n    if predictions_dtype != jnp.bfloat16:\n      raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n\n    targets_dtype = infer_floating_dtype(targets_template)  # pytype: disable=wrong-arg-types\n    return tree_map_cast(\n        predictions, input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n  def loss(self,\n           inputs: xarray.Dataset,\n           targets: xarray.Dataset,\n           forcings: xarray.Dataset,\n           **kwargs,\n           ) -> predictor_base.LossAndDiagnostics:\n    if not self._enabled:\n      return self._predictor.loss(inputs, targets, forcings, **kwargs)\n\n    with bfloat16_variable_view():\n      loss, scalars = self._predictor.loss(\n          *_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n\n    if loss.dtype != jnp.bfloat16:\n      raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n\n    targets_dtype = infer_floating_dtype(targets)  # pytype: disable=wrong-arg-types\n\n    # Note that casting back the loss to e.g. float32 should not affect data\n    # types of the backwards pass, because the first thing the backwards pass\n    # should do is to go backwards the casting op and cast back to bfloat16\n    # (and xprofs seem to confirm this).\n    return tree_map_cast((loss, scalars),\n                         input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n  def loss_and_predictions(  # pytype: disable=signature-mismatch  # jax-ndarray\n      self,\n      inputs: xarray.Dataset,\n      targets: xarray.Dataset,\n      forcings: xarray.Dataset,\n      **kwargs,\n      ) -> Tuple[predictor_base.LossAndDiagnostics,\n                 xarray.Dataset]:\n    if not self._enabled:\n      return self._predictor.loss_and_predictions(inputs, targets, forcings,  # pytype: disable=bad-return-type  # jax-ndarray\n                                                  **kwargs)\n\n    with bfloat16_variable_view():\n      (loss, scalars), predictions = self._predictor.loss_and_predictions(\n          *_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n\n    if loss.dtype != jnp.bfloat16:\n      raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n\n    predictions_dtype = infer_floating_dtype(predictions)  # pytype: disable=wrong-arg-types\n    if predictions_dtype != jnp.bfloat16:\n      raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n\n    targets_dtype = infer_floating_dtype(targets)  # pytype: disable=wrong-arg-types\n    return tree_map_cast(((loss, scalars), predictions),\n                         input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n\ndef infer_floating_dtype(data_vars: Mapping[str, chex.Array]) -> np.dtype:\n  \"\"\"Infers a floating dtype from an input mapping of data.\"\"\"\n  dtypes = {\n      v.dtype\n      for k, v in data_vars.items() if jnp.issubdtype(v.dtype, np.floating)}\n  if len(dtypes) != 1:\n    dtypes_and_shapes = {\n        k: (v.dtype, v.shape)\n        for k, v in data_vars.items() if jnp.issubdtype(v.dtype, np.floating)}\n    raise ValueError(\n        f'Did not found exactly one floating dtype {dtypes} in input variables:'\n        f'{dtypes_and_shapes}')\n  return list(dtypes)[0]\n\n\ndef _all_inputs_to_bfloat16(\n    inputs: xarray.Dataset,\n    targets: xarray.Dataset,\n    forcings: xarray.Dataset,\n    ) -> Tuple[xarray.Dataset,\n               xarray.Dataset,\n               xarray.Dataset]:\n  return (inputs.astype(jnp.bfloat16),\n          jax.tree_map(lambda x: x.astype(jnp.bfloat16), targets),\n          forcings.astype(jnp.bfloat16))\n\n\ndef tree_map_cast(inputs: PyTree, input_dtype: np.dtype, output_dtype: np.dtype,\n                  ) -> PyTree:\n  def cast_fn(x):\n    if x.dtype == input_dtype:\n      return x.astype(output_dtype)\n  return jax.tree_map(cast_fn, inputs)\n\n\n@contextlib.contextmanager\ndef bfloat16_variable_view(enabled: bool = True):\n  \"\"\"Context for Haiku modules with float32 params, but bfloat16 activations.\n\n  It works as follows:\n  * Every time a variable is requested to be created/set as np.bfloat16,\n    it will create an underlying float32 variable, instead.\n  * Every time a variable a variable is requested as bfloat16, it will check the\n    variable is of float32 type, and cast the variable to bfloat16.\n\n  Note the gradients are still computed and accumulated as float32, because\n  the params returned by init are float32, so the gradient function with\n  respect to the params will already include an implicit casting to float32.\n\n  Args:\n    enabled: Only enables bfloat16 behavior if True.\n\n  Yields:\n    None\n  \"\"\"\n\n  if enabled:\n    with hk.custom_creator(\n        _bfloat16_creator, state=True), hk.custom_getter(\n            _bfloat16_getter, state=True), hk.custom_setter(\n                _bfloat16_setter):\n      yield\n  else:\n    yield\n\n\ndef _bfloat16_creator(next_creator, shape, dtype, init, context):\n  \"\"\"Creates float32 variables when bfloat16 is requested.\"\"\"\n  if context.original_dtype == jnp.bfloat16:\n    dtype = jnp.float32\n  return next_creator(shape, dtype, init)\n\n\ndef _bfloat16_getter(next_getter, value, context):\n  \"\"\"Casts float32 to bfloat16 when bfloat16 was originally requested.\"\"\"\n  if context.original_dtype == jnp.bfloat16:\n    assert value.dtype == jnp.float32\n    value = value.astype(jnp.bfloat16)\n  return next_getter(value)\n\n\ndef _bfloat16_setter(next_setter, value, context):\n  \"\"\"Casts bfloat16 to float32 when bfloat16 was originally set.\"\"\"\n  if context.original_dtype == jnp.bfloat16:\n    value = value.astype(jnp.float32)\n  return next_setter(value)\n",
    "3": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Serialize and deserialize trees.\"\"\"\n\nimport dataclasses\nimport io\nimport types\nfrom typing import Any, BinaryIO, Optional, TypeVar\n\nimport numpy as np\n\n_T = TypeVar(\"_T\")\n\n\ndef dump(dest: BinaryIO, value: Any) -> None:\n  \"\"\"Dump a tree of dicts/dataclasses to a file object.\n\n  Args:\n    dest: a file object to write to.\n    value: A tree of dicts, lists, tuples and dataclasses of numpy arrays and\n      other basic types. Unions are not supported, other than Optional/None\n      which is only supported in dataclasses, not in dicts, lists or tuples.\n      All leaves must be coercible to a numpy array, and recoverable as a single\n      arg to a type.\n  \"\"\"\n  buffer = io.BytesIO()  # In case the destination doesn't support seeking.\n  np.savez(buffer, **_flatten(value))\n  dest.write(buffer.getvalue())\n\n\ndef load(source: BinaryIO, typ: type[_T]) -> _T:\n  \"\"\"Load from a file object and convert it to the specified type.\n\n  Args:\n    source: a file object to read from.\n    typ: a type object that acts as a schema for deserialization. It must match\n      what was serialized. If a type is Any, it will be returned however numpy\n      serialized it, which is what you want for a tree of numpy arrays.\n\n  Returns:\n    the deserialized value as the specified type.\n  \"\"\"\n  return _convert_types(typ, _unflatten(np.load(source)))\n\n\n_SEP = \":\"\n\n\ndef _flatten(tree: Any) -> dict[str, Any]:\n  \"\"\"Flatten a tree of dicts/dataclasses/lists/tuples to a single dict.\"\"\"\n  if dataclasses.is_dataclass(tree):\n    # Don't use dataclasses.asdict as it is recursive so skips dropping None.\n    tree = {f.name: v for f in dataclasses.fields(tree)\n            if (v := getattr(tree, f.name)) is not None}\n  elif isinstance(tree, (list, tuple)):\n    tree = dict(enumerate(tree))\n\n  assert isinstance(tree, dict)\n\n  flat = {}\n  for k, v in tree.items():\n    k = str(k)\n    assert _SEP not in k\n    if dataclasses.is_dataclass(v) or isinstance(v, (dict, list, tuple)):\n      for a, b in _flatten(v).items():\n        flat[f\"{k}{_SEP}{a}\"] = b\n    else:\n      assert v is not None\n      flat[k] = v\n  return flat\n\n\ndef _unflatten(flat: dict[str, Any]) -> dict[str, Any]:\n  \"\"\"Unflatten a dict to a tree of dicts.\"\"\"\n  tree = {}\n  for flat_key, v in flat.items():\n    node = tree\n    keys = flat_key.split(_SEP)\n    for k in keys[:-1]:\n      if k not in node:\n        node[k] = {}\n      node = node[k]\n    node[keys[-1]] = v\n  return tree\n\n\ndef _convert_types(typ: type[_T], value: Any) -> _T:\n  \"\"\"Convert some structure into the given type. The structures must match.\"\"\"\n  if typ in (Any, ...):\n    return value\n\n  if typ in (int, float, str, bool):\n    return typ(value)\n\n  if typ is np.ndarray:\n    assert isinstance(value, np.ndarray)\n    return value\n\n  if dataclasses.is_dataclass(typ):\n    kwargs = {}\n    for f in dataclasses.fields(typ):\n      # Only support Optional for dataclasses, as numpy can't serialize it\n      # directly (without pickle), and dataclasses are the only case where we\n      # can know the full set of values and types and therefore know the\n      # non-existence must mean None.\n      if isinstance(f.type, (types.UnionType, type(Optional[int]))):\n        constructors = [t for t in f.type.__args__ if t is not types.NoneType]\n        if len(constructors) != 1:\n          raise TypeError(\n              \"Optional works, Union with anything except None doesn't\")\n        if f.name not in value:\n          kwargs[f.name] = None\n          continue\n        constructor = constructors[0]\n      else:\n        constructor = f.type\n\n      if f.name in value:\n        kwargs[f.name] = _convert_types(constructor, value[f.name])\n      else:\n        raise ValueError(f\"Missing value: {f.name}\")\n    return typ(**kwargs)\n\n  base_type = getattr(typ, \"__origin__\", None)\n\n  if base_type is dict:\n    assert len(typ.__args__) == 2\n    key_type, value_type = typ.__args__\n    return {_convert_types(key_type, k): _convert_types(value_type, v)\n            for k, v in value.items()}\n\n  if base_type is list:\n    assert len(typ.__args__) == 1\n    value_type = typ.__args__[0]\n    return [_convert_types(value_type, v)\n            for _, v in sorted(value.items(), key=lambda x: int(x[0]))]\n\n  if base_type is tuple:\n    if len(typ.__args__) == 2 and typ.__args__[1] == ...:\n      # An arbitrary length tuple of a single type, eg: tuple[int, ...]\n      value_type = typ.__args__[0]\n      return tuple(_convert_types(value_type, v)\n                   for _, v in sorted(value.items(), key=lambda x: int(x[0])))\n    else:\n      # A fixed length tuple of arbitrary types, eg: tuple[int, str, float]\n      assert len(typ.__args__) == len(value)\n      return tuple(\n          _convert_types(t, v)\n          for t, (_, v) in zip(\n              typ.__args__, sorted(value.items(), key=lambda x: int(x[0]))))\n\n  # This is probably unreachable with reasonable serializable inputs.\n  try:\n    return typ(value)\n  except TypeError as e:\n    raise TypeError(\n        \"_convert_types expects the type argument to be a dataclass defined \"\n        \"with types that are valid constructors (eg tuple is fine, Tuple \"\n        \"isn't), and accept a numpy array as the sole argument.\") from e\n",
    "4": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Check that the checkpoint serialization is reversable.\"\"\"\n\nimport dataclasses\nimport io\nfrom typing import Any, Optional, Union\n\nfrom absl.testing import absltest\nfrom graphcast import checkpoint\nimport numpy as np\n\n\n@dataclasses.dataclass\nclass SubConfig:\n  a: int\n  b: str\n\n\n@dataclasses.dataclass\nclass Config:\n  bt: bool\n  bf: bool\n  i: int\n  f: float\n  o1: Optional[int]\n  o2: Optional[int]\n  o3: Union[int, None]\n  o4: Union[int, None]\n  o5: int | None\n  o6: int | None\n  li: list[int]\n  ls: list[str]\n  ldc: list[SubConfig]\n  tf: tuple[float, ...]\n  ts: tuple[str, ...]\n  t: tuple[str, int, SubConfig]\n  tdc: tuple[SubConfig, ...]\n  dsi: dict[str, int]\n  dss: dict[str, str]\n  dis: dict[int, str]\n  dsdis: dict[str, dict[int, str]]\n  dc: SubConfig\n  dco: Optional[SubConfig]\n  ddc: dict[str, SubConfig]\n\n\n@dataclasses.dataclass\nclass Checkpoint:\n  params: dict[str, Any]\n  config: Config\n\n\nclass DataclassTest(absltest.TestCase):\n\n  def test_serialize_dataclass(self):\n    ckpt = Checkpoint(\n        params={\n            \"layer1\": {\n                \"w\": np.arange(10).reshape(2, 5),\n                \"b\": np.array([2, 6]),\n            },\n            \"layer2\": {\n                \"w\": np.arange(8).reshape(2, 4),\n                \"b\": np.array([2, 6]),\n            },\n            \"blah\": np.array([3, 9]),\n        },\n        config=Config(\n            bt=True,\n            bf=False,\n            i=42,\n            f=3.14,\n            o1=1,\n            o2=None,\n            o3=2,\n            o4=None,\n            o5=3,\n            o6=None,\n            li=[12, 9, 7, 15, 16, 14, 1, 6, 11, 4, 10, 5, 13, 3, 8, 2],\n            ls=list(\"qhjfdxtpzgemryoikwvblcaus\"),\n            ldc=[SubConfig(1, \"hello\"), SubConfig(2, \"world\")],\n            tf=(1, 4, 2, 10, 5, 9, 13, 16, 15, 8, 12, 7, 11, 14, 3, 6),\n            ts=(\"hello\", \"world\"),\n            t=(\"foo\", 42, SubConfig(1, \"bar\")),\n            tdc=(SubConfig(1, \"hello\"), SubConfig(2, \"world\")),\n            dsi={\"a\": 1, \"b\": 2, \"c\": 3},\n            dss={\"d\": \"e\", \"f\": \"g\"},\n            dis={1: \"a\", 2: \"b\", 3: \"c\"},\n            dsdis={\"a\": {1: \"hello\", 2: \"world\"}, \"b\": {1: \"world\"}},\n            dc=SubConfig(1, \"hello\"),\n            dco=None,\n            ddc={\"a\": SubConfig(1, \"hello\"), \"b\": SubConfig(2, \"world\")},\n        ))\n\n    buffer = io.BytesIO()\n    checkpoint.dump(buffer, ckpt)\n    buffer.seek(0)\n    ckpt2 = checkpoint.load(buffer, Checkpoint)\n    np.testing.assert_array_equal(ckpt.params[\"layer1\"][\"w\"],\n                                  ckpt2.params[\"layer1\"][\"w\"])\n    np.testing.assert_array_equal(ckpt.params[\"layer1\"][\"b\"],\n                                  ckpt2.params[\"layer1\"][\"b\"])\n    np.testing.assert_array_equal(ckpt.params[\"layer2\"][\"w\"],\n                                  ckpt2.params[\"layer2\"][\"w\"])\n    np.testing.assert_array_equal(ckpt.params[\"layer2\"][\"b\"],\n                                  ckpt2.params[\"layer2\"][\"b\"])\n    np.testing.assert_array_equal(ckpt.params[\"blah\"], ckpt2.params[\"blah\"])\n    self.assertEqual(ckpt.config, ckpt2.config)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "5": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Dataset utilities.\"\"\"\n\nfrom typing import Any, Mapping, Sequence, Tuple, Union\n\nfrom graphcast import solar_radiation\nimport numpy as np\nimport pandas as pd\nimport xarray\n\nTimedeltaLike = Any  # Something convertible to pd.Timedelta.\nTimedeltaStr = str  # A string convertible to pd.Timedelta.\n\nTargetLeadTimes = Union[\n    TimedeltaLike,\n    Sequence[TimedeltaLike],\n    slice  # with TimedeltaLike as its start and stop.\n]\n\n_SEC_PER_HOUR = 3600\n_HOUR_PER_DAY = 24\nSEC_PER_DAY = _SEC_PER_HOUR * _HOUR_PER_DAY\n_AVG_DAY_PER_YEAR = 365.24219\nAVG_SEC_PER_YEAR = SEC_PER_DAY * _AVG_DAY_PER_YEAR\n\nDAY_PROGRESS = \"day_progress\"\nYEAR_PROGRESS = \"year_progress\"\n_DERIVED_VARS = {\n    DAY_PROGRESS,\n    f\"{DAY_PROGRESS}_sin\",\n    f\"{DAY_PROGRESS}_cos\",\n    YEAR_PROGRESS,\n    f\"{YEAR_PROGRESS}_sin\",\n    f\"{YEAR_PROGRESS}_cos\",\n}\nTISR = \"toa_incident_solar_radiation\"\n\n\ndef get_year_progress(seconds_since_epoch: np.ndarray) -> np.ndarray:\n  \"\"\"Computes year progress for times in seconds.\n\n  Args:\n    seconds_since_epoch: Times in seconds since the \"epoch\" (the point at which\n      UNIX time starts).\n\n  Returns:\n    Year progress normalized to be in the [0, 1) interval for each time point.\n  \"\"\"\n\n  # Start with the pure integer division, and then float at the very end.\n  # We will try to keep as much precision as possible.\n  years_since_epoch = (\n      seconds_since_epoch / SEC_PER_DAY / np.float64(_AVG_DAY_PER_YEAR)\n  )\n  # Note depending on how these ops are down, we may end up with a \"weak_type\"\n  # which can cause issues in subtle ways, and hard to track here.\n  # In any case, casting to float32 should get rid of the weak type.\n  # [0, 1.) Interval.\n  return np.mod(years_since_epoch, 1.0).astype(np.float32)\n\n\ndef get_day_progress(\n    seconds_since_epoch: np.ndarray,\n    longitude: np.ndarray,\n) -> np.ndarray:\n  \"\"\"Computes day progress for times in seconds at each longitude.\n\n  Args:\n    seconds_since_epoch: 1D array of times in seconds since the 'epoch' (the\n      point at which UNIX time starts).\n    longitude: 1D array of longitudes at which day progress is computed.\n\n  Returns:\n    2D array of day progress values normalized to be in the [0, 1) inverval\n      for each time point at each longitude.\n  \"\"\"\n\n  # [0.0, 1.0) Interval.\n  day_progress_greenwich = (\n      np.mod(seconds_since_epoch, SEC_PER_DAY) / SEC_PER_DAY\n  )\n\n  # Offset the day progress to the longitude of each point on Earth.\n  longitude_offsets = np.deg2rad(longitude) / (2 * np.pi)\n  day_progress = np.mod(\n      day_progress_greenwich[..., np.newaxis] + longitude_offsets, 1.0\n  )\n  return day_progress.astype(np.float32)\n\n\ndef featurize_progress(\n    name: str, dims: Sequence[str], progress: np.ndarray\n) -> Mapping[str, xarray.Variable]:\n  \"\"\"Derives features used by ML models from the `progress` variable.\n\n  Args:\n    name: Base variable name from which features are derived.\n    dims: List of the output feature dimensions, e.g. (\"day\", \"lon\").\n    progress: Progress variable values.\n\n  Returns:\n    Dictionary of xarray variables derived from the `progress` values. It\n    includes the original `progress` variable along with its sin and cos\n    transformations.\n\n  Raises:\n    ValueError if the number of feature dimensions is not equal to the number\n      of data dimensions.\n  \"\"\"\n  if len(dims) != progress.ndim:\n    raise ValueError(\n        f\"Number of feature dimensions ({len(dims)}) must be equal to the\"\n        f\" number of data dimensions: {progress.ndim}.\"\n    )\n  progress_phase = progress * (2 * np.pi)\n  return {\n      name: xarray.Variable(dims, progress),\n      name + \"_sin\": xarray.Variable(dims, np.sin(progress_phase)),\n      name + \"_cos\": xarray.Variable(dims, np.cos(progress_phase)),\n  }\n\n\ndef add_derived_vars(data: xarray.Dataset) -> None:\n  \"\"\"Adds year and day progress features to `data` in place if missing.\n\n  Args:\n    data: Xarray dataset to which derived features will be added.\n\n  Raises:\n    ValueError if `datetime` or `lon` are not in `data` coordinates.\n  \"\"\"\n\n  for coord in (\"datetime\", \"lon\"):\n    if coord not in data.coords:\n      raise ValueError(f\"'{coord}' must be in `data` coordinates.\")\n\n  # Compute seconds since epoch.\n  # Note `data.coords[\"datetime\"].astype(\"datetime64[s]\").astype(np.int64)`\n  # does not work as xarrays always cast dates into nanoseconds!\n  seconds_since_epoch = (\n      data.coords[\"datetime\"].data.astype(\"datetime64[s]\").astype(np.int64)\n  )\n  batch_dim = (\"batch\",) if \"batch\" in data.dims else ()\n\n  # Add year progress features if missing.\n  if YEAR_PROGRESS not in data.data_vars:\n    year_progress = get_year_progress(seconds_since_epoch)\n    data.update(\n        featurize_progress(\n            name=YEAR_PROGRESS,\n            dims=batch_dim + (\"time\",),\n            progress=year_progress,\n        )\n    )\n\n  # Add day progress features if missing.\n  if DAY_PROGRESS not in data.data_vars:\n    longitude_coord = data.coords[\"lon\"]\n    day_progress = get_day_progress(seconds_since_epoch, longitude_coord.data)\n    data.update(\n        featurize_progress(\n            name=DAY_PROGRESS,\n            dims=batch_dim + (\"time\",) + longitude_coord.dims,\n            progress=day_progress,\n        )\n    )\n\n\ndef add_tisr_var(data: xarray.Dataset) -> None:\n  \"\"\"Adds TISR feature to `data` in place if missing.\n\n  Args:\n    data: Xarray dataset to which TISR feature will be added.\n\n  Raises:\n    ValueError if `datetime`, 'lat', or `lon` are not in `data` coordinates.\n  \"\"\"\n\n  if TISR in data.data_vars:\n    return\n\n  for coord in (\"datetime\", \"lat\", \"lon\"):\n    if coord not in data.coords:\n      raise ValueError(f\"'{coord}' must be in `data` coordinates.\")\n\n  # Remove `batch` dimension of size one if present. An error will be raised if\n  # the `batch` dimension exists and has size greater than one.\n  data_no_batch = data.squeeze(\"batch\") if \"batch\" in data.dims else data\n\n  tisr = solar_radiation.get_toa_incident_solar_radiation_for_xarray(\n      data_no_batch, use_jit=True\n  )\n\n  if \"batch\" in data.dims:\n    tisr = tisr.expand_dims(\"batch\", axis=0)\n\n  data.update({TISR: tisr})\n\n\ndef extract_input_target_times(\n    dataset: xarray.Dataset,\n    input_duration: TimedeltaLike,\n    target_lead_times: TargetLeadTimes,\n    ) -> Tuple[xarray.Dataset, xarray.Dataset]:\n  \"\"\"Extracts inputs and targets for prediction, from a Dataset with a time dim.\n\n  The input period is assumed to be contiguous (specified by a duration), but\n  the targets can be a list of arbitrary lead times.\n\n  Examples:\n\n    # Use 18 hours of data as inputs, and two specific lead times as targets:\n    # 3 days and 5 days after the final input.\n    extract_inputs_targets(\n        dataset,\n        input_duration='18h',\n        target_lead_times=('3d', '5d')\n    )\n\n    # Use 1 day of data as input, and all lead times between 6 hours and\n    # 24 hours inclusive as targets. Demonstrates a friendlier supported string\n    # syntax.\n    extract_inputs_targets(\n        dataset,\n        input_duration='1 day',\n        target_lead_times=slice('6 hours', '24 hours')\n    )\n\n    # Just use a single target lead time of 3 days:\n    extract_inputs_targets(\n        dataset,\n        input_duration='24h',\n        target_lead_times='3d'\n    )\n\n  Args:\n    dataset: An xarray.Dataset with a 'time' dimension whose coordinates are\n      timedeltas. It's assumed that the time coordinates have a fixed offset /\n      time resolution, and that the input_duration and target_lead_times are\n      multiples of this.\n    input_duration: pandas.Timedelta or something convertible to it (e.g. a\n      shorthand string like '6h' or '5d12h').\n    target_lead_times: Either a single lead time, a slice with start and stop\n      (inclusive) lead times, or a sequence of lead times. Lead times should be\n      Timedeltas (or something convertible to). They are given relative to the\n      final input timestep, and should be positive.\n\n  Returns:\n    inputs:\n    targets:\n      Two datasets with the same shape as the input dataset except that a\n      selection has been made from the time axis, and the origin of the\n      time coordinate will be shifted to refer to lead times relative to the\n      final input timestep. So for inputs the times will end at lead time 0,\n      for targets the time coordinates will refer to the lead times requested.\n  \"\"\"\n\n  (target_lead_times, target_duration\n   ) = _process_target_lead_times_and_get_duration(target_lead_times)\n\n  # Shift the coordinates for the time axis so that a timedelta of zero\n  # corresponds to the forecast reference time. That is, the final timestep\n  # that's available as input to the forecast, with all following timesteps\n  # forming the target period which needs to be predicted.\n  # This means the time coordinates are now forecast lead times.\n  time = dataset.coords[\"time\"]\n  dataset = dataset.assign_coords(time=time + target_duration - time[-1])\n\n  # Slice out targets:\n  targets = dataset.sel({\"time\": target_lead_times})\n\n  input_duration = pd.Timedelta(input_duration)\n  # Both endpoints are inclusive with label-based slicing, so we offset by a\n  # small epsilon to make one of the endpoints non-inclusive:\n  zero = pd.Timedelta(0)\n  epsilon = pd.Timedelta(1, \"ns\")\n  inputs = dataset.sel({\"time\": slice(-input_duration + epsilon, zero)})\n  return inputs, targets\n\n\ndef _process_target_lead_times_and_get_duration(\n    target_lead_times: TargetLeadTimes) -> TimedeltaLike:\n  \"\"\"Returns the minimum duration for the target lead times.\"\"\"\n  if isinstance(target_lead_times, slice):\n    # A slice of lead times. xarray already accepts timedelta-like values for\n    # the begin/end/step of the slice.\n    if target_lead_times.start is None:\n      # If the start isn't specified, we assume it starts at the next timestep\n      # after lead time 0 (lead time 0 is the final input timestep):\n      target_lead_times = slice(\n          pd.Timedelta(1, \"ns\"), target_lead_times.stop, target_lead_times.step\n      )\n    target_duration = pd.Timedelta(target_lead_times.stop)\n  else:\n    if not isinstance(target_lead_times, (list, tuple, set)):\n      # A single lead time, which we wrap as a length-1 array to ensure there\n      # still remains a time dimension (here of length 1) for consistency.\n      target_lead_times = [target_lead_times]\n\n    # A list of multiple (not necessarily contiguous) lead times:\n    target_lead_times = [pd.Timedelta(x) for x in target_lead_times]\n    target_lead_times.sort()\n    target_duration = target_lead_times[-1]\n  return target_lead_times, target_duration\n\n\ndef extract_inputs_targets_forcings(\n    dataset: xarray.Dataset,\n    *,\n    input_variables: Tuple[str, ...],\n    target_variables: Tuple[str, ...],\n    forcing_variables: Tuple[str, ...],\n    pressure_levels: Tuple[int, ...],\n    input_duration: TimedeltaLike,\n    target_lead_times: TargetLeadTimes,\n    ) -> Tuple[xarray.Dataset, xarray.Dataset, xarray.Dataset]:\n  \"\"\"Extracts inputs, targets and forcings according to requirements.\"\"\"\n  dataset = dataset.sel(level=list(pressure_levels))\n\n  # \"Forcings\" include derived variables that do not exist in the original ERA5\n  # or HRES datasets, as well as other variables (e.g. tisr) that need to be\n  # computed manually for the target lead times. Compute the requested ones.\n  if set(forcing_variables) & _DERIVED_VARS:\n    add_derived_vars(dataset)\n  if set(forcing_variables) & {TISR}:\n    add_tisr_var(dataset)\n\n  # `datetime` is needed by add_derived_vars but breaks autoregressive rollouts.\n  dataset = dataset.drop_vars(\"datetime\")\n\n  inputs, targets = extract_input_target_times(\n      dataset,\n      input_duration=input_duration,\n      target_lead_times=target_lead_times)\n\n  if set(forcing_variables) & set(target_variables):\n    raise ValueError(\n        f\"Forcing variables {forcing_variables} should not \"\n        f\"overlap with target variables {target_variables}.\"\n    )\n\n  inputs = inputs[list(input_variables)]\n  # The forcing uses the same time coordinates as the target.\n  forcings = targets[list(forcing_variables)]\n  targets = targets[list(target_variables)]\n\n  return inputs, targets, forcings\n",
    "6": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for `data_utils.py`.\"\"\"\n\nimport datetime\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom graphcast import data_utils\nimport numpy as np\nimport xarray as xa\n\n\nclass DataUtilsTest(parameterized.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    # Fix the seed for reproducibility.\n    np.random.seed(0)\n\n  def test_year_progress_is_zero_at_year_start_or_end(self):\n    year_progress = data_utils.get_year_progress(\n        np.array([\n            0,\n            data_utils.AVG_SEC_PER_YEAR,\n            data_utils.AVG_SEC_PER_YEAR * 42,  # 42 years.\n        ])\n    )\n    np.testing.assert_array_equal(year_progress, np.zeros(year_progress.shape))\n\n  def test_year_progress_is_almost_one_before_year_ends(self):\n    year_progress = data_utils.get_year_progress(\n        np.array([\n            data_utils.AVG_SEC_PER_YEAR - 1,\n            (data_utils.AVG_SEC_PER_YEAR - 1) * 42,  # ~42 years\n        ])\n    )\n    with self.subTest(\"Year progress values are close to 1\"):\n      self.assertTrue(np.all(year_progress > 0.999))\n    with self.subTest(\"Year progress values != 1\"):\n      self.assertTrue(np.all(year_progress < 1.0))\n\n  def test_day_progress_computes_for_all_times_and_longitudes(self):\n    times = np.random.randint(low=0, high=1e10, size=10)\n    longitudes = np.arange(0, 360.0, 1.0)\n    day_progress = data_utils.get_day_progress(times, longitudes)\n    with self.subTest(\"Day progress is computed for all times and longinutes\"):\n      self.assertSequenceEqual(\n          day_progress.shape, (len(times), len(longitudes))\n      )\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"random_date_1\",\n          year=1988,\n          month=11,\n          day=7,\n          hour=2,\n          minute=45,\n          second=34,\n      ),\n      dict(\n          testcase_name=\"random_date_2\",\n          year=2022,\n          month=3,\n          day=12,\n          hour=7,\n          minute=1,\n          second=0,\n      ),\n  )\n  def test_day_progress_is_in_between_zero_and_one(\n      self, year, month, day, hour, minute, second\n  ):\n    # Datetime from a timestamp.\n    dt = datetime.datetime(year, month, day, hour, minute, second)\n    # Epoch time.\n    epoch_time = datetime.datetime(1970, 1, 1)\n    # Seconds since epoch.\n    seconds_since_epoch = np.array([(dt - epoch_time).total_seconds()])\n\n    # Longitudes with 1 degree resolution.\n    longitudes = np.arange(0, 360.0, 1.0)\n\n    day_progress = data_utils.get_day_progress(seconds_since_epoch, longitudes)\n    with self.subTest(\"Day progress >= 0\"):\n      self.assertTrue(np.all(day_progress >= 0.0))\n    with self.subTest(\"Day progress < 1\"):\n      self.assertTrue(np.all(day_progress < 1.0))\n\n  def test_day_progress_is_zero_at_day_start_or_end(self):\n    day_progress = data_utils.get_day_progress(\n        seconds_since_epoch=np.array([\n            0,\n            data_utils.SEC_PER_DAY,\n            data_utils.SEC_PER_DAY * 42,  # 42 days.\n        ]),\n        longitude=np.array([0.0]),\n    )\n    np.testing.assert_array_equal(day_progress, np.zeros(day_progress.shape))\n\n  def test_day_progress_specific_value(self):\n    day_progress = data_utils.get_day_progress(\n        seconds_since_epoch=np.array([123]),\n        longitude=np.array([0.0]),\n    )\n    np.testing.assert_array_almost_equal(\n        day_progress, np.array([[0.00142361]]), decimal=6\n    )\n\n  def test_featurize_progress_valid_values_and_dimensions(self):\n    day_progress = np.array([0.0, 0.45, 0.213])\n    feature_dimensions = (\"time\",)\n    progress_features = data_utils.featurize_progress(\n        name=\"day_progress\", dims=feature_dimensions, progress=day_progress\n    )\n    for feature in progress_features.values():\n      with self.subTest(f\"Valid dimensions for {feature}\"):\n        self.assertSequenceEqual(feature.dims, feature_dimensions)\n\n    with self.subTest(\"Valid values for day_progress\"):\n      np.testing.assert_array_equal(\n          day_progress, progress_features[\"day_progress\"].values\n      )\n\n    with self.subTest(\"Valid values for day_progress_sin\"):\n      np.testing.assert_array_almost_equal(\n          np.array([0.0, 0.30901699, 0.97309851]),\n          progress_features[\"day_progress_sin\"].values,\n          decimal=6,\n      )\n\n    with self.subTest(\"Valid values for day_progress_cos\"):\n      np.testing.assert_array_almost_equal(\n          np.array([1.0, -0.95105652, 0.23038943]),\n          progress_features[\"day_progress_cos\"].values,\n          decimal=6,\n      )\n\n  def test_featurize_progress_invalid_dimensions(self):\n    year_progress = np.array([0.0, 0.45, 0.213])\n    feature_dimensions = (\"time\", \"longitude\")\n    with self.assertRaises(ValueError):\n      data_utils.featurize_progress(\n          name=\"year_progress\", dims=feature_dimensions, progress=year_progress\n      )\n\n  def test_add_derived_vars_variables_added(self):\n    data = xa.Dataset(\n        data_vars={\n            \"var1\": ([\"x\", \"lon\", \"datetime\"], 8 * np.random.randn(2, 2, 3))\n        },\n        coords={\n            \"lon\": np.array([0.0, 0.5]),\n            \"datetime\": np.array([\n                datetime.datetime(2021, 1, 1),\n                datetime.datetime(2023, 1, 1),\n                datetime.datetime(2023, 1, 3),\n            ]),\n        },\n    )\n    data_utils.add_derived_vars(data)\n    all_variables = set(data.variables)\n\n    with self.subTest(\"Original value was not removed\"):\n      self.assertIn(\"var1\", all_variables)\n    with self.subTest(\"Year progress feature was added\"):\n      self.assertIn(data_utils.YEAR_PROGRESS, all_variables)\n    with self.subTest(\"Day progress feature was added\"):\n      self.assertIn(data_utils.DAY_PROGRESS, all_variables)\n\n  def test_add_derived_vars_existing_vars_not_overridden(self):\n    dims = [\"x\", \"lon\", \"datetime\"]\n    data = xa.Dataset(\n        data_vars={\n            \"var1\": (dims, 8 * np.random.randn(2, 2, 3)),\n            data_utils.YEAR_PROGRESS: (dims, np.full((2, 2, 3), 0.111)),\n            data_utils.DAY_PROGRESS: (dims, np.full((2, 2, 3), 0.222)),\n        },\n        coords={\n            \"lon\": np.array([0.0, 0.5]),\n            \"datetime\": np.array([\n                datetime.datetime(2021, 1, 1),\n                datetime.datetime(2023, 1, 1),\n                datetime.datetime(2023, 1, 3),\n            ]),\n        },\n    )\n\n    data_utils.add_derived_vars(data)\n\n    with self.subTest(\"Year progress feature was not overridden\"):\n      np.testing.assert_allclose(data[data_utils.YEAR_PROGRESS], 0.111)\n    with self.subTest(\"Day progress feature was not overridden\"):\n      np.testing.assert_allclose(data[data_utils.DAY_PROGRESS], 0.222)\n\n  @parameterized.named_parameters(\n      dict(testcase_name=\"missing_datetime\", coord_name=\"lon\"),\n      dict(testcase_name=\"missing_lon\", coord_name=\"datetime\"),\n  )\n  def test_add_derived_vars_missing_coordinate_raises_value_error(\n      self, coord_name\n  ):\n    with self.subTest(f\"Missing {coord_name} coordinate\"):\n      data = xa.Dataset(\n          data_vars={\"var1\": ([\"x\", coord_name], 8 * np.random.randn(2, 2))},\n          coords={\n              coord_name: np.array([0.0, 0.5]),\n          },\n      )\n      with self.assertRaises(ValueError):\n        data_utils.add_derived_vars(data)\n\n  def test_add_tisr_var_variable_added(self):\n    data = xa.Dataset(\n        data_vars={\n            \"var1\": ([\"time\", \"lat\", \"lon\"], np.full((2, 2, 2), 8.0))\n        },\n        coords={\n            \"lat\": np.array([2.0, 1.0]),\n            \"lon\": np.array([0.0, 0.5]),\n            \"time\": np.array([100, 200], dtype=\"timedelta64[s]\"),\n            \"datetime\": xa.Variable(\n                \"time\", np.array([10, 20], dtype=\"datetime64[D]\")\n            ),\n        },\n    )\n\n    data_utils.add_tisr_var(data)\n\n    self.assertIn(data_utils.TISR, set(data.variables))\n\n  def test_add_tisr_var_existing_var_not_overridden(self):\n    dims = [\"time\", \"lat\", \"lon\"]\n    data = xa.Dataset(\n        data_vars={\n            \"var1\": (dims, np.full((2, 2, 2), 8.0)),\n            data_utils.TISR: (dims, np.full((2, 2, 2), 1200.0)),\n        },\n        coords={\n            \"lat\": np.array([2.0, 1.0]),\n            \"lon\": np.array([0.0, 0.5]),\n            \"time\": np.array([100, 200], dtype=\"timedelta64[s]\"),\n            \"datetime\": xa.Variable(\n                \"time\", np.array([10, 20], dtype=\"datetime64[D]\")\n            ),\n        },\n    )\n\n    data_utils.add_derived_vars(data)\n\n    np.testing.assert_allclose(data[data_utils.TISR], 1200.0)\n\n  def test_add_tisr_var_works_with_batch_dim_size_one(self):\n    data = xa.Dataset(\n        data_vars={\n            \"var1\": (\n                [\"batch\", \"time\", \"lat\", \"lon\"],\n                np.full((1, 2, 2, 2), 8.0),\n            )\n        },\n        coords={\n            \"lat\": np.array([2.0, 1.0]),\n            \"lon\": np.array([0.0, 0.5]),\n            \"time\": np.array([100, 200], dtype=\"timedelta64[s]\"),\n            \"datetime\": xa.Variable(\n                (\"batch\", \"time\"), np.array([[10, 20]], dtype=\"datetime64[D]\")\n            ),\n        },\n    )\n\n    data_utils.add_tisr_var(data)\n\n    self.assertIn(data_utils.TISR, set(data.variables))\n\n  def test_add_tisr_var_fails_with_batch_dim_size_greater_than_one(self):\n    data = xa.Dataset(\n        data_vars={\n            \"var1\": (\n                [\"batch\", \"time\", \"lat\", \"lon\"],\n                np.full((2, 2, 2, 2), 8.0),\n            )\n        },\n        coords={\n            \"lat\": np.array([2.0, 1.0]),\n            \"lon\": np.array([0.0, 0.5]),\n            \"time\": np.array([100, 200], dtype=\"timedelta64[s]\"),\n            \"datetime\": xa.Variable(\n                (\"batch\", \"time\"),\n                np.array([[10, 20], [100, 200]], dtype=\"datetime64[D]\"),\n            ),\n        },\n    )\n\n    with self.assertRaisesRegex(ValueError, r\"cannot select a dimension\"):\n      data_utils.add_tisr_var(data)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "7": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"JAX implementation of Graph Networks Simulator.\n\nGeneralization to TypedGraphs of the deep Graph Neural Network from:\n\n@inproceedings{pfaff2021learning,\n  title={Learning Mesh-Based Simulation with Graph Networks},\n  author={Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and\n      Battaglia, Peter},\n  booktitle={International Conference on Learning Representations},\n  year={2021}\n}\n\n@inproceedings{sanchez2020learning,\n  title={Learning to simulate complex physics with graph networks},\n  author={Sanchez-Gonzalez, Alvaro and Godwin, Jonathan and Pfaff, Tobias and\n      Ying, Rex and Leskovec, Jure and Battaglia, Peter},\n  booktitle={International conference on machine learning},\n  pages={8459--8468},\n  year={2020},\n  organization={PMLR}\n}\n\"\"\"\n\nfrom typing import Mapping, Optional\n\nfrom graphcast import typed_graph\nfrom graphcast import typed_graph_net\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport jraph\n\n\nclass DeepTypedGraphNet(hk.Module):\n  \"\"\"Deep Graph Neural Network.\n\n  It works with TypedGraphs with typed nodes and edges. It runs message\n  passing on all of the node sets and all of the edge sets in the graph. For\n  each message passing step a `typed_graph_net.InteractionNetwork` is used to\n  update the full TypedGraph by using different MLPs for each of the node sets\n  and each of the edge sets.\n\n  If embed_{nodes,edges} is specified the node/edge features will be embedded\n  into a fixed dimensionality before running the first step of message passing.\n\n  If {node,edge}_output_size the final node/edge features will be embedded into\n  the specified output size.\n\n  This class may be used for shared or unshared message passing:\n  * num_message_passing_steps = N, num_processor_repetitions = 1, gives\n    N layers of message passing with fully unshared weights:\n    [W_1, W_2, ... , W_M] (default)\n  * num_message_passing_steps = 1, num_processor_repetitions = M, gives\n    N layers of message passing with fully shared weights:\n    [W_1] * M\n  * num_message_passing_steps = N, num_processor_repetitions = M, gives\n    M*N layers of message passing with both shared and unshared message passing\n    such that the weights used at each iteration are:\n    [W_1, W_2, ... , W_N] * M\n\n  \"\"\"\n\n  def __init__(self,\n               *,\n               node_latent_size: Mapping[str, int],\n               edge_latent_size: Mapping[str, int],\n               mlp_hidden_size: int,\n               mlp_num_hidden_layers: int,\n               num_message_passing_steps: int,\n               num_processor_repetitions: int = 1,\n               embed_nodes: bool = True,\n               embed_edges: bool = True,\n               node_output_size: Optional[Mapping[str, int]] = None,\n               edge_output_size: Optional[Mapping[str, int]] = None,\n               include_sent_messages_in_node_update: bool = False,\n               use_layer_norm: bool = True,\n               activation: str = \"relu\",\n               f32_aggregation: bool = False,\n               aggregate_edges_for_nodes_fn: str = \"segment_sum\",\n               aggregate_normalization: Optional[float] = None,\n               name: str = \"DeepTypedGraphNet\"):\n    \"\"\"Inits the model.\n\n    Args:\n      node_latent_size: Size of the node latent representations.\n      edge_latent_size: Size of the edge latent representations.\n      mlp_hidden_size: Hidden layer size for all MLPs.\n      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n      num_message_passing_steps: Number of unshared message passing steps\n         in the processor steps.\n      num_processor_repetitions: Number of times that the same processor is\n         applied sequencially.\n      embed_nodes: If False, the node embedder will be omitted.\n      embed_edges: If False, the edge embedder will be omitted.\n      node_output_size: Size of the output node representations for\n         each node type. For node types not specified here, the latent node\n         representation from the output of the processor will be returned.\n      edge_output_size: Size of the output edge representations for\n         each edge type. For edge types not specified here, the latent edge\n         representation from the output of the processor will be returned.\n      include_sent_messages_in_node_update: Whether to include pooled sent\n          messages from each node in the node update.\n      use_layer_norm: Whether it uses layer norm or not.\n      activation: name of activation function.\n      f32_aggregation: Use float32 in the edge aggregation.\n      aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n        node.\n      aggregate_normalization: An optional constant that normalizes the output\n        of aggregate_edges_for_nodes_fn. For context, this can be used to\n        reduce the shock the model undergoes when switching resolution, which\n        increase the number of edges connected to a node. In particular, this is\n        useful when using segment_sum, but should not be combined with\n        segment_mean.\n      name: Name of the model.\n    \"\"\"\n\n    super().__init__(name=name)\n\n    self._node_latent_size = node_latent_size\n    self._edge_latent_size = edge_latent_size\n    self._mlp_hidden_size = mlp_hidden_size\n    self._mlp_num_hidden_layers = mlp_num_hidden_layers\n    self._num_message_passing_steps = num_message_passing_steps\n    self._num_processor_repetitions = num_processor_repetitions\n    self._embed_nodes = embed_nodes\n    self._embed_edges = embed_edges\n    self._node_output_size = node_output_size\n    self._edge_output_size = edge_output_size\n    self._include_sent_messages_in_node_update = (\n        include_sent_messages_in_node_update)\n    self._use_layer_norm = use_layer_norm\n    self._activation = _get_activation_fn(activation)\n    self._initialized = False\n    self._f32_aggregation = f32_aggregation\n    self._aggregate_edges_for_nodes_fn = _get_aggregate_edges_for_nodes_fn(\n        aggregate_edges_for_nodes_fn)\n    self._aggregate_normalization = aggregate_normalization\n\n    if aggregate_normalization:\n      # using aggregate_normalization only makes sense with segment_sum.\n      assert aggregate_edges_for_nodes_fn == \"segment_sum\"\n\n  def __call__(self,\n               input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n    self._networks_builder(input_graph)\n\n    # Embed input features (if applicable).\n    latent_graph_0 = self._embed(input_graph)\n\n    # Do `m` message passing steps in the latent graphs.\n    latent_graph_m = self._process(latent_graph_0)\n\n    # Compute outputs from the last latent graph (if applicable).\n    return self._output(latent_graph_m)\n\n  def _networks_builder(self, graph_template):\n    if self._initialized:\n      return\n    self._initialized = True\n\n    def build_mlp(name, output_size):\n      mlp = hk.nets.MLP(\n          output_sizes=[self._mlp_hidden_size] * self._mlp_num_hidden_layers + [\n              output_size], name=name + \"_mlp\", activation=self._activation)\n      return jraph.concatenated_args(mlp)\n\n    def build_mlp_with_maybe_layer_norm(name, output_size):\n      network = build_mlp(name, output_size)\n      if self._use_layer_norm:\n        layer_norm = hk.LayerNorm(\n            axis=-1, create_scale=True, create_offset=True,\n            name=name + \"_layer_norm\")\n        network = hk.Sequential([network, layer_norm])\n      return jraph.concatenated_args(network)\n\n    # The embedder graph network independently embeds edge and node features.\n    if self._embed_edges:\n      embed_edge_fn = _build_update_fns_for_edge_types(\n          build_mlp_with_maybe_layer_norm,\n          graph_template,\n          \"encoder_edges_\",\n          output_sizes=self._edge_latent_size)\n    else:\n      embed_edge_fn = None\n    if self._embed_nodes:\n      embed_node_fn = _build_update_fns_for_node_types(\n          build_mlp_with_maybe_layer_norm,\n          graph_template,\n          \"encoder_nodes_\",\n          output_sizes=self._node_latent_size)\n    else:\n      embed_node_fn = None\n    embedder_kwargs = dict(\n        embed_edge_fn=embed_edge_fn,\n        embed_node_fn=embed_node_fn,\n    )\n    self._embedder_network = typed_graph_net.GraphMapFeatures(\n        **embedder_kwargs)\n\n    if self._f32_aggregation:\n      def aggregate_fn(data, *args, **kwargs):\n        dtype = data.dtype\n        data = data.astype(jnp.float32)\n        output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n        if self._aggregate_normalization:\n          output = output / self._aggregate_normalization\n        output = output.astype(dtype)\n        return output\n\n    else:\n      def aggregate_fn(data, *args, **kwargs):\n        output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n        if self._aggregate_normalization:\n          output = output / self._aggregate_normalization\n        return output\n\n    # Create `num_message_passing_steps` graph networks with unshared parameters\n    # that update the node and edge latent features.\n    # Note that we can use `modules.InteractionNetwork` because\n    # it also outputs the messages as updated edge latent features.\n    self._processor_networks = []\n    for step_i in range(self._num_message_passing_steps):\n      self._processor_networks.append(\n          typed_graph_net.InteractionNetwork(\n              update_edge_fn=_build_update_fns_for_edge_types(\n                  build_mlp_with_maybe_layer_norm,\n                  graph_template,\n                  f\"processor_edges_{step_i}_\",\n                  output_sizes=self._edge_latent_size),\n              update_node_fn=_build_update_fns_for_node_types(\n                  build_mlp_with_maybe_layer_norm,\n                  graph_template,\n                  f\"processor_nodes_{step_i}_\",\n                  output_sizes=self._node_latent_size),\n              aggregate_edges_for_nodes_fn=aggregate_fn,\n              include_sent_messages_in_node_update=(\n                  self._include_sent_messages_in_node_update),\n              ))\n\n    # The output MLPs converts edge/node latent features into the output sizes.\n    output_kwargs = dict(\n        embed_edge_fn=_build_update_fns_for_edge_types(\n            build_mlp, graph_template, \"decoder_edges_\", self._edge_output_size)\n        if self._edge_output_size else None,\n        embed_node_fn=_build_update_fns_for_node_types(\n            build_mlp, graph_template, \"decoder_nodes_\", self._node_output_size)\n        if self._node_output_size else None,)\n    self._output_network = typed_graph_net.GraphMapFeatures(\n        **output_kwargs)\n\n  def _embed(\n      self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Embeds the input graph features into a latent graph.\"\"\"\n\n    # Copy the context to all of the node types, if applicable.\n    context_features = input_graph.context.features\n    if jax.tree_util.tree_leaves(context_features):\n      # This code assumes a single input feature array for the context and for\n      # each node type.\n      assert len(jax.tree_util.tree_leaves(context_features)) == 1\n      new_nodes = {}\n      for node_set_name, node_set in input_graph.nodes.items():\n        node_features = node_set.features\n        broadcasted_context = jnp.repeat(\n            context_features, node_set.n_node, axis=0,\n            total_repeat_length=node_features.shape[0])\n        new_nodes[node_set_name] = node_set._replace(\n            features=jnp.concatenate(\n                [node_features, broadcasted_context], axis=-1))\n      input_graph = input_graph._replace(\n          nodes=new_nodes,\n          context=input_graph.context._replace(features=()))\n\n    # Embeds the node and edge features.\n    latent_graph_0 = self._embedder_network(input_graph)\n    return latent_graph_0\n\n  def _process(\n      self, latent_graph_0: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n\n    # Do `num_message_passing_steps` with each of the `self._processor_networks`\n    # with unshared weights, and repeat that `self._num_processor_repetitions`\n    # times.\n    latent_graph = latent_graph_0\n    for unused_repetition_i in range(self._num_processor_repetitions):\n      for processor_network in self._processor_networks:\n        latent_graph = self._process_step(processor_network, latent_graph)\n\n    return latent_graph\n\n  def _process_step(\n      self, processor_network_k,\n      latent_graph_prev_k: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n\n    # One step of message passing.\n    latent_graph_k = processor_network_k(latent_graph_prev_k)\n\n    # Add residuals.\n    nodes_with_residuals = {}\n    for k, prev_set in latent_graph_prev_k.nodes.items():\n      nodes_with_residuals[k] = prev_set._replace(\n          features=prev_set.features + latent_graph_k.nodes[k].features)\n\n    edges_with_residuals = {}\n    for k, prev_set in latent_graph_prev_k.edges.items():\n      edges_with_residuals[k] = prev_set._replace(\n          features=prev_set.features + latent_graph_k.edges[k].features)\n\n    latent_graph_k = latent_graph_k._replace(\n        nodes=nodes_with_residuals, edges=edges_with_residuals)\n    return latent_graph_k\n\n  def _output(self,\n              latent_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Produces the output from the latent graph.\"\"\"\n    return self._output_network(latent_graph)\n\n\ndef _build_update_fns_for_node_types(\n    builder_fn, graph_template, prefix, output_sizes=None):\n  \"\"\"Builds an update function for all node types or a subset of them.\"\"\"\n\n  output_fns = {}\n  for node_set_name in graph_template.nodes.keys():\n    if output_sizes is None:\n      # Use the default output size for all types.\n      output_size = None\n    else:\n      # Otherwise, ignore any type that does not have an explicit output size.\n      if node_set_name in output_sizes:\n        output_size = output_sizes[node_set_name]\n      else:\n        continue\n    output_fns[node_set_name] = builder_fn(\n        f\"{prefix}{node_set_name}\", output_size)\n  return output_fns\n\n\ndef _build_update_fns_for_edge_types(\n    builder_fn, graph_template, prefix, output_sizes=None):\n  \"\"\"Builds an edge function for all node types or a subset of them.\"\"\"\n  output_fns = {}\n  for edge_set_key in graph_template.edges.keys():\n    edge_set_name = edge_set_key.name\n    if output_sizes is None:\n      # Use the default output size for all types.\n      output_size = None\n    else:\n      # Otherwise, ignore any type that does not have an explicit output size.\n      if edge_set_name in output_sizes:\n        output_size = output_sizes[edge_set_name]\n      else:\n        continue\n    output_fns[edge_set_name] = builder_fn(\n        f\"{prefix}{edge_set_name}\", output_size)\n  return output_fns\n\n\ndef _get_activation_fn(name):\n  \"\"\"Return activation function corresponding to function_name.\"\"\"\n  if name == \"identity\":\n    return lambda x: x\n  if hasattr(jax.nn, name):\n    return getattr(jax.nn, name)\n  if hasattr(jnp, name):\n    return getattr(jnp, name)\n  raise ValueError(f\"Unknown activation function {name} specified.\")\n\n\ndef _get_aggregate_edges_for_nodes_fn(name):\n  \"\"\"Return aggregate_edges_for_nodes_fn corresponding to function_name.\"\"\"\n  if hasattr(jraph, name):\n    return getattr(jraph, name)\n  raise ValueError(\n      f\"Unknown aggregate_edges_for_nodes_fn function {name} specified.\")\n",
    "8": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"A predictor that runs multiple graph neural networks on mesh data.\n\nIt learns to interpolate between the grid and the mesh nodes, with the loss\nand the rollouts ultimately computed at the grid level.\n\nIt uses ideas similar to those in Keisler (2022):\n\nReference:\n  https://arxiv.org/pdf/2202.07575.pdf\n\nIt assumes data across time and level is stacked, and operates only operates in\na 2D mesh over latitudes and longitudes.\n\"\"\"\n\nfrom typing import Any, Callable, Mapping, Optional\n\nimport chex\nfrom graphcast import deep_typed_graph_net\nfrom graphcast import grid_mesh_connectivity\nfrom graphcast import icosahedral_mesh\nfrom graphcast import losses\nfrom graphcast import model_utils\nfrom graphcast import predictor_base\nfrom graphcast import typed_graph\nfrom graphcast import xarray_jax\nimport jax.numpy as jnp\nimport jraph\nimport numpy as np\nimport xarray\n\nKwargs = Mapping[str, Any]\n\nGNN = Callable[[jraph.GraphsTuple], jraph.GraphsTuple]\n\n\n# https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5\nPRESSURE_LEVELS_ERA5_37 = (\n    1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100, 125, 150, 175, 200, 225, 250, 300,\n    350, 400, 450, 500, 550, 600, 650, 700, 750, 775, 800, 825, 850, 875, 900,\n    925, 950, 975, 1000)\n\n# https://www.ecmwf.int/en/forecasts/datasets/set-i\nPRESSURE_LEVELS_HRES_25 = (\n    1, 2, 3, 5, 7, 10, 20, 30, 50, 70, 100, 150, 200, 250, 300, 400, 500, 600,\n    700, 800, 850, 900, 925, 950, 1000)\n\n# https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2020MS002203\nPRESSURE_LEVELS_WEATHERBENCH_13 = (\n    50, 100, 150, 200, 250, 300, 400, 500, 600, 700, 850, 925, 1000)\n\nPRESSURE_LEVELS = {\n    13: PRESSURE_LEVELS_WEATHERBENCH_13,\n    25: PRESSURE_LEVELS_HRES_25,\n    37: PRESSURE_LEVELS_ERA5_37,\n}\n\n# The list of all possible atmospheric variables. Taken from:\n# https://confluence.ecmwf.int/display/CKB/ERA5%3A+data+documentation#ERA5:datadocumentation-Table9\nALL_ATMOSPHERIC_VARS = (\n    \"potential_vorticity\",\n    \"specific_rain_water_content\",\n    \"specific_snow_water_content\",\n    \"geopotential\",\n    \"temperature\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n    \"specific_humidity\",\n    \"vertical_velocity\",\n    \"vorticity\",\n    \"divergence\",\n    \"relative_humidity\",\n    \"ozone_mass_mixing_ratio\",\n    \"specific_cloud_liquid_water_content\",\n    \"specific_cloud_ice_water_content\",\n    \"fraction_of_cloud_cover\",\n)\n\nTARGET_SURFACE_VARS = (\n    \"2m_temperature\",\n    \"mean_sea_level_pressure\",\n    \"10m_v_component_of_wind\",\n    \"10m_u_component_of_wind\",\n    \"total_precipitation_6hr\",\n)\nTARGET_SURFACE_NO_PRECIP_VARS = (\n    \"2m_temperature\",\n    \"mean_sea_level_pressure\",\n    \"10m_v_component_of_wind\",\n    \"10m_u_component_of_wind\",\n)\nTARGET_ATMOSPHERIC_VARS = (\n    \"temperature\",\n    \"geopotential\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n    \"vertical_velocity\",\n    \"specific_humidity\",\n)\nTARGET_ATMOSPHERIC_NO_W_VARS = (\n    \"temperature\",\n    \"geopotential\",\n    \"u_component_of_wind\",\n    \"v_component_of_wind\",\n    \"specific_humidity\",\n)\nEXTERNAL_FORCING_VARS = (\n    \"toa_incident_solar_radiation\",\n)\nGENERATED_FORCING_VARS = (\n    \"year_progress_sin\",\n    \"year_progress_cos\",\n    \"day_progress_sin\",\n    \"day_progress_cos\",\n)\nFORCING_VARS = EXTERNAL_FORCING_VARS + GENERATED_FORCING_VARS\nSTATIC_VARS = (\n    \"geopotential_at_surface\",\n    \"land_sea_mask\",\n)\n\n\n@chex.dataclass(frozen=True, eq=True)\nclass TaskConfig:\n  \"\"\"Defines inputs and targets on which a model is trained and/or evaluated.\"\"\"\n  input_variables: tuple[str, ...]\n  # Target variables which the model is expected to predict.\n  target_variables: tuple[str, ...]\n  forcing_variables: tuple[str, ...]\n  pressure_levels: tuple[int, ...]\n  input_duration: str\n\nTASK = TaskConfig(\n    input_variables=(\n        TARGET_SURFACE_VARS + TARGET_ATMOSPHERIC_VARS + FORCING_VARS +\n        STATIC_VARS),\n    target_variables=TARGET_SURFACE_VARS + TARGET_ATMOSPHERIC_VARS,\n    forcing_variables=FORCING_VARS,\n    pressure_levels=PRESSURE_LEVELS_ERA5_37,\n    input_duration=\"12h\",\n)\nTASK_13 = TaskConfig(\n    input_variables=(\n        TARGET_SURFACE_VARS + TARGET_ATMOSPHERIC_VARS + FORCING_VARS +\n        STATIC_VARS),\n    target_variables=TARGET_SURFACE_VARS + TARGET_ATMOSPHERIC_VARS,\n    forcing_variables=FORCING_VARS,\n    pressure_levels=PRESSURE_LEVELS_WEATHERBENCH_13,\n    input_duration=\"12h\",\n)\nTASK_13_PRECIP_OUT = TaskConfig(\n    input_variables=(\n        TARGET_SURFACE_NO_PRECIP_VARS + TARGET_ATMOSPHERIC_VARS + FORCING_VARS +\n        STATIC_VARS),\n    target_variables=TARGET_SURFACE_VARS + TARGET_ATMOSPHERIC_VARS,\n    forcing_variables=FORCING_VARS,\n    pressure_levels=PRESSURE_LEVELS_WEATHERBENCH_13,\n    input_duration=\"12h\",\n)\n\n\n@chex.dataclass(frozen=True, eq=True)\nclass ModelConfig:\n  \"\"\"Defines the architecture of the GraphCast neural network architecture.\n\n  Properties:\n    resolution: The resolution of the data, in degrees (e.g. 0.25 or 1.0).\n    mesh_size: How many refinements to do on the multi-mesh.\n    gnn_msg_steps: How many Graph Network message passing steps to do.\n    latent_size: How many latent features to include in the various MLPs.\n    hidden_layers: How many hidden layers for each MLP.\n    radius_query_fraction_edge_length: Scalar that will be multiplied by the\n        length of the longest edge of the finest mesh to define the radius of\n        connectivity to use in the Grid2Mesh graph. Reasonable values are\n        between 0.6 and 1. 0.6 reduces the number of grid points feeding into\n        multiple mesh nodes and therefore reduces edge count and memory use, but\n        1 gives better predictions.\n    mesh2grid_edge_normalization_factor: Allows explicitly controlling edge\n        normalization for mesh2grid edges. If None, defaults to max edge length.\n        This supports using pre-trained model weights with a different graph\n        structure to what it was trained on.\n  \"\"\"\n  resolution: float\n  mesh_size: int\n  latent_size: int\n  gnn_msg_steps: int\n  hidden_layers: int\n  radius_query_fraction_edge_length: float\n  mesh2grid_edge_normalization_factor: Optional[float] = None\n\n\n@chex.dataclass(frozen=True, eq=True)\nclass CheckPoint:\n  params: dict[str, Any]\n  model_config: ModelConfig\n  task_config: TaskConfig\n  description: str\n  license: str\n\n\nclass GraphCast(predictor_base.Predictor):\n  \"\"\"GraphCast Predictor.\n\n  The model works on graphs that take into account:\n  * Mesh nodes: nodes for the vertices of the mesh.\n  * Grid nodes: nodes for the points of the grid.\n  * Nodes: When referring to just \"nodes\", this means the joint set of\n    both mesh nodes, concatenated with grid nodes.\n\n  The model works with 3 graphs:\n  * Grid2Mesh graph: Graph that contains all nodes. This graph is strictly\n    bipartite with edges going from grid nodes to mesh nodes using a\n    fixed radius query. The grid2mesh_gnn will operate in this graph. The output\n    of this stage will be a latent representation for the mesh nodes, and a\n    latent representation for the grid nodes.\n  * Mesh graph: Graph that contains mesh nodes only. The mesh_gnn will\n    operate in this graph. It will update the latent state of the mesh nodes\n    only.\n  * Mesh2Grid graph: Graph that contains all nodes. This graph is strictly\n    bipartite with edges going from mesh nodes to grid nodes such that each grid\n    nodes is connected to 3 nodes of the mesh triangular face that contains\n    the grid points. The mesh2grid_gnn will operate in this graph. It will\n    process the updated latent state of the mesh nodes, and the latent state\n    of the grid nodes, to produce the final output for the grid nodes.\n\n  The model is built on top of `TypedGraph`s so the different types of nodes and\n  edges can be stored and treated separately.\n\n  \"\"\"\n\n  def __init__(self, model_config: ModelConfig, task_config: TaskConfig):\n    \"\"\"Initializes the predictor.\"\"\"\n    self._spatial_features_kwargs = dict(\n        add_node_positions=False,\n        add_node_latitude=True,\n        add_node_longitude=True,\n        add_relative_positions=True,\n        relative_longitude_local_coordinates=True,\n        relative_latitude_local_coordinates=True,\n    )\n\n    # Specification of the multimesh.\n    self._meshes = (\n        icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(\n            splits=model_config.mesh_size))\n\n    # Encoder, which moves data from the grid to the mesh with a single message\n    # passing step.\n    self._grid2mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(\n        embed_nodes=True,  # Embed raw features of the grid and mesh nodes.\n        embed_edges=True,  # Embed raw features of the grid2mesh edges.\n        edge_latent_size=dict(grid2mesh=model_config.latent_size),\n        node_latent_size=dict(\n            mesh_nodes=model_config.latent_size,\n            grid_nodes=model_config.latent_size),\n        mlp_hidden_size=model_config.latent_size,\n        mlp_num_hidden_layers=model_config.hidden_layers,\n        num_message_passing_steps=1,\n        use_layer_norm=True,\n        include_sent_messages_in_node_update=False,\n        activation=\"swish\",\n        f32_aggregation=True,\n        aggregate_normalization=None,\n        name=\"grid2mesh_gnn\",\n    )\n\n    # Processor, which performs message passing on the multi-mesh.\n    self._mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(\n        embed_nodes=False,  # Node features already embdded by previous layers.\n        embed_edges=True,  # Embed raw features of the multi-mesh edges.\n        node_latent_size=dict(mesh_nodes=model_config.latent_size),\n        edge_latent_size=dict(mesh=model_config.latent_size),\n        mlp_hidden_size=model_config.latent_size,\n        mlp_num_hidden_layers=model_config.hidden_layers,\n        num_message_passing_steps=model_config.gnn_msg_steps,\n        use_layer_norm=True,\n        include_sent_messages_in_node_update=False,\n        activation=\"swish\",\n        f32_aggregation=False,\n        name=\"mesh_gnn\",\n    )\n\n    num_surface_vars = len(\n        set(task_config.target_variables) - set(ALL_ATMOSPHERIC_VARS))\n    num_atmospheric_vars = len(\n        set(task_config.target_variables) & set(ALL_ATMOSPHERIC_VARS))\n    num_outputs = (num_surface_vars +\n                   len(task_config.pressure_levels) * num_atmospheric_vars)\n\n    # Decoder, which moves data from the mesh back into the grid with a single\n    # message passing step.\n    self._mesh2grid_gnn = deep_typed_graph_net.DeepTypedGraphNet(\n        # Require a specific node dimensionaly for the grid node outputs.\n        node_output_size=dict(grid_nodes=num_outputs),\n        embed_nodes=False,  # Node features already embdded by previous layers.\n        embed_edges=True,  # Embed raw features of the mesh2grid edges.\n        edge_latent_size=dict(mesh2grid=model_config.latent_size),\n        node_latent_size=dict(\n            mesh_nodes=model_config.latent_size,\n            grid_nodes=model_config.latent_size),\n        mlp_hidden_size=model_config.latent_size,\n        mlp_num_hidden_layers=model_config.hidden_layers,\n        num_message_passing_steps=1,\n        use_layer_norm=True,\n        include_sent_messages_in_node_update=False,\n        activation=\"swish\",\n        f32_aggregation=False,\n        name=\"mesh2grid_gnn\",\n    )\n\n    # Obtain the query radius in absolute units for the unit-sphere for the\n    # grid2mesh model, by rescaling the `radius_query_fraction_edge_length`.\n    self._query_radius = (_get_max_edge_distance(self._finest_mesh)\n                          * model_config.radius_query_fraction_edge_length)\n    self._mesh2grid_edge_normalization_factor = (\n        model_config.mesh2grid_edge_normalization_factor\n    )\n\n    # Other initialization is delayed until the first call (`_maybe_init`)\n    # when we get some sample data so we know the lat/lon values.\n    self._initialized = False\n\n    # A \"_init_mesh_properties\":\n    # This one could be initialized at init but we delay it for consistency too.\n    self._num_mesh_nodes = None  # num_mesh_nodes\n    self._mesh_nodes_lat = None  # [num_mesh_nodes]\n    self._mesh_nodes_lon = None  # [num_mesh_nodes]\n\n    # A \"_init_grid_properties\":\n    self._grid_lat = None  # [num_lat_points]\n    self._grid_lon = None  # [num_lon_points]\n    self._num_grid_nodes = None  # num_lat_points * num_lon_points\n    self._grid_nodes_lat = None  # [num_grid_nodes]\n    self._grid_nodes_lon = None  # [num_grid_nodes]\n\n    # A \"_init_{grid2mesh,processor,mesh2grid}_graph\"\n    self._grid2mesh_graph_structure = None\n    self._mesh_graph_structure = None\n    self._mesh2grid_graph_structure = None\n\n  @property\n  def _finest_mesh(self):\n    return self._meshes[-1]\n\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               is_training: bool = False,\n               ) -> xarray.Dataset:\n    self._maybe_init(inputs)\n\n    # Convert all input data into flat vectors for each of the grid nodes.\n    # xarray (batch, time, lat, lon, level, multiple vars, forcings)\n    # -> [num_grid_nodes, batch, num_channels]\n    grid_node_features = self._inputs_to_grid_node_features(inputs, forcings)\n\n    # Transfer data for the grid to the mesh,\n    # [num_mesh_nodes, batch, latent_size], [num_grid_nodes, batch, latent_size]\n    (latent_mesh_nodes, latent_grid_nodes\n     ) = self._run_grid2mesh_gnn(grid_node_features)\n\n    # Run message passing in the multimesh.\n    # [num_mesh_nodes, batch, latent_size]\n    updated_latent_mesh_nodes = self._run_mesh_gnn(latent_mesh_nodes)\n\n    # Transfer data frome the mesh to the grid.\n    # [num_grid_nodes, batch, output_size]\n    output_grid_nodes = self._run_mesh2grid_gnn(\n        updated_latent_mesh_nodes, latent_grid_nodes)\n\n    # Conver output flat vectors for the grid nodes to the format of the output.\n    # [num_grid_nodes, batch, output_size] ->\n    # xarray (batch, one time step, lat, lon, level, multiple vars)\n    return self._grid_node_outputs_to_prediction(\n        output_grid_nodes, targets_template)\n\n  def loss_and_predictions(  # pytype: disable=signature-mismatch  # jax-ndarray\n      self,\n      inputs: xarray.Dataset,\n      targets: xarray.Dataset,\n      forcings: xarray.Dataset,\n      ) -> tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    # Forward pass.\n    predictions = self(\n        inputs, targets_template=targets, forcings=forcings, is_training=True)\n    # Compute loss.\n    loss = losses.weighted_mse_per_level(\n        predictions, targets,\n        per_variable_weights={\n            # Any variables not specified here are weighted as 1.0.\n            # A single-level variable, but an important headline variable\n            # and also one which we have struggled to get good performance\n            # on at short lead times, so leaving it weighted at 1.0, equal\n            # to the multi-level variables:\n            \"2m_temperature\": 1.0,\n            # New single-level variables, which we don't weight too highly\n            # to avoid hurting performance on other variables.\n            \"10m_u_component_of_wind\": 0.1,\n            \"10m_v_component_of_wind\": 0.1,\n            \"mean_sea_level_pressure\": 0.1,\n            \"total_precipitation_6hr\": 0.1,\n        })\n    return loss, predictions  # pytype: disable=bad-return-type  # jax-ndarray\n\n  def loss(  # pytype: disable=signature-mismatch  # jax-ndarray\n      self,\n      inputs: xarray.Dataset,\n      targets: xarray.Dataset,\n      forcings: xarray.Dataset,\n      ) -> predictor_base.LossAndDiagnostics:\n    loss, _ = self.loss_and_predictions(inputs, targets, forcings)\n    return loss  # pytype: disable=bad-return-type  # jax-ndarray\n\n  def _maybe_init(self, sample_inputs: xarray.Dataset):\n    \"\"\"Inits everything that has a dependency on the input coordinates.\"\"\"\n    if not self._initialized:\n      self._init_mesh_properties()\n      self._init_grid_properties(\n          grid_lat=sample_inputs.lat, grid_lon=sample_inputs.lon)\n      self._grid2mesh_graph_structure = self._init_grid2mesh_graph()\n      self._mesh_graph_structure = self._init_mesh_graph()\n      self._mesh2grid_graph_structure = self._init_mesh2grid_graph()\n\n      self._initialized = True\n\n  def _init_mesh_properties(self):\n    \"\"\"Inits static properties that have to do with mesh nodes.\"\"\"\n    self._num_mesh_nodes = self._finest_mesh.vertices.shape[0]\n    mesh_phi, mesh_theta = model_utils.cartesian_to_spherical(\n        self._finest_mesh.vertices[:, 0],\n        self._finest_mesh.vertices[:, 1],\n        self._finest_mesh.vertices[:, 2])\n    (\n        mesh_nodes_lat,\n        mesh_nodes_lon,\n    ) = model_utils.spherical_to_lat_lon(\n        phi=mesh_phi, theta=mesh_theta)\n    # Convert to f32 to ensure the lat/lon features aren't in f64.\n    self._mesh_nodes_lat = mesh_nodes_lat.astype(np.float32)\n    self._mesh_nodes_lon = mesh_nodes_lon.astype(np.float32)\n\n  def _init_grid_properties(self, grid_lat: np.ndarray, grid_lon: np.ndarray):\n    \"\"\"Inits static properties that have to do with grid nodes.\"\"\"\n    self._grid_lat = grid_lat.astype(np.float32)\n    self._grid_lon = grid_lon.astype(np.float32)\n    # Initialized the counters.\n    self._num_grid_nodes = grid_lat.shape[0] * grid_lon.shape[0]\n\n    # Initialize lat and lon for the grid.\n    grid_nodes_lon, grid_nodes_lat = np.meshgrid(grid_lon, grid_lat)\n    self._grid_nodes_lon = grid_nodes_lon.reshape([-1]).astype(np.float32)\n    self._grid_nodes_lat = grid_nodes_lat.reshape([-1]).astype(np.float32)\n\n  def _init_grid2mesh_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Grid2Mesh graph.\"\"\"\n\n    # Create some edges according to distance between mesh and grid nodes.\n    assert self._grid_lat is not None and self._grid_lon is not None\n    (grid_indices, mesh_indices) = grid_mesh_connectivity.radius_query_indices(\n        grid_latitude=self._grid_lat,\n        grid_longitude=self._grid_lon,\n        mesh=self._finest_mesh,\n        radius=self._query_radius)\n\n    # Edges sending info from grid to mesh.\n    senders = grid_indices\n    receivers = mesh_indices\n\n    # Precompute structural node and edge features according to config options.\n    # Structural features are those that depend on the fixed values of the\n    # latitude and longitudes of the nodes.\n    (senders_node_features, receivers_node_features,\n     edge_features) = model_utils.get_bipartite_graph_spatial_features(\n         senders_node_lat=self._grid_nodes_lat,\n         senders_node_lon=self._grid_nodes_lon,\n         receivers_node_lat=self._mesh_nodes_lat,\n         receivers_node_lon=self._mesh_nodes_lon,\n         senders=senders,\n         receivers=receivers,\n         edge_normalization_factor=None,\n         **self._spatial_features_kwargs,\n     )\n\n    n_grid_node = np.array([self._num_grid_nodes])\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([mesh_indices.shape[0]])\n    grid_node_set = typed_graph.NodeSet(\n        n_node=n_grid_node, features=senders_node_features)\n    mesh_node_set = typed_graph.NodeSet(\n        n_node=n_mesh_node, features=receivers_node_features)\n    edge_set = typed_graph.EdgeSet(\n        n_edge=n_edge,\n        indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers),\n        features=edge_features)\n    nodes = {\"grid_nodes\": grid_node_set, \"mesh_nodes\": mesh_node_set}\n    edges = {\n        typed_graph.EdgeSetKey(\"grid2mesh\", (\"grid_nodes\", \"mesh_nodes\")):\n            edge_set\n    }\n    grid2mesh_graph = typed_graph.TypedGraph(\n        context=typed_graph.Context(n_graph=np.array([1]), features=()),\n        nodes=nodes,\n        edges=edges)\n    return grid2mesh_graph\n\n  def _init_mesh_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Mesh graph.\"\"\"\n    merged_mesh = icosahedral_mesh.merge_meshes(self._meshes)\n\n    # Work simply on the mesh edges.\n    senders, receivers = icosahedral_mesh.faces_to_edges(merged_mesh.faces)\n\n    # Precompute structural node and edge features according to config options.\n    # Structural features are those that depend on the fixed values of the\n    # latitude and longitudes of the nodes.\n    assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n    node_features, edge_features = model_utils.get_graph_spatial_features(\n        node_lat=self._mesh_nodes_lat,\n        node_lon=self._mesh_nodes_lon,\n        senders=senders,\n        receivers=receivers,\n        **self._spatial_features_kwargs,\n    )\n\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([senders.shape[0]])\n    assert n_mesh_node == len(node_features)\n    mesh_node_set = typed_graph.NodeSet(\n        n_node=n_mesh_node, features=node_features)\n    edge_set = typed_graph.EdgeSet(\n        n_edge=n_edge,\n        indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers),\n        features=edge_features)\n    nodes = {\"mesh_nodes\": mesh_node_set}\n    edges = {\n        typed_graph.EdgeSetKey(\"mesh\", (\"mesh_nodes\", \"mesh_nodes\")): edge_set\n    }\n    mesh_graph = typed_graph.TypedGraph(\n        context=typed_graph.Context(n_graph=np.array([1]), features=()),\n        nodes=nodes,\n        edges=edges)\n\n    return mesh_graph\n\n  def _init_mesh2grid_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Mesh2Grid graph.\"\"\"\n\n    # Create some edges according to how the grid nodes are contained by\n    # mesh triangles.\n    (grid_indices,\n     mesh_indices) = grid_mesh_connectivity.in_mesh_triangle_indices(\n         grid_latitude=self._grid_lat,\n         grid_longitude=self._grid_lon,\n         mesh=self._finest_mesh)\n\n    # Edges sending info from mesh to grid.\n    senders = mesh_indices\n    receivers = grid_indices\n\n    # Precompute structural node and edge features according to config options.\n    assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n    (senders_node_features, receivers_node_features,\n     edge_features) = model_utils.get_bipartite_graph_spatial_features(\n         senders_node_lat=self._mesh_nodes_lat,\n         senders_node_lon=self._mesh_nodes_lon,\n         receivers_node_lat=self._grid_nodes_lat,\n         receivers_node_lon=self._grid_nodes_lon,\n         senders=senders,\n         receivers=receivers,\n         edge_normalization_factor=self._mesh2grid_edge_normalization_factor,\n         **self._spatial_features_kwargs,\n     )\n\n    n_grid_node = np.array([self._num_grid_nodes])\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([senders.shape[0]])\n    grid_node_set = typed_graph.NodeSet(\n        n_node=n_grid_node, features=receivers_node_features)\n    mesh_node_set = typed_graph.NodeSet(\n        n_node=n_mesh_node, features=senders_node_features)\n    edge_set = typed_graph.EdgeSet(\n        n_edge=n_edge,\n        indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers),\n        features=edge_features)\n    nodes = {\"grid_nodes\": grid_node_set, \"mesh_nodes\": mesh_node_set}\n    edges = {\n        typed_graph.EdgeSetKey(\"mesh2grid\", (\"mesh_nodes\", \"grid_nodes\")):\n            edge_set\n    }\n    mesh2grid_graph = typed_graph.TypedGraph(\n        context=typed_graph.Context(n_graph=np.array([1]), features=()),\n        nodes=nodes,\n        edges=edges)\n    return mesh2grid_graph\n\n  def _run_grid2mesh_gnn(self, grid_node_features: chex.Array,\n                         ) -> tuple[chex.Array, chex.Array]:\n    \"\"\"Runs the grid2mesh_gnn, extracting latent mesh and grid nodes.\"\"\"\n\n    # Concatenate node structural features with input features.\n    batch_size = grid_node_features.shape[1]\n\n    grid2mesh_graph = self._grid2mesh_graph_structure\n    assert grid2mesh_graph is not None\n    grid_nodes = grid2mesh_graph.nodes[\"grid_nodes\"]\n    mesh_nodes = grid2mesh_graph.nodes[\"mesh_nodes\"]\n    new_grid_nodes = grid_nodes._replace(\n        features=jnp.concatenate([\n            grid_node_features,\n            _add_batch_second_axis(\n                grid_nodes.features.astype(grid_node_features.dtype),\n                batch_size)\n        ],\n                                 axis=-1))\n\n    # To make sure capacity of the embedded is identical for the grid nodes and\n    # the mesh nodes, we also append some dummy zero input features for the\n    # mesh nodes.\n    dummy_mesh_node_features = jnp.zeros(\n        (self._num_mesh_nodes,) + grid_node_features.shape[1:],\n        dtype=grid_node_features.dtype)\n    new_mesh_nodes = mesh_nodes._replace(\n        features=jnp.concatenate([\n            dummy_mesh_node_features,\n            _add_batch_second_axis(\n                mesh_nodes.features.astype(dummy_mesh_node_features.dtype),\n                batch_size)\n        ],\n                                 axis=-1))\n\n    # Broadcast edge structural features to the required batch size.\n    grid2mesh_edges_key = grid2mesh_graph.edge_key_by_name(\"grid2mesh\")\n    edges = grid2mesh_graph.edges[grid2mesh_edges_key]\n\n    new_edges = edges._replace(\n        features=_add_batch_second_axis(\n            edges.features.astype(dummy_mesh_node_features.dtype), batch_size))\n\n    input_graph = self._grid2mesh_graph_structure._replace(\n        edges={grid2mesh_edges_key: new_edges},\n        nodes={\n            \"grid_nodes\": new_grid_nodes,\n            \"mesh_nodes\": new_mesh_nodes\n        })\n\n    # Run the GNN.\n    grid2mesh_out = self._grid2mesh_gnn(input_graph)\n    latent_mesh_nodes = grid2mesh_out.nodes[\"mesh_nodes\"].features\n    latent_grid_nodes = grid2mesh_out.nodes[\"grid_nodes\"].features\n    return latent_mesh_nodes, latent_grid_nodes\n\n  def _run_mesh_gnn(self, latent_mesh_nodes: chex.Array) -> chex.Array:\n    \"\"\"Runs the mesh_gnn, extracting updated latent mesh nodes.\"\"\"\n\n    # Add the structural edge features of this graph. Note we don't need\n    # to add the structural node features, because these are already part of\n    # the latent state, via the original Grid2Mesh gnn, however, we need\n    # the edge ones, because it is the first time we are seeing this particular\n    # set of edges.\n    batch_size = latent_mesh_nodes.shape[1]\n\n    mesh_graph = self._mesh_graph_structure\n    assert mesh_graph is not None\n    mesh_edges_key = mesh_graph.edge_key_by_name(\"mesh\")\n    edges = mesh_graph.edges[mesh_edges_key]\n\n    # We are assuming here that the mesh gnn uses a single set of edge keys\n    # named \"mesh\" for the edges and that it uses a single set of nodes named\n    # \"mesh_nodes\"\n    msg = (\"The setup currently requires to only have one kind of edge in the\"\n           \" mesh GNN.\")\n    assert len(mesh_graph.edges) == 1, msg\n\n    new_edges = edges._replace(\n        features=_add_batch_second_axis(\n            edges.features.astype(latent_mesh_nodes.dtype), batch_size))\n\n    nodes = mesh_graph.nodes[\"mesh_nodes\"]\n    nodes = nodes._replace(features=latent_mesh_nodes)\n\n    input_graph = mesh_graph._replace(\n        edges={mesh_edges_key: new_edges}, nodes={\"mesh_nodes\": nodes})\n\n    # Run the GNN.\n    return self._mesh_gnn(input_graph).nodes[\"mesh_nodes\"].features\n\n  def _run_mesh2grid_gnn(self,\n                         updated_latent_mesh_nodes: chex.Array,\n                         latent_grid_nodes: chex.Array,\n                         ) -> chex.Array:\n    \"\"\"Runs the mesh2grid_gnn, extracting the output grid nodes.\"\"\"\n\n    # Add the structural edge features of this graph. Note we don't need\n    # to add the structural node features, because these are already part of\n    # the latent state, via the original Grid2Mesh gnn, however, we need\n    # the edge ones, because it is the first time we are seeing this particular\n    # set of edges.\n    batch_size = updated_latent_mesh_nodes.shape[1]\n\n    mesh2grid_graph = self._mesh2grid_graph_structure\n    assert mesh2grid_graph is not None\n    mesh_nodes = mesh2grid_graph.nodes[\"mesh_nodes\"]\n    grid_nodes = mesh2grid_graph.nodes[\"grid_nodes\"]\n    new_mesh_nodes = mesh_nodes._replace(features=updated_latent_mesh_nodes)\n    new_grid_nodes = grid_nodes._replace(features=latent_grid_nodes)\n    mesh2grid_key = mesh2grid_graph.edge_key_by_name(\"mesh2grid\")\n    edges = mesh2grid_graph.edges[mesh2grid_key]\n\n    new_edges = edges._replace(\n        features=_add_batch_second_axis(\n            edges.features.astype(latent_grid_nodes.dtype), batch_size))\n\n    input_graph = mesh2grid_graph._replace(\n        edges={mesh2grid_key: new_edges},\n        nodes={\n            \"mesh_nodes\": new_mesh_nodes,\n            \"grid_nodes\": new_grid_nodes\n        })\n\n    # Run the GNN.\n    output_graph = self._mesh2grid_gnn(input_graph)\n    output_grid_nodes = output_graph.nodes[\"grid_nodes\"].features\n\n    return output_grid_nodes\n\n  def _inputs_to_grid_node_features(\n      self,\n      inputs: xarray.Dataset,\n      forcings: xarray.Dataset,\n      ) -> chex.Array:\n    \"\"\"xarrays -> [num_grid_nodes, batch, num_channels].\"\"\"\n\n    # xarray `Dataset` (batch, time, lat, lon, level, multiple vars)\n    # to xarray `DataArray` (batch, lat, lon, channels)\n    stacked_inputs = model_utils.dataset_to_stacked(inputs)\n    stacked_forcings = model_utils.dataset_to_stacked(forcings)\n    stacked_inputs = xarray.concat(\n        [stacked_inputs, stacked_forcings], dim=\"channels\")\n\n    # xarray `DataArray` (batch, lat, lon, channels)\n    # to single numpy array with shape [lat_lon_node, batch, channels]\n    grid_xarray_lat_lon_leading = model_utils.lat_lon_to_leading_axes(\n        stacked_inputs)\n    return xarray_jax.unwrap(grid_xarray_lat_lon_leading.data).reshape(\n        (-1,) + grid_xarray_lat_lon_leading.data.shape[2:])\n\n  def _grid_node_outputs_to_prediction(\n      self,\n      grid_node_outputs: chex.Array,\n      targets_template: xarray.Dataset,\n      ) -> xarray.Dataset:\n    \"\"\"[num_grid_nodes, batch, num_outputs] -> xarray.\"\"\"\n\n    # numpy array with shape [lat_lon_node, batch, channels]\n    # to xarray `DataArray` (batch, lat, lon, channels)\n    assert self._grid_lat is not None and self._grid_lon is not None\n    grid_shape = (self._grid_lat.shape[0], self._grid_lon.shape[0])\n    grid_outputs_lat_lon_leading = grid_node_outputs.reshape(\n        grid_shape + grid_node_outputs.shape[1:])\n    dims = (\"lat\", \"lon\", \"batch\", \"channels\")\n    grid_xarray_lat_lon_leading = xarray_jax.DataArray(\n        data=grid_outputs_lat_lon_leading,\n        dims=dims)\n    grid_xarray = model_utils.restore_leading_axes(grid_xarray_lat_lon_leading)\n\n    # xarray `DataArray` (batch, lat, lon, channels)\n    # to xarray `Dataset` (batch, one time step, lat, lon, level, multiple vars)\n    return model_utils.stacked_to_dataset(\n        grid_xarray.variable, targets_template)\n\n\ndef _add_batch_second_axis(data, batch_size):\n  # data [leading_dim, trailing_dim]\n  assert data.ndim == 2\n  ones = jnp.ones([batch_size, 1], dtype=data.dtype)\n  return data[:, None] * ones  # [leading_dim, batch, trailing_dim]\n\n\ndef _get_max_edge_distance(mesh):\n  senders, receivers = icosahedral_mesh.faces_to_edges(mesh.faces)\n  edge_distances = np.linalg.norm(\n      mesh.vertices[senders] - mesh.vertices[receivers], axis=-1)\n  return edge_distances.max()\n",
    "9": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tools for converting from regular grids on a sphere, to triangular meshes.\"\"\"\n\nfrom graphcast import icosahedral_mesh\nimport numpy as np\nimport scipy\nimport trimesh\n\n\ndef _grid_lat_lon_to_coordinates(\n    grid_latitude: np.ndarray, grid_longitude: np.ndarray) -> np.ndarray:\n  \"\"\"Lat [num_lat] lon [num_lon] to 3d coordinates [num_lat, num_lon, 3].\"\"\"\n  # Convert to spherical coordinates phi and theta defined in the grid.\n  # Each [num_latitude_points, num_longitude_points]\n  phi_grid, theta_grid = np.meshgrid(\n      np.deg2rad(grid_longitude),\n      np.deg2rad(90 - grid_latitude))\n\n  # [num_latitude_points, num_longitude_points, 3]\n  # Note this assumes unit radius, since for now we model the earth as a\n  # sphere of unit radius, and keep any vertical dimension as a regular grid.\n  return np.stack(\n      [np.cos(phi_grid)*np.sin(theta_grid),\n       np.sin(phi_grid)*np.sin(theta_grid),\n       np.cos(theta_grid)], axis=-1)\n\n\ndef radius_query_indices(\n    *,\n    grid_latitude: np.ndarray,\n    grid_longitude: np.ndarray,\n    mesh: icosahedral_mesh.TriangularMesh,\n    radius: float) -> tuple[np.ndarray, np.ndarray]:\n  \"\"\"Returns mesh-grid edge indices for radius query.\n\n  Args:\n    grid_latitude: Latitude values for the grid [num_lat_points]\n    grid_longitude: Longitude values for the grid [num_lon_points]\n    mesh: Mesh object.\n    radius: Radius of connectivity in R3. for a sphere of unit radius.\n\n  Returns:\n    tuple with `grid_indices` and `mesh_indices` indicating edges between the\n    grid and the mesh such that the distances in a straight line (not geodesic)\n    are smaller than or equal to `radius`.\n    * grid_indices: Indices of shape [num_edges], that index into a\n      [num_lat_points, num_lon_points] grid, after flattening the leading axes.\n    * mesh_indices: Indices of shape [num_edges], that index into mesh.vertices.\n  \"\"\"\n\n  # [num_grid_points=num_lat_points * num_lon_points, 3]\n  grid_positions = _grid_lat_lon_to_coordinates(\n      grid_latitude, grid_longitude).reshape([-1, 3])\n\n  # [num_mesh_points, 3]\n  mesh_positions = mesh.vertices\n  kd_tree = scipy.spatial.cKDTree(mesh_positions)\n\n  # [num_grid_points, num_mesh_points_per_grid_point]\n  # Note `num_mesh_points_per_grid_point` is not constant, so this is a list\n  # of arrays, rather than a 2d array.\n  query_indices = kd_tree.query_ball_point(x=grid_positions, r=radius)\n\n  grid_edge_indices = []\n  mesh_edge_indices = []\n  for grid_index, mesh_neighbors in enumerate(query_indices):\n    grid_edge_indices.append(np.repeat(grid_index, len(mesh_neighbors)))\n    mesh_edge_indices.append(mesh_neighbors)\n\n  # [num_edges]\n  grid_edge_indices = np.concatenate(grid_edge_indices, axis=0).astype(int)\n  mesh_edge_indices = np.concatenate(mesh_edge_indices, axis=0).astype(int)\n\n  return grid_edge_indices, mesh_edge_indices\n\n\ndef in_mesh_triangle_indices(\n    *,\n    grid_latitude: np.ndarray,\n    grid_longitude: np.ndarray,\n    mesh: icosahedral_mesh.TriangularMesh) -> tuple[np.ndarray, np.ndarray]:\n  \"\"\"Returns mesh-grid edge indices for grid points contained in mesh triangles.\n\n  Args:\n    grid_latitude: Latitude values for the grid [num_lat_points]\n    grid_longitude: Longitude values for the grid [num_lon_points]\n    mesh: Mesh object.\n\n  Returns:\n    tuple with `grid_indices` and `mesh_indices` indicating edges between the\n    grid and the mesh vertices of the triangle that contain each grid point.\n    The number of edges is always num_lat_points * num_lon_points * 3\n    * grid_indices: Indices of shape [num_edges], that index into a\n      [num_lat_points, num_lon_points] grid, after flattening the leading axes.\n    * mesh_indices: Indices of shape [num_edges], that index into mesh.vertices.\n  \"\"\"\n\n  # [num_grid_points=num_lat_points * num_lon_points, 3]\n  grid_positions = _grid_lat_lon_to_coordinates(\n      grid_latitude, grid_longitude).reshape([-1, 3])\n\n  mesh_trimesh = trimesh.Trimesh(vertices=mesh.vertices, faces=mesh.faces)\n\n  # [num_grid_points] with mesh face indices for each grid point.\n  _, _, query_face_indices = trimesh.proximity.closest_point(\n      mesh_trimesh, grid_positions)\n\n  # [num_grid_points, 3] with mesh node indices for each grid point.\n  mesh_edge_indices = mesh.faces[query_face_indices]\n\n  # [num_grid_points, 3] with grid node indices, where every row simply contains\n  # the row (grid_point) index.\n  grid_indices = np.arange(grid_positions.shape[0])\n  grid_edge_indices = np.tile(grid_indices.reshape([-1, 1]), [1, 3])\n\n  # Flatten to get a regular list.\n  # [num_edges=num_grid_points*3]\n  mesh_edge_indices = mesh_edge_indices.reshape([-1])\n  grid_edge_indices = grid_edge_indices.reshape([-1])\n\n  return grid_edge_indices, mesh_edge_indices\n",
    "10": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for graphcast.grid_mesh_connectivity.\"\"\"\n\nfrom absl.testing import absltest\nfrom graphcast import grid_mesh_connectivity\nfrom graphcast import icosahedral_mesh\nimport numpy as np\n\n\nclass GridMeshConnectivityTest(absltest.TestCase):\n\n  def test_grid_lat_lon_to_coordinates(self):\n\n    # Intervals of 30 degrees.\n    grid_latitude = np.array([-45., 0., 45])\n    grid_longitude = np.array([0., 90., 180., 270.])\n\n    inv_sqrt2 = 1 / np.sqrt(2)\n    expected_coordinates = np.array([\n        [[inv_sqrt2, 0., -inv_sqrt2],\n         [0., inv_sqrt2, -inv_sqrt2],\n         [-inv_sqrt2, 0., -inv_sqrt2],\n         [0., -inv_sqrt2, -inv_sqrt2]],\n        [[1., 0., 0.],\n         [0., 1., 0.],\n         [-1., 0., 0.],\n         [0., -1., 0.]],\n        [[inv_sqrt2, 0., inv_sqrt2],\n         [0., inv_sqrt2, inv_sqrt2],\n         [-inv_sqrt2, 0., inv_sqrt2],\n         [0., -inv_sqrt2, inv_sqrt2]],\n    ])\n\n    coordinates = grid_mesh_connectivity._grid_lat_lon_to_coordinates(\n        grid_latitude, grid_longitude)\n    np.testing.assert_allclose(expected_coordinates, coordinates, atol=1e-15)\n\n  def test_radius_query_indices_smoke(self):\n    # TODO(alvarosg): Add non-smoke test?\n    grid_latitude = np.linspace(-75, 75, 6)\n    grid_longitude = np.arange(12) * 30.\n    mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(\n        splits=3)[-1]\n    grid_mesh_connectivity.radius_query_indices(\n        grid_latitude=grid_latitude,\n        grid_longitude=grid_longitude,\n        mesh=mesh, radius=0.2)\n\n  def test_in_mesh_triangle_indices_smoke(self):\n    # TODO(alvarosg): Add non-smoke test?\n    grid_latitude = np.linspace(-75, 75, 6)\n    grid_longitude = np.arange(12) * 30.\n    mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(\n        splits=3)[-1]\n    grid_mesh_connectivity.in_mesh_triangle_indices(\n        grid_latitude=grid_latitude,\n        grid_longitude=grid_longitude,\n        mesh=mesh)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "11": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utils for creating icosahedral meshes.\"\"\"\n\nimport itertools\nfrom typing import List, NamedTuple, Sequence, Tuple\n\nimport numpy as np\nfrom scipy.spatial import transform\n\n\nclass TriangularMesh(NamedTuple):\n  \"\"\"Data structure for triangular meshes.\n\n  Attributes:\n    vertices: spatial positions of the vertices of the mesh of shape\n        [num_vertices, num_dims].\n    faces: triangular faces of the mesh of shape [num_faces, 3]. Contains\n        integer indices into `vertices`.\n\n  \"\"\"\n  vertices: np.ndarray\n  faces: np.ndarray\n\n\ndef merge_meshes(\n    mesh_list: Sequence[TriangularMesh]) -> TriangularMesh:\n  \"\"\"Merges all meshes into one. Assumes the last mesh is the finest.\n\n  Args:\n     mesh_list: Sequence of meshes, from coarse to fine refinement levels. The\n       vertices and faces may contain those from preceding, coarser levels.\n\n  Returns:\n     `TriangularMesh` for which the vertices correspond to the highest\n     resolution mesh in the hierarchy, and the faces are the join set of the\n     faces at all levels of the hierarchy.\n  \"\"\"\n  for mesh_i, mesh_ip1 in itertools.pairwise(mesh_list):\n    num_nodes_mesh_i = mesh_i.vertices.shape[0]\n    assert np.allclose(mesh_i.vertices, mesh_ip1.vertices[:num_nodes_mesh_i])\n\n  return TriangularMesh(\n      vertices=mesh_list[-1].vertices,\n      faces=np.concatenate([mesh.faces for mesh in mesh_list], axis=0))\n\n\ndef get_hierarchy_of_triangular_meshes_for_sphere(\n    splits: int) -> List[TriangularMesh]:\n  \"\"\"Returns a sequence of meshes, each with triangularization sphere.\n\n  Starting with a regular icosahedron (12 vertices, 20 faces, 30 edges) with\n  circumscribed unit sphere. Then, each triangular face is iteratively\n  subdivided into 4 triangular faces `splits` times. The new vertices are then\n  projected back onto the unit sphere. All resulting meshes are returned in a\n  list, from lowest to highest resolution.\n\n  The vertices in each face are specified in counter-clockwise order as\n  observed from the outside the icosahedron.\n\n  Args:\n     splits: How many times to split each triangle.\n  Returns:\n     Sequence of `TriangularMesh`s of length `splits + 1` each with:\n\n       vertices: [num_vertices, 3] vertex positions in 3D, all with unit norm.\n       faces: [num_faces, 3] with triangular faces joining sets of 3 vertices.\n           Each row contains three indices into the vertices array, indicating\n           the vertices adjacent to the face. Always with positive orientation\n           (counterclock-wise when looking from the outside).\n  \"\"\"\n  current_mesh = get_icosahedron()\n  output_meshes = [current_mesh]\n  for _ in range(splits):\n    current_mesh = _two_split_unit_sphere_triangle_faces(current_mesh)\n    output_meshes.append(current_mesh)\n  return output_meshes\n\n\ndef get_icosahedron() -> TriangularMesh:\n  \"\"\"Returns a regular icosahedral mesh with circumscribed unit sphere.\n\n  See https://en.wikipedia.org/wiki/Regular_icosahedron#Cartesian_coordinates\n  for details on the construction of the regular icosahedron.\n\n  The vertices in each face are specified in counter-clockwise order as observed\n  from the outside of the icosahedron.\n\n  Returns:\n     TriangularMesh with:\n\n     vertices: [num_vertices=12, 3] vertex positions in 3D, all with unit norm.\n     faces: [num_faces=20, 3] with triangular faces joining sets of 3 vertices.\n         Each row contains three indices into the vertices array, indicating\n         the vertices adjacent to the face. Always with positive orientation (\n         counterclock-wise when looking from the outside).\n\n  \"\"\"\n  phi = (1 + np.sqrt(5)) / 2\n  vertices = []\n  for c1 in [1., -1.]:\n    for c2 in [phi, -phi]:\n      vertices.append((c1, c2, 0.))\n      vertices.append((0., c1, c2))\n      vertices.append((c2, 0., c1))\n\n  vertices = np.array(vertices, dtype=np.float32)\n  vertices /= np.linalg.norm([1., phi])\n\n  # I did this manually, checking the orientation one by one.\n  faces = [(0, 1, 2),\n           (0, 6, 1),\n           (8, 0, 2),\n           (8, 4, 0),\n           (3, 8, 2),\n           (3, 2, 7),\n           (7, 2, 1),\n           (0, 4, 6),\n           (4, 11, 6),\n           (6, 11, 5),\n           (1, 5, 7),\n           (4, 10, 11),\n           (4, 8, 10),\n           (10, 8, 3),\n           (10, 3, 9),\n           (11, 10, 9),\n           (11, 9, 5),\n           (5, 9, 7),\n           (9, 3, 7),\n           (1, 6, 5),\n           ]\n\n  # By default the top is an aris parallel to the Y axis.\n  # Need to rotate around the y axis by half the supplementary to the\n  # angle between faces divided by two to get the desired orientation.\n  #                          /O\\  (top arist)\n  #                     /          \\                           Z\n  # (adjacent face)/                    \\  (adjacent face)     ^\n  #           /     angle_between_faces      \\                 |\n  #      /                                        \\            |\n  #  /                                                 \\      YO-----> X\n  # This results in:\n  #  (adjacent faceis now top plane)\n  #  ----------------------O\\  (top arist)\n  #                           \\\n  #                             \\\n  #                               \\     (adjacent face)\n  #                                 \\\n  #                                   \\\n  #                                     \\\n\n  angle_between_faces = 2 * np.arcsin(phi / np.sqrt(3))\n  rotation_angle = (np.pi - angle_between_faces) / 2\n  rotation = transform.Rotation.from_euler(seq=\"y\", angles=rotation_angle)\n  rotation_matrix = rotation.as_matrix()\n  vertices = np.dot(vertices, rotation_matrix)\n\n  return TriangularMesh(vertices=vertices.astype(np.float32),\n                        faces=np.array(faces, dtype=np.int32))\n\n\ndef _two_split_unit_sphere_triangle_faces(\n    triangular_mesh: TriangularMesh) -> TriangularMesh:\n  \"\"\"Splits each triangular face into 4 triangles keeping the orientation.\"\"\"\n\n  # Every time we split a triangle into 4 we will be adding 3 extra vertices,\n  # located at the edge centres.\n  # This class handles the positioning of the new vertices, and avoids creating\n  # duplicates.\n  new_vertices_builder = _ChildVerticesBuilder(triangular_mesh.vertices)\n\n  new_faces = []\n  for ind1, ind2, ind3 in triangular_mesh.faces:\n    # Transform each triangular face into 4 triangles,\n    # preserving the orientation.\n    #                    ind3\n    #                   /    \\\n    #                /          \\\n    #              /      #3       \\\n    #            /                  \\\n    #         ind31 -------------- ind23\n    #         /   \\                /   \\\n    #       /       \\     #4     /      \\\n    #     /    #1     \\        /    #2    \\\n    #   /               \\    /              \\\n    # ind1 ------------ ind12 ------------ ind2\n    ind12 = new_vertices_builder.get_new_child_vertex_index((ind1, ind2))\n    ind23 = new_vertices_builder.get_new_child_vertex_index((ind2, ind3))\n    ind31 = new_vertices_builder.get_new_child_vertex_index((ind3, ind1))\n    # Note how each of the 4 triangular new faces specifies the order of the\n    # vertices to preserve the orientation of the original face. As the input\n    # face should always be counter-clockwise as specified in the diagram,\n    # this means child faces should also be counter-clockwise.\n    new_faces.extend([[ind1, ind12, ind31],  # 1\n                      [ind12, ind2, ind23],  # 2\n                      [ind31, ind23, ind3],  # 3\n                      [ind12, ind23, ind31],  # 4\n                      ])\n  return TriangularMesh(vertices=new_vertices_builder.get_all_vertices(),\n                        faces=np.array(new_faces, dtype=np.int32))\n\n\nclass _ChildVerticesBuilder(object):\n  \"\"\"Bookkeeping of new child vertices added to an existing set of vertices.\"\"\"\n\n  def __init__(self, parent_vertices):\n\n    # Because the same new vertex will be required when splitting adjacent\n    # triangles (which share an edge) we keep them in a hash table indexed by\n    # sorted indices of the vertices adjacent to the edge, to avoid creating\n    # duplicated child vertices.\n    self._child_vertices_index_mapping = {}\n    self._parent_vertices = parent_vertices\n    # We start with all previous vertices.\n    self._all_vertices_list = list(parent_vertices)\n\n  def _get_child_vertex_key(self, parent_vertex_indices):\n    return tuple(sorted(parent_vertex_indices))\n\n  def _create_child_vertex(self, parent_vertex_indices):\n    \"\"\"Creates a new vertex.\"\"\"\n    # Position for new vertex is the middle point, between the parent points,\n    # projected to unit sphere.\n    child_vertex_position = self._parent_vertices[\n        list(parent_vertex_indices)].mean(0)\n    child_vertex_position /= np.linalg.norm(child_vertex_position)\n\n    # Add the vertex to the output list. The index for this new vertex will\n    # match the length of the list before adding it.\n    child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n    self._child_vertices_index_mapping[child_vertex_key] = len(\n        self._all_vertices_list)\n    self._all_vertices_list.append(child_vertex_position)\n\n  def get_new_child_vertex_index(self, parent_vertex_indices):\n    \"\"\"Returns index for a child vertex, creating it if necessary.\"\"\"\n    # Get the key to see if we already have a new vertex in the middle.\n    child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n    if child_vertex_key not in self._child_vertices_index_mapping:\n      self._create_child_vertex(parent_vertex_indices)\n    return self._child_vertices_index_mapping[child_vertex_key]\n\n  def get_all_vertices(self):\n    \"\"\"Returns an array with old vertices.\"\"\"\n    return np.array(self._all_vertices_list)\n\n\ndef faces_to_edges(faces: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n  \"\"\"Transforms polygonal faces to sender and receiver indices.\n\n  It does so by transforming every face into N_i edges. Such if the triangular\n  face has indices [0, 1, 2], three edges are added 0->1, 1->2, and 2->0.\n\n  If all faces have consistent orientation, and the surface represented by the\n  faces is closed, then every edge in a polygon with a certain orientation\n  is also part of another polygon with the opposite orientation. In this\n  situation, the edges returned by the method are always bidirectional.\n\n  Args:\n    faces: Integer array of shape [num_faces, 3]. Contains node indices\n        adjacent to each face.\n  Returns:\n    Tuple with sender/receiver indices, each of shape [num_edges=num_faces*3].\n\n  \"\"\"\n  assert faces.ndim == 2\n  assert faces.shape[-1] == 3\n  senders = np.concatenate([faces[:, 0], faces[:, 1], faces[:, 2]])\n  receivers = np.concatenate([faces[:, 1], faces[:, 2], faces[:, 0]])\n  return senders, receivers\n",
    "12": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for icosahedral_mesh.\"\"\"\n\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nimport chex\nfrom graphcast import icosahedral_mesh\nimport numpy as np\n\n\ndef _get_mesh_spec(splits: int):\n  \"\"\"Returns size of the final icosahedral mesh resulting from the splitting.\"\"\"\n  num_vertices = 12\n  num_faces = 20\n  for _ in range(splits):\n    # Each previous face adds three new vertices, but each vertex is shared\n    # by two faces.\n    num_vertices += num_faces * 3 // 2\n    num_faces *= 4\n  return num_vertices, num_faces\n\n\nclass IcosahedralMeshTest(parameterized.TestCase):\n\n  def test_icosahedron(self):\n    mesh = icosahedral_mesh.get_icosahedron()\n    _assert_valid_mesh(\n        mesh, num_expected_vertices=12, num_expected_faces=20)\n\n  @parameterized.parameters(list(range(5)))\n  def test_get_hierarchy_of_triangular_meshes_for_sphere(self, splits):\n    meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(\n        splits=splits)\n    prev_vertices = None\n    for mesh_i, mesh in enumerate(meshes):\n      # Check that `mesh` is valid.\n      num_expected_vertices, num_expected_faces = _get_mesh_spec(mesh_i)\n      _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces)\n\n      # Check that the first N vertices from this mesh match all of the\n      # vertices from the previous mesh.\n      if prev_vertices is not None:\n        leading_mesh_vertices = mesh.vertices[:prev_vertices.shape[0]]\n        np.testing.assert_array_equal(leading_mesh_vertices, prev_vertices)\n\n      # Increase the expected/previous values for the next iteration.\n      if mesh_i < len(meshes) - 1:\n        prev_vertices = mesh.vertices\n\n  @parameterized.parameters(list(range(4)))\n  def test_merge_meshes(self, splits):\n    mesh_hierarchy = (\n        icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(\n            splits=splits))\n    mesh = icosahedral_mesh.merge_meshes(mesh_hierarchy)\n\n    expected_faces = np.concatenate([m.faces for m in mesh_hierarchy], axis=0)\n    np.testing.assert_array_equal(mesh.vertices, mesh_hierarchy[-1].vertices)\n    np.testing.assert_array_equal(mesh.faces, expected_faces)\n\n  def test_faces_to_edges(self):\n\n    faces = np.array([[0, 1, 2],\n                      [3, 4, 5]])\n\n    # This also documents the order of the edges returned by the method.\n    expected_edges = np.array(\n        [[0, 1],\n         [3, 4],\n         [1, 2],\n         [4, 5],\n         [2, 0],\n         [5, 3]])\n    expected_senders = expected_edges[:, 0]\n    expected_receivers = expected_edges[:, 1]\n\n    senders, receivers = icosahedral_mesh.faces_to_edges(faces)\n\n    np.testing.assert_array_equal(senders, expected_senders)\n    np.testing.assert_array_equal(receivers, expected_receivers)\n\n\ndef _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces):\n  vertices = mesh.vertices\n  faces = mesh.faces\n  chex.assert_shape(vertices, [num_expected_vertices, 3])\n  chex.assert_shape(faces, [num_expected_faces, 3])\n\n  # Vertices norm should be 1.\n  vertices_norm = np.linalg.norm(vertices, axis=-1)\n  np.testing.assert_allclose(vertices_norm, 1., rtol=1e-6)\n\n  _assert_positive_face_orientation(vertices, faces)\n\n\ndef _assert_positive_face_orientation(vertices, faces):\n\n  # Obtain a unit vector that points, in the direction of the face.\n  face_orientation = np.cross(vertices[faces[:, 1]] - vertices[faces[:, 0]],\n                              vertices[faces[:, 2]] - vertices[faces[:, 1]])\n  face_orientation /= np.linalg.norm(face_orientation, axis=-1, keepdims=True)\n\n  # And a unit vector pointing from the origin to the center of the face.\n  face_centers = vertices[faces].mean(1)\n  face_centers /= np.linalg.norm(face_centers, axis=-1, keepdims=True)\n\n  # Positive orientation means those two vectors should be parallel\n  # (dot product, 1), and not anti-parallel (dot product, -1).\n  dot_center_orientation = np.einsum(\"ik,ik->i\", face_orientation, face_centers)\n\n  # Check that the face normal is parallel to the vector that joins the center\n  # of the face to the center of the sphere. Note we need a small tolerance\n  # because some discretizations are not exactly uniform, so it will not be\n  # exactly parallel.\n  np.testing.assert_allclose(dot_center_orientation, 1., atol=6e-4)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "13": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Loss functions (and terms for use in loss functions) used for weather.\"\"\"\n\nfrom typing import Mapping\n\nfrom graphcast import xarray_tree\nimport numpy as np\nfrom typing_extensions import Protocol\nimport xarray\n\n\nLossAndDiagnostics = tuple[xarray.DataArray, xarray.Dataset]\n\n\nclass LossFunction(Protocol):\n  \"\"\"A loss function.\n\n  This is a protocol so it's fine to use a plain function which 'quacks like'\n  this. This is just to document the interface.\n  \"\"\"\n\n  def __call__(self,\n               predictions: xarray.Dataset,\n               targets: xarray.Dataset,\n               **optional_kwargs) -> LossAndDiagnostics:\n    \"\"\"Computes a loss function.\n\n    Args:\n      predictions: Dataset of predictions.\n      targets: Dataset of targets.\n      **optional_kwargs: Implementations may support extra optional kwargs.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n    \"\"\"\n\n\ndef weighted_mse_per_level(\n    predictions: xarray.Dataset,\n    targets: xarray.Dataset,\n    per_variable_weights: Mapping[str, float],\n) -> LossAndDiagnostics:\n  \"\"\"Latitude- and pressure-level-weighted MSE loss.\"\"\"\n  def loss(prediction, target):\n    loss = (prediction - target)**2\n    loss *= normalized_latitude_weights(target).astype(loss.dtype)\n    if 'level' in target.dims:\n      loss *= normalized_level_weights(target).astype(loss.dtype)\n    return _mean_preserving_batch(loss)\n\n  losses = xarray_tree.map_structure(loss, predictions, targets)\n  return sum_per_variable_losses(losses, per_variable_weights)\n\n\ndef _mean_preserving_batch(x: xarray.DataArray) -> xarray.DataArray:\n  return x.mean([d for d in x.dims if d != 'batch'], skipna=False)\n\n\ndef sum_per_variable_losses(\n    per_variable_losses: Mapping[str, xarray.DataArray],\n    weights: Mapping[str, float],\n) -> LossAndDiagnostics:\n  \"\"\"Weighted sum of per-variable losses.\"\"\"\n  if not set(weights.keys()).issubset(set(per_variable_losses.keys())):\n    raise ValueError(\n        'Passing a weight that does not correspond to any variable '\n        f'{set(weights.keys())-set(per_variable_losses.keys())}')\n\n  weighted_per_variable_losses = {\n      name: loss * weights.get(name, 1)\n      for name, loss in per_variable_losses.items()\n  }\n  total = xarray.concat(\n      weighted_per_variable_losses.values(), dim='variable', join='exact').sum(\n          'variable', skipna=False)\n  return total, per_variable_losses  # pytype: disable=bad-return-type\n\n\ndef normalized_level_weights(data: xarray.DataArray) -> xarray.DataArray:\n  \"\"\"Weights proportional to pressure at each level.\"\"\"\n  level = data.coords['level']\n  return level / level.mean(skipna=False)\n\n\ndef normalized_latitude_weights(data: xarray.DataArray) -> xarray.DataArray:\n  \"\"\"Weights based on latitude, roughly proportional to grid cell area.\n\n  This method supports two use cases only (both for equispaced values):\n  * Latitude values such that the closest value to the pole is at latitude\n    (90 - d_lat/2), where d_lat is the difference between contiguous latitudes.\n    For example: [-89, -87, -85, ..., 85, 87, 89]) (d_lat = 2)\n    In this case each point with `lat` value represents a sphere slice between\n    `lat - d_lat/2` and `lat + d_lat/2`, and the area of this slice would be\n    proportional to:\n    `sin(lat + d_lat/2) - sin(lat - d_lat/2) = 2 * sin(d_lat/2) * cos(lat)`, and\n    we can simply omit the term `2 * sin(d_lat/2)` which is just a constant\n    that cancels during normalization.\n  * Latitude values that fall exactly at the poles.\n    For example: [-90, -88, -86, ..., 86, 88, 90]) (d_lat = 2)\n    In this case each point with `lat` value also represents\n    a sphere slice between `lat - d_lat/2` and `lat + d_lat/2`,\n    except for the points at the poles, that represent a slice between\n    `90 - d_lat/2` and `90` or, `-90` and  `-90 + d_lat/2`.\n    The areas of the first type of point are still proportional to:\n    * sin(lat + d_lat/2) - sin(lat - d_lat/2) = 2 * sin(d_lat/2) * cos(lat)\n    but for the points at the poles now is:\n    * sin(90) - sin(90 - d_lat/2) = 2 * sin(d_lat/4) ^ 2\n    and we will be using these weights, depending on whether we are looking at\n    pole cells, or non-pole cells (omitting the common factor of 2 which will be\n    absorbed by the normalization).\n\n    It can be shown via a limit, or simple geometry, that in the small angles\n    regime, the proportion of area per pole-point is equal to 1/8th\n    the proportion of area covered by each of the nearest non-pole point, and we\n    test for this in the test.\n\n  Args:\n    data: `DataArray` with latitude coordinates.\n  Returns:\n    Unit mean latitude weights.\n  \"\"\"\n  latitude = data.coords['lat']\n\n  if np.any(np.isclose(np.abs(latitude), 90.)):\n    weights = _weight_for_latitude_vector_with_poles(latitude)\n  else:\n    weights = _weight_for_latitude_vector_without_poles(latitude)\n\n  return weights / weights.mean(skipna=False)\n\n\ndef _weight_for_latitude_vector_without_poles(latitude):\n  \"\"\"Weights for uniform latitudes of the form [+-90-+d/2, ..., -+90+-d/2].\"\"\"\n  delta_latitude = np.abs(_check_uniform_spacing_and_get_delta(latitude))\n  if (not np.isclose(np.max(latitude), 90 - delta_latitude/2) or\n      not np.isclose(np.min(latitude), -90 + delta_latitude/2)):\n    raise ValueError(\n        f'Latitude vector {latitude} does not start/end at '\n        '+- (90 - delta_latitude/2) degrees.')\n  return np.cos(np.deg2rad(latitude))\n\n\ndef _weight_for_latitude_vector_with_poles(latitude):\n  \"\"\"Weights for uniform latitudes of the form [+- 90, ..., -+90].\"\"\"\n  delta_latitude = np.abs(_check_uniform_spacing_and_get_delta(latitude))\n  if (not np.isclose(np.max(latitude), 90.) or\n      not np.isclose(np.min(latitude), -90.)):\n    raise ValueError(\n        f'Latitude vector {latitude} does not start/end at +- 90 degrees.')\n  weights = np.cos(np.deg2rad(latitude)) * np.sin(np.deg2rad(delta_latitude/2))\n  # The two checks above enough to guarantee that latitudes are sorted, so\n  # the extremes are the poles\n  weights[[0, -1]] = np.sin(np.deg2rad(delta_latitude/4)) ** 2\n  return weights\n\n\ndef _check_uniform_spacing_and_get_delta(vector):\n  diff = np.diff(vector)\n  if not np.all(np.isclose(diff[0], diff)):\n    raise ValueError(f'Vector {diff} is not uniformly spaced.')\n  return diff[0]\n",
    "14": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities for building models.\"\"\"\n\nfrom typing import Mapping, Optional, Tuple\n\nimport numpy as np\nfrom scipy.spatial import transform\nimport xarray\n\n\ndef get_graph_spatial_features(\n    *, node_lat: np.ndarray, node_lon: np.ndarray,\n    senders: np.ndarray, receivers: np.ndarray,\n    add_node_positions: bool,\n    add_node_latitude: bool,\n    add_node_longitude: bool,\n    add_relative_positions: bool,\n    relative_longitude_local_coordinates: bool,\n    relative_latitude_local_coordinates: bool,\n    sine_cosine_encoding: bool = False,\n    encoding_num_freqs: int = 10,\n    encoding_multiplicative_factor: float = 1.2,\n    ) -> Tuple[np.ndarray, np.ndarray]:\n  \"\"\"Computes spatial features for the nodes.\n\n  Args:\n    node_lat: Latitudes in the [-90, 90] interval of shape [num_nodes]\n    node_lon: Longitudes in the [0, 360] interval of shape [num_nodes]\n    senders: Sender indices of shape [num_edges]\n    receivers: Receiver indices of shape [num_edges]\n    add_node_positions: Add unit norm absolute positions.\n    add_node_latitude: Add a feature for latitude (cos(90 - lat))\n        Note even if this is set to False, the model may be able to infer the\n        longitude from relative features, unless\n        `relative_latitude_local_coordinates` is also True, or if there is any\n        bias on the relative edge sizes for different longitudes.\n    add_node_longitude: Add features for longitude (cos(lon), sin(lon)).\n        Note even if this is set to False, the model may be able to infer the\n        longitude from relative features, unless\n        `relative_longitude_local_coordinates` is also True, or if there is any\n        bias on the relative edge sizes for different longitudes.\n    add_relative_positions: Whether to relative positions in R3 to the edges.\n    relative_longitude_local_coordinates: If True, relative positions are\n        computed in a local space where the receiver is at 0 longitude.\n    relative_latitude_local_coordinates: If True, relative positions are\n        computed in a local space where the receiver is at 0 latitude.\n    sine_cosine_encoding: If True, we will transform the node/edge features\n        with sine and cosine functions, similar to NERF.\n    encoding_num_freqs: frequency parameter\n    encoding_multiplicative_factor: used for calculating the frequency.\n\n  Returns:\n    Arrays of shape: [num_nodes, num_features] and [num_edges, num_features].\n    with node and edge features.\n\n  \"\"\"\n\n  num_nodes = node_lat.shape[0]\n  num_edges = senders.shape[0]\n  dtype = node_lat.dtype\n  node_phi, node_theta = lat_lon_deg_to_spherical(node_lat, node_lon)\n\n  # Computing some node features.\n  node_features = []\n  if add_node_positions:\n    # Already in [-1, 1.] range.\n    node_features.extend(spherical_to_cartesian(node_phi, node_theta))\n\n  if add_node_latitude:\n    # Using the cos of theta.\n    # From 1. (north pole) to -1 (south pole).\n    node_features.append(np.cos(node_theta))\n\n  if add_node_longitude:\n    # Using the cos and sin, which is already normalized.\n    node_features.append(np.cos(node_phi))\n    node_features.append(np.sin(node_phi))\n\n  if not node_features:\n    node_features = np.zeros([num_nodes, 0], dtype=dtype)\n  else:\n    node_features = np.stack(node_features, axis=-1)\n\n  # Computing some edge features.\n  edge_features = []\n\n  if add_relative_positions:\n\n    relative_position = get_relative_position_in_receiver_local_coordinates(\n        node_phi=node_phi,\n        node_theta=node_theta,\n        senders=senders,\n        receivers=receivers,\n        latitude_local_coordinates=relative_latitude_local_coordinates,\n        longitude_local_coordinates=relative_longitude_local_coordinates\n        )\n\n    # Note this is L2 distance in 3d space, rather than geodesic distance.\n    relative_edge_distances = np.linalg.norm(\n        relative_position, axis=-1, keepdims=True)\n\n    # Normalize to the maximum edge distance. Note that we expect to always\n    # have an edge that goes in the opposite direction of any given edge\n    # so the distribution of relative positions should be symmetric around\n    # zero. So by scaling by the maximum length, we expect all relative\n    # positions to fall in the [-1., 1.] interval, and all relative distances\n    # to fall in the [0., 1.] interval.\n    max_edge_distance = relative_edge_distances.max()\n    edge_features.append(relative_edge_distances / max_edge_distance)\n    edge_features.append(relative_position / max_edge_distance)\n\n  if not edge_features:\n    edge_features = np.zeros([num_edges, 0], dtype=dtype)\n  else:\n    edge_features = np.concatenate(edge_features, axis=-1)\n\n  if sine_cosine_encoding:\n    def sine_cosine_transform(x: np.ndarray) -> np.ndarray:\n      freqs = encoding_multiplicative_factor**np.arange(encoding_num_freqs)\n      phases = freqs * x[..., None]\n      x_sin = np.sin(phases)\n      x_cos = np.cos(phases)\n      x_cat = np.concatenate([x_sin, x_cos], axis=-1)\n      return x_cat.reshape([x.shape[0], -1])\n\n    node_features = sine_cosine_transform(node_features)\n    edge_features = sine_cosine_transform(edge_features)\n\n  return node_features, edge_features\n\n\ndef lat_lon_to_leading_axes(\n    grid_xarray: xarray.DataArray) -> xarray.DataArray:\n  \"\"\"Reorders xarray so lat/lon axes come first.\"\"\"\n  # leading + [\"lat\", \"lon\"] + trailing\n  # to\n  # [\"lat\", \"lon\"] + leading + trailing\n  return grid_xarray.transpose(\"lat\", \"lon\", ...)\n\n\ndef restore_leading_axes(grid_xarray: xarray.DataArray) -> xarray.DataArray:\n  \"\"\"Reorders xarray so batch/time/level axes come first (if present).\"\"\"\n\n  # [\"lat\", \"lon\"] + [(batch,) (time,) (level,)] + trailing\n  # to\n  # [(batch,) (time,) (level,)] + [\"lat\", \"lon\"] + trailing\n\n  input_dims = list(grid_xarray.dims)\n  output_dims = list(input_dims)\n  for leading_key in [\"level\", \"time\", \"batch\"]:  # reverse order for insert\n    if leading_key in input_dims:\n      output_dims.remove(leading_key)\n      output_dims.insert(0, leading_key)\n  return grid_xarray.transpose(*output_dims)\n\n\ndef lat_lon_deg_to_spherical(node_lat: np.ndarray,\n                             node_lon: np.ndarray,\n                            ) -> Tuple[np.ndarray, np.ndarray]:\n  phi = np.deg2rad(node_lon)\n  theta = np.deg2rad(90 - node_lat)\n  return phi, theta\n\n\ndef spherical_to_lat_lon(phi: np.ndarray,\n                         theta: np.ndarray,\n                        ) -> Tuple[np.ndarray, np.ndarray]:\n  lon = np.mod(np.rad2deg(phi), 360)\n  lat = 90 - np.rad2deg(theta)\n  return lat, lon\n\n\ndef cartesian_to_spherical(x: np.ndarray,\n                           y: np.ndarray,\n                           z: np.ndarray,\n                          ) -> Tuple[np.ndarray, np.ndarray]:\n  phi = np.arctan2(y, x)\n  with np.errstate(invalid=\"ignore\"):  # circumventing b/253179568\n    theta = np.arccos(z)  # Assuming unit radius.\n  return phi, theta\n\n\ndef spherical_to_cartesian(\n    phi: np.ndarray, theta: np.ndarray\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n  # Assuming unit radius.\n  return (np.cos(phi)*np.sin(theta),\n          np.sin(phi)*np.sin(theta),\n          np.cos(theta))\n\n\ndef get_relative_position_in_receiver_local_coordinates(\n    node_phi: np.ndarray,\n    node_theta: np.ndarray,\n    senders: np.ndarray,\n    receivers: np.ndarray,\n    latitude_local_coordinates: bool,\n    longitude_local_coordinates: bool\n    ) -> np.ndarray:\n  \"\"\"Returns relative position features for the edges.\n\n  The relative positions will be computed in a rotated space for a local\n  coordinate system as defined by the receiver. The relative positions are\n  simply obtained by subtracting sender position minues receiver position in\n  that local coordinate system after the rotation in R^3.\n\n  Args:\n    node_phi: [num_nodes] with polar angles.\n    node_theta: [num_nodes] with azimuthal angles.\n    senders: [num_edges] with indices.\n    receivers: [num_edges] with indices.\n    latitude_local_coordinates: Whether to rotate edges such that in the\n        positions are computed such that the receiver is always at latitude 0.\n    longitude_local_coordinates: Whether to rotate edges such that in the\n        positions are computed such that the receiver is always at longitude 0.\n\n  Returns:\n    Array of relative positions in R3 [num_edges, 3]\n  \"\"\"\n\n  node_pos = np.stack(spherical_to_cartesian(node_phi, node_theta), axis=-1)\n\n  # No rotation in this case.\n  if not (latitude_local_coordinates or longitude_local_coordinates):\n    return node_pos[senders] - node_pos[receivers]\n\n  # Get rotation matrices for the local space space for every node.\n  rotation_matrices = get_rotation_matrices_to_local_coordinates(\n      reference_phi=node_phi,\n      reference_theta=node_theta,\n      rotate_latitude=latitude_local_coordinates,\n      rotate_longitude=longitude_local_coordinates)\n\n  # Each edge will be rotated according to the rotation matrix of its receiver\n  # node.\n  edge_rotation_matrices = rotation_matrices[receivers]\n\n  # Rotate all nodes to the rotated space of the corresponding edge.\n  # Note for receivers we can also do the matmul first and the gather second:\n  # ```\n  # receiver_pos_in_rotated_space = rotate_with_matrices(\n  #    rotation_matrices, node_pos)[receivers]\n  # ```\n  # which is more efficient, however, we do gather first to keep it more\n  # symmetric with the sender computation.\n  receiver_pos_in_rotated_space = rotate_with_matrices(\n      edge_rotation_matrices, node_pos[receivers])\n  sender_pos_in_in_rotated_space = rotate_with_matrices(\n      edge_rotation_matrices, node_pos[senders])\n  # Note, here, that because the rotated space is chosen according to the\n  # receiver, if:\n  # * latitude_local_coordinates = True: latitude for the receivers will be\n  #   0, that is the z coordinate will always be 0.\n  # * longitude_local_coordinates = True: longitude for the receivers will be\n  #   0, that is the y coordinate will be 0.\n\n  # Now we can just subtract.\n  # Note we are rotating to a local coordinate system, where the y-z axes are\n  # parallel to a tangent plane to the sphere, but still remain in a 3d space.\n  # Note that if both `latitude_local_coordinates` and\n  # `longitude_local_coordinates` are True, and edges are short,\n  # then the difference in x coordinate between sender and receiver\n  # should be small, so we could consider dropping the new x coordinate if\n  # we wanted to the tangent plane, however in doing so\n  # we would lose information about the curvature of the mesh, which may be\n  # important for very coarse meshes.\n  return sender_pos_in_in_rotated_space - receiver_pos_in_rotated_space\n\n\ndef get_rotation_matrices_to_local_coordinates(\n    reference_phi: np.ndarray,\n    reference_theta: np.ndarray,\n    rotate_latitude: bool,\n    rotate_longitude: bool) -> np.ndarray:\n\n  \"\"\"Returns a rotation matrix to rotate to a point based on a reference vector.\n\n  The rotation matrix is build such that, a vector in the\n  same coordinate system at the reference point that points towards the pole\n  before the rotation, continues to point towards the pole after the rotation.\n\n  Args:\n    reference_phi: [leading_axis] Polar angles of the reference.\n    reference_theta: [leading_axis] Azimuthal angles of the reference.\n    rotate_latitude: Whether to produce a rotation matrix that would rotate\n        R^3 vectors to zero latitude.\n    rotate_longitude: Whether to produce a rotation matrix that would rotate\n        R^3 vectors to zero longitude.\n\n  Returns:\n    Matrices of shape [leading_axis] such that when applied to the reference\n        position with `rotate_with_matrices(rotation_matrices, reference_pos)`\n\n        * phi goes to 0. if \"rotate_longitude\" is True.\n\n        * theta goes to np.pi / 2 if \"rotate_latitude\" is True.\n\n        The rotation consists of:\n        * rotate_latitude = False, rotate_longitude = True:\n            Latitude preserving rotation.\n        * rotate_latitude = True, rotate_longitude = True:\n            Latitude preserving rotation, followed by longitude preserving\n            rotation.\n        * rotate_latitude = True, rotate_longitude = False:\n            Latitude preserving rotation, followed by longitude preserving\n            rotation, and the inverse of the latitude preserving rotation. Note\n            this is computationally different from rotating the longitude only\n            and is. We do it like this, so the polar geodesic curve, continues\n            to be aligned with one of the axis after the rotation.\n\n  \"\"\"\n\n  if rotate_longitude and rotate_latitude:\n\n    # We first rotate around the z axis \"minus the azimuthal angle\", to get the\n    # point with zero longitude\n    azimuthal_rotation = - reference_phi\n\n    # One then we will do a polar rotation (which can be done along the y\n    # axis now that we are at longitude 0.), \"minus the polar angle plus 2pi\"\n    # to get the point with zero latitude.\n    polar_rotation = - reference_theta + np.pi/2\n\n    return transform.Rotation.from_euler(\n        \"zy\", np.stack([azimuthal_rotation, polar_rotation],\n                       axis=1)).as_matrix()\n  elif rotate_longitude:\n    # Just like the previous case, but applying only the azimuthal rotation.\n    azimuthal_rotation = - reference_phi\n    return transform.Rotation.from_euler(\"z\", -reference_phi).as_matrix()\n  elif rotate_latitude:\n    # Just like the first case, but after doing the polar rotation, undoing\n    # the azimuthal rotation.\n    azimuthal_rotation = - reference_phi\n    polar_rotation = - reference_theta + np.pi/2\n\n    return transform.Rotation.from_euler(\n        \"zyz\", np.stack(\n            [azimuthal_rotation, polar_rotation, -azimuthal_rotation]\n            , axis=1)).as_matrix()\n  else:\n    raise ValueError(\n        \"At least one of longitude and latitude should be rotated.\")\n\n\ndef rotate_with_matrices(rotation_matrices: np.ndarray, positions: np.ndarray\n                         ) -> np.ndarray:\n  return np.einsum(\"bji,bi->bj\", rotation_matrices, positions)\n\n\ndef get_bipartite_graph_spatial_features(\n    *,\n    senders_node_lat: np.ndarray,\n    senders_node_lon: np.ndarray,\n    senders: np.ndarray,\n    receivers_node_lat: np.ndarray,\n    receivers_node_lon: np.ndarray,\n    receivers: np.ndarray,\n    add_node_positions: bool,\n    add_node_latitude: bool,\n    add_node_longitude: bool,\n    add_relative_positions: bool,\n    edge_normalization_factor: Optional[float] = None,\n    relative_longitude_local_coordinates: bool,\n    relative_latitude_local_coordinates: bool,\n) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n  \"\"\"Computes spatial features for the nodes.\n\n  This function is almost identical to `get_graph_spatial_features`. The only\n  difference is that sender nodes and receiver nodes can be in different arrays.\n  This is necessary to enable combination with typed Graph.\n\n  Args:\n    senders_node_lat: Latitudes in the [-90, 90] interval of shape\n      [num_sender_nodes]\n    senders_node_lon: Longitudes in the [0, 360] interval of shape\n      [num_sender_nodes]\n    senders: Sender indices of shape [num_edges], indices in [0,\n      num_sender_nodes)\n    receivers_node_lat: Latitudes in the [-90, 90] interval of shape\n      [num_receiver_nodes]\n    receivers_node_lon: Longitudes in the [0, 360] interval of shape\n      [num_receiver_nodes]\n    receivers: Receiver indices of shape [num_edges], indices in [0,\n      num_receiver_nodes)\n    add_node_positions: Add unit norm absolute positions.\n    add_node_latitude: Add a feature for latitude (cos(90 - lat)) Note even if\n      this is set to False, the model may be able to infer the longitude from\n      relative features, unless `relative_latitude_local_coordinates` is also\n      True, or if there is any bias on the relative edge sizes for different\n      longitudes.\n    add_node_longitude: Add features for longitude (cos(lon), sin(lon)). Note\n      even if this is set to False, the model may be able to infer the longitude\n      from relative features, unless `relative_longitude_local_coordinates` is\n      also True, or if there is any bias on the relative edge sizes for\n      different longitudes.\n    add_relative_positions: Whether to relative positions in R3 to the edges.\n    edge_normalization_factor: Allows explicitly controlling edge normalization.\n      If None, defaults to max edge length. This supports using pre-trained\n      model weights with a different graph structure to what it was trained on.\n    relative_longitude_local_coordinates: If True, relative positions are\n      computed in a local space where the receiver is at 0 longitude.\n    relative_latitude_local_coordinates: If True, relative positions are\n      computed in a local space where the receiver is at 0 latitude.\n\n  Returns:\n    Arrays of shape: [num_nodes, num_features] and [num_edges, num_features].\n    with node and edge features.\n\n  \"\"\"\n\n  num_senders = senders_node_lat.shape[0]\n  num_receivers = receivers_node_lat.shape[0]\n  num_edges = senders.shape[0]\n  dtype = senders_node_lat.dtype\n  assert receivers_node_lat.dtype == dtype\n  senders_node_phi, senders_node_theta = lat_lon_deg_to_spherical(\n      senders_node_lat, senders_node_lon)\n  receivers_node_phi, receivers_node_theta = lat_lon_deg_to_spherical(\n      receivers_node_lat, receivers_node_lon)\n\n  # Computing some node features.\n  senders_node_features = []\n  receivers_node_features = []\n  if add_node_positions:\n    # Already in [-1, 1.] range.\n    senders_node_features.extend(\n        spherical_to_cartesian(senders_node_phi, senders_node_theta))\n    receivers_node_features.extend(\n        spherical_to_cartesian(receivers_node_phi, receivers_node_theta))\n\n  if add_node_latitude:\n    # Using the cos of theta.\n    # From 1. (north pole) to -1 (south pole).\n    senders_node_features.append(np.cos(senders_node_theta))\n    receivers_node_features.append(np.cos(receivers_node_theta))\n\n  if add_node_longitude:\n    # Using the cos and sin, which is already normalized.\n    senders_node_features.append(np.cos(senders_node_phi))\n    senders_node_features.append(np.sin(senders_node_phi))\n\n    receivers_node_features.append(np.cos(receivers_node_phi))\n    receivers_node_features.append(np.sin(receivers_node_phi))\n\n  if not senders_node_features:\n    senders_node_features = np.zeros([num_senders, 0], dtype=dtype)\n    receivers_node_features = np.zeros([num_receivers, 0], dtype=dtype)\n  else:\n    senders_node_features = np.stack(senders_node_features, axis=-1)\n    receivers_node_features = np.stack(receivers_node_features, axis=-1)\n\n  # Computing some edge features.\n  edge_features = []\n\n  if add_relative_positions:\n\n    relative_position = get_bipartite_relative_position_in_receiver_local_coordinates(  # pylint: disable=line-too-long\n        senders_node_phi=senders_node_phi,\n        senders_node_theta=senders_node_theta,\n        receivers_node_phi=receivers_node_phi,\n        receivers_node_theta=receivers_node_theta,\n        senders=senders,\n        receivers=receivers,\n        latitude_local_coordinates=relative_latitude_local_coordinates,\n        longitude_local_coordinates=relative_longitude_local_coordinates)\n\n    # Note this is L2 distance in 3d space, rather than geodesic distance.\n    relative_edge_distances = np.linalg.norm(\n        relative_position, axis=-1, keepdims=True)\n\n    if edge_normalization_factor is None:\n      # Normalize to the maximum edge distance. Note that we expect to always\n      # have an edge that goes in the opposite direction of any given edge\n      # so the distribution of relative positions should be symmetric around\n      # zero. So by scaling by the maximum length, we expect all relative\n      # positions to fall in the [-1., 1.] interval, and all relative distances\n      # to fall in the [0., 1.] interval.\n      edge_normalization_factor = relative_edge_distances.max()\n\n    edge_features.append(relative_edge_distances / edge_normalization_factor)\n    edge_features.append(relative_position / edge_normalization_factor)\n\n  if not edge_features:\n    edge_features = np.zeros([num_edges, 0], dtype=dtype)\n  else:\n    edge_features = np.concatenate(edge_features, axis=-1)\n\n  return senders_node_features, receivers_node_features, edge_features\n\n\ndef get_bipartite_relative_position_in_receiver_local_coordinates(\n    senders_node_phi: np.ndarray,\n    senders_node_theta: np.ndarray,\n    senders: np.ndarray,\n    receivers_node_phi: np.ndarray,\n    receivers_node_theta: np.ndarray,\n    receivers: np.ndarray,\n    latitude_local_coordinates: bool,\n    longitude_local_coordinates: bool) -> np.ndarray:\n  \"\"\"Returns relative position features for the edges.\n\n  This function is equivalent to\n  `get_relative_position_in_receiver_local_coordinates`, but adapted to work\n  with bipartite typed graphs.\n\n  The relative positions will be computed in a rotated space for a local\n  coordinate system as defined by the receiver. The relative positions are\n  simply obtained by subtracting sender position minues receiver position in\n  that local coordinate system after the rotation in R^3.\n\n  Args:\n    senders_node_phi: [num_sender_nodes] with polar angles.\n    senders_node_theta: [num_sender_nodes] with azimuthal angles.\n    senders: [num_edges] with indices into sender nodes.\n    receivers_node_phi: [num_sender_nodes] with polar angles.\n    receivers_node_theta: [num_sender_nodes] with azimuthal angles.\n    receivers: [num_edges] with indices into receiver nodes.\n    latitude_local_coordinates: Whether to rotate edges such that in the\n      positions are computed such that the receiver is always at latitude 0.\n    longitude_local_coordinates: Whether to rotate edges such that in the\n      positions are computed such that the receiver is always at longitude 0.\n\n  Returns:\n    Array of relative positions in R3 [num_edges, 3]\n  \"\"\"\n\n  senders_node_pos = np.stack(\n      spherical_to_cartesian(senders_node_phi, senders_node_theta), axis=-1)\n\n  receivers_node_pos = np.stack(\n      spherical_to_cartesian(receivers_node_phi, receivers_node_theta), axis=-1)\n\n  # No rotation in this case.\n  if not (latitude_local_coordinates or longitude_local_coordinates):\n    return senders_node_pos[senders] - receivers_node_pos[receivers]\n\n  # Get rotation matrices for the local space space for every receiver node.\n  receiver_rotation_matrices = get_rotation_matrices_to_local_coordinates(\n      reference_phi=receivers_node_phi,\n      reference_theta=receivers_node_theta,\n      rotate_latitude=latitude_local_coordinates,\n      rotate_longitude=longitude_local_coordinates)\n\n  # Each edge will be rotated according to the rotation matrix of its receiver\n  # node.\n  edge_rotation_matrices = receiver_rotation_matrices[receivers]\n\n  # Rotate all nodes to the rotated space of the corresponding edge.\n  # Note for receivers we can also do the matmul first and the gather second:\n  # ```\n  # receiver_pos_in_rotated_space = rotate_with_matrices(\n  #    rotation_matrices, node_pos)[receivers]\n  # ```\n  # which is more efficient, however, we do gather first to keep it more\n  # symmetric with the sender computation.\n  receiver_pos_in_rotated_space = rotate_with_matrices(\n      edge_rotation_matrices, receivers_node_pos[receivers])\n  sender_pos_in_in_rotated_space = rotate_with_matrices(\n      edge_rotation_matrices, senders_node_pos[senders])\n  # Note, here, that because the rotated space is chosen according to the\n  # receiver, if:\n  # * latitude_local_coordinates = True: latitude for the receivers will be\n  #   0, that is the z coordinate will always be 0.\n  # * longitude_local_coordinates = True: longitude for the receivers will be\n  #   0, that is the y coordinate will be 0.\n\n  # Now we can just subtract.\n  # Note we are rotating to a local coordinate system, where the y-z axes are\n  # parallel to a tangent plane to the sphere, but still remain in a 3d space.\n  # Note that if both `latitude_local_coordinates` and\n  # `longitude_local_coordinates` are True, and edges are short,\n  # then the difference in x coordinate between sender and receiver\n  # should be small, so we could consider dropping the new x coordinate if\n  # we wanted to the tangent plane, however in doing so\n  # we would lose information about the curvature of the mesh, which may be\n  # important for very coarse meshes.\n  return sender_pos_in_in_rotated_space - receiver_pos_in_rotated_space\n\n\ndef variable_to_stacked(\n    variable: xarray.Variable,\n    sizes: Mapping[str, int],\n    preserved_dims: Tuple[str, ...] = (\"batch\", \"lat\", \"lon\"),\n    ) -> xarray.Variable:\n  \"\"\"Converts an xarray.Variable to preserved_dims + (\"channels\",).\n\n  Any dimensions other than those included in preserved_dims get stacked into a\n  final \"channels\" dimension. If any of the preserved_dims are missing then they\n  are added, with the data broadcast/tiled to match the sizes specified in\n  `sizes`.\n\n  Args:\n    variable: An xarray.Variable.\n    sizes: Mapping including sizes for any dimensions which are not present in\n      `variable` but are needed for the output. This may be needed for example\n      for a static variable with only (\"lat\", \"lon\") dims, or if you want to\n      encode just the latitude coordinates (a variable with dims (\"lat\",)).\n    preserved_dims: dimensions of variable to not be folded in channels.\n\n  Returns:\n    An xarray.Variable with dimensions preserved_dims + (\"channels\",).\n  \"\"\"\n  stack_to_channels_dims = [\n      d for d in variable.dims if d not in preserved_dims]\n  if stack_to_channels_dims:\n    variable = variable.stack(channels=stack_to_channels_dims)\n  dims = {dim: variable.sizes.get(dim) or sizes[dim] for dim in preserved_dims}\n  dims[\"channels\"] = variable.sizes.get(\"channels\", 1)\n  return variable.set_dims(dims)\n\n\ndef dataset_to_stacked(\n    dataset: xarray.Dataset,\n    sizes: Optional[Mapping[str, int]] = None,\n    preserved_dims: Tuple[str, ...] = (\"batch\", \"lat\", \"lon\"),\n) -> xarray.DataArray:\n  \"\"\"Converts an xarray.Dataset to a single stacked array.\n\n  This takes each consistuent data_var, converts it into BHWC layout\n  using `variable_to_stacked`, then concats them all along the channels axis.\n\n  Args:\n    dataset: An xarray.Dataset.\n    sizes: Mapping including sizes for any dimensions which are not present in\n      the `dataset` but are needed for the output. See variable_to_stacked.\n    preserved_dims: dimensions from the dataset that should not be folded in\n      the predictions channels.\n\n  Returns:\n    An xarray.DataArray with dimensions preserved_dims + (\"channels\",).\n    Existing coordinates for preserved_dims axes will be preserved, however\n    there will be no coordinates for \"channels\".\n  \"\"\"\n  data_vars = [\n      variable_to_stacked(dataset.variables[name], sizes or dataset.sizes,\n                          preserved_dims)\n      for name in sorted(dataset.data_vars.keys())\n  ]\n  coords = {\n      dim: coord\n      for dim, coord in dataset.coords.items()\n      if dim in preserved_dims\n  }\n  return xarray.DataArray(\n      data=xarray.Variable.concat(data_vars, dim=\"channels\"), coords=coords)\n\n\ndef stacked_to_dataset(\n    stacked_array: xarray.Variable,\n    template_dataset: xarray.Dataset,\n    preserved_dims: Tuple[str, ...] = (\"batch\", \"lat\", \"lon\"),\n    ) -> xarray.Dataset:\n  \"\"\"The inverse of dataset_to_stacked.\n\n  Requires a template dataset to demonstrate the variables/shapes/coordinates\n  required.\n  All variables must have preserved_dims dimensions.\n\n  Args:\n    stacked_array: Data in BHWC layout, encoded the same as dataset_to_stacked\n      would if it was asked to encode `template_dataset`.\n    template_dataset: A template Dataset (or other mapping of DataArrays)\n      demonstrating the shape of output required (variables, shapes,\n      coordinates etc).\n    preserved_dims: dimensions from the target_template that were not folded in\n      the predictions channels. The preserved_dims need to be a subset of the\n      dims of all the variables of template_dataset.\n\n  Returns:\n    An xarray.Dataset (or other mapping of DataArrays) with the same shape and\n    type as template_dataset.\n  \"\"\"\n  unstack_from_channels_sizes = {}\n  var_names = sorted(template_dataset.keys())\n  for name in var_names:\n    template_var = template_dataset[name]\n    if not all(dim in template_var.dims for dim in preserved_dims):\n      raise ValueError(\n          f\"stacked_to_dataset requires all Variables to have {preserved_dims} \"\n          f\"dimensions, but found only {template_var.dims}.\")\n    unstack_from_channels_sizes[name] = {\n        dim: size for dim, size in template_var.sizes.items()\n        if dim not in preserved_dims}\n\n  channels = {name: np.prod(list(unstack_sizes.values()), dtype=np.int64)\n              for name, unstack_sizes in unstack_from_channels_sizes.items()}\n  total_expected_channels = sum(channels.values())\n  found_channels = stacked_array.sizes[\"channels\"]\n  if total_expected_channels != found_channels:\n    raise ValueError(\n        f\"Expected {total_expected_channels} channels but found \"\n        f\"{found_channels}, when trying to convert a stacked array of shape \"\n        f\"{stacked_array.sizes} to a dataset of shape {template_dataset}.\")\n\n  data_vars = {}\n  index = 0\n  for name in var_names:\n    template_var = template_dataset[name]\n    var = stacked_array.isel({\"channels\": slice(index, index + channels[name])})\n    index += channels[name]\n    var = var.unstack({\"channels\": unstack_from_channels_sizes[name]})\n    var = var.transpose(*template_var.dims)\n    data_vars[name] = xarray.DataArray(\n        data=var,\n        coords=template_var.coords,\n        # This might not always be the same as the name it's keyed under; it\n        # will refer to the original variable name, whereas the key might be\n        # some alias e.g. temperature_850 under which it should be logged:\n        name=template_var.name,\n    )\n  return type(template_dataset)(data_vars)  # pytype:disable=not-callable,wrong-arg-count\n",
    "15": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Wrappers for Predictors which allow them to work with normalized data.\n\nThe Predictor which is wrapped sees normalized inputs and targets, and makes\nnormalized predictions. The wrapper handles translating the predictions back\nto the original domain.\n\"\"\"\n\nimport logging\nfrom typing import Optional, Tuple\n\nfrom graphcast import predictor_base\nfrom graphcast import xarray_tree\nimport xarray\n\n\ndef normalize(values: xarray.Dataset,\n              scales: xarray.Dataset,\n              locations: Optional[xarray.Dataset],\n              ) -> xarray.Dataset:\n  \"\"\"Normalize variables using the given scales and (optionally) locations.\"\"\"\n  def normalize_array(array):\n    if array.name is None:\n      raise ValueError(\n          \"Can't look up normalization constants because array has no name.\")\n    if locations is not None:\n      if array.name in locations:\n        array = array - locations[array.name].astype(array.dtype)\n      else:\n        logging.warning('No normalization location found for %s', array.name)\n    if array.name in scales:\n      array = array / scales[array.name].astype(array.dtype)\n    else:\n      logging.warning('No normalization scale found for %s', array.name)\n    return array\n  return xarray_tree.map_structure(normalize_array, values)\n\n\ndef unnormalize(values: xarray.Dataset,\n                scales: xarray.Dataset,\n                locations: Optional[xarray.Dataset],\n                ) -> xarray.Dataset:\n  \"\"\"Unnormalize variables using the given scales and (optionally) locations.\"\"\"\n  def unnormalize_array(array):\n    if array.name is None:\n      raise ValueError(\n          \"Can't look up normalization constants because array has no name.\")\n    if array.name in scales:\n      array = array * scales[array.name].astype(array.dtype)\n    else:\n      logging.warning('No normalization scale found for %s', array.name)\n    if locations is not None:\n      if array.name in locations:\n        array = array + locations[array.name].astype(array.dtype)\n      else:\n        logging.warning('No normalization location found for %s', array.name)\n    return array\n  return xarray_tree.map_structure(unnormalize_array, values)\n\n\nclass InputsAndResiduals(predictor_base.Predictor):\n  \"\"\"Wraps with a residual connection, normalizing inputs and target residuals.\n\n  The inner predictor is given inputs that are normalized using `locations`\n  and `scales` to roughly zero-mean unit variance.\n\n  For target variables that are present in the inputs, the inner predictor is\n  trained to predict residuals (target - last_frame_of_input) that have been\n  normalized using `residual_scales` (and optionally `residual_locations`) to\n  roughly unit variance / zero mean.\n\n  This replaces `residual.Predictor` in the case where you want normalization\n  that's based on the scales of the residuals.\n\n  Since we return the underlying predictor's loss on the normalized residuals,\n  if the underlying predictor is a sum of per-variable losses, the normalization\n  will affect the relative weighting of the per-variable loss terms (hopefully\n  in a good way).\n\n  For target variables *not* present in the inputs, the inner predictor is\n  trained to predict targets directly, that have been normalized in the same\n  way as the inputs.\n\n  The transforms applied to the targets (the residual connection and the\n  normalization) are applied in reverse to the predictions before returning\n  them.\n  \"\"\"\n\n  def __init__(\n      self,\n      predictor: predictor_base.Predictor,\n      stddev_by_level: xarray.Dataset,\n      mean_by_level: xarray.Dataset,\n      diffs_stddev_by_level: xarray.Dataset):\n    self._predictor = predictor\n    self._scales = stddev_by_level\n    self._locations = mean_by_level\n    self._residual_scales = diffs_stddev_by_level\n    self._residual_locations = None\n\n  def _unnormalize_prediction_and_add_input(self, inputs, norm_prediction):\n    if norm_prediction.sizes.get('time') != 1:\n      raise ValueError(\n          'normalization.InputsAndResiduals only supports predicting a '\n          'single timestep.')\n    if norm_prediction.name in inputs:\n      # Residuals are assumed to be predicted as normalized (unit variance),\n      # but the scale and location they need mapping to is that of the residuals\n      # not of the values themselves.\n      prediction = unnormalize(\n          norm_prediction, self._residual_scales, self._residual_locations)\n      # A prediction for which we have a corresponding input -- we are\n      # predicting the residual:\n      last_input = inputs[norm_prediction.name].isel(time=-1)\n      prediction = prediction + last_input\n      return prediction\n    else:\n      # A predicted variable which is not an input variable. We are predicting\n      # it directly, so unnormalize it directly to the target scale/location:\n      return unnormalize(norm_prediction, self._scales, self._locations)\n\n  def _subtract_input_and_normalize_target(self, inputs, target):\n    if target.sizes.get('time') != 1:\n      raise ValueError(\n          'normalization.InputsAndResiduals only supports wrapping predictors'\n          'that predict a single timestep.')\n    if target.name in inputs:\n      target_residual = target\n      last_input = inputs[target.name].isel(time=-1)\n      target_residual = target_residual - last_input\n      return normalize(\n          target_residual, self._residual_scales, self._residual_locations)\n    else:\n      return normalize(target, self._scales, self._locations)\n\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               **kwargs\n               ) -> xarray.Dataset:\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_predictions = self._predictor(\n        norm_inputs, targets_template, forcings=norm_forcings, **kwargs)\n    return xarray_tree.map_structure(\n        lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred),\n        norm_predictions)\n\n  def loss(self,\n           inputs: xarray.Dataset,\n           targets: xarray.Dataset,\n           forcings: xarray.Dataset,\n           **kwargs,\n           ) -> predictor_base.LossAndDiagnostics:\n    \"\"\"Returns the loss computed on normalized inputs and targets.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(\n        lambda t: self._subtract_input_and_normalize_target(inputs, t),\n        targets)\n    return self._predictor.loss(\n        norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n\n  def loss_and_predictions(  # pytype: disable=signature-mismatch  # jax-ndarray\n      self,\n      inputs: xarray.Dataset,\n      targets: xarray.Dataset,\n      forcings: xarray.Dataset,\n      **kwargs,\n      ) -> Tuple[predictor_base.LossAndDiagnostics,\n                 xarray.Dataset]:\n    \"\"\"The loss computed on normalized data, with unnormalized predictions.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(\n        lambda t: self._subtract_input_and_normalize_target(inputs, t),\n        targets)\n    (loss, scalars), norm_predictions = self._predictor.loss_and_predictions(\n        norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n    predictions = xarray_tree.map_structure(\n        lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred),\n        norm_predictions)\n    return (loss, scalars), predictions\n",
    "16": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Abstract base classes for an xarray-based Predictor API.\"\"\"\n\nimport abc\n\nfrom typing import Tuple\n\nfrom graphcast import losses\nfrom graphcast import xarray_jax\nimport jax.numpy as jnp\nimport xarray\n\nLossAndDiagnostics = losses.LossAndDiagnostics\n\n\nclass Predictor(abc.ABC):\n  \"\"\"A possibly-trainable predictor of weather, exposing an xarray-based API.\n\n  Typically wraps an underlying JAX model and handles translating the xarray\n  Dataset values to and from plain JAX arrays that are convenient for input to\n  (and output from) the underlying model.\n\n  Different subclasses may exist to wrap different kinds of underlying model,\n  e.g. models taking stacked inputs/outputs, models taking separate 2D and 3D\n  inputs/outputs, autoregressive models.\n\n  You can also implement a specific model directly as a Predictor if you want,\n  for example if it has quite specific/unique requirements for its input/output\n  or loss function, or if it's convenient to implement directly using xarray.\n  \"\"\"\n\n  @abc.abstractmethod\n  def __call__(self,\n               inputs: xarray.Dataset,\n               targets_template: xarray.Dataset,\n               forcings: xarray.Dataset,\n               **optional_kwargs\n               ) -> xarray.Dataset:\n    \"\"\"Makes predictions.\n\n    This is only used by the Experiment for inference / evaluation, with\n    training going via the .loss method. So it should default to making\n    predictions for evaluation, although you can also support making predictions\n    for use in the loss via an is_training argument -- see\n    LossFunctionPredictor which helps with that.\n\n    Args:\n      inputs: An xarray.Dataset of inputs.\n      targets_template: An xarray.Dataset or other mapping of xarray.DataArrays,\n        with the same shape as the targets, to demonstrate what kind of\n        predictions are required. You can use this to determine which variables,\n        levels and lead times must be predicted.\n        You are free to raise an error if you don't support predicting what is\n        requested.\n      forcings: An xarray.Dataset of forcings terms. Forcings are variables\n        that can be fed to the model, but do not need to be predicted. This is\n        often because this variable can be computed analytically (e.g. the toa\n        radiation of the sun is mostly a function of geometry) or are considered\n        to be controlled for the experiment (e.g., impose a scenario of C02\n        emission into the atmosphere). Unlike `inputs`, the `forcings` can\n        include information \"from the future\", that is, information at target\n        times specified in the `targets_template`.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      Predictions, as an xarray.Dataset or other mapping of DataArrays which\n      is capable of being evaluated against targets with shape given by\n      targets_template.\n      For probabilistic predictors which can return multiple samples from a\n      predictive distribution, these should (by convention) be returned along\n      an additional 'sample' dimension.\n    \"\"\"\n\n  def loss(self,\n           inputs: xarray.Dataset,\n           targets: xarray.Dataset,\n           forcings: xarray.Dataset,\n           **optional_kwargs,\n           ) -> LossAndDiagnostics:\n    \"\"\"Computes a training loss, for predictors that are trainable.\n\n    Why make this the Predictor's responsibility, rather than letting callers\n    compute their own loss function using predictions obtained from\n    Predictor.__call__?\n\n    Doing it this way gives Predictors more control over their training setup.\n    For example, some predictors may wish to train using different targets to\n    the ones they predict at evaluation time -- perhaps different lead times and\n    variables, perhaps training to predict transformed versions of targets\n    where the transform needs to be inverted at evaluation time, etc.\n\n    It's also necessary for generative models (VAEs, GANs, ...) where the\n    training loss is more complex and isn't expressible as a parameter-free\n    function of predictions and targets.\n\n    Args:\n      inputs: An xarray.Dataset.\n      targets: An xarray.Dataset or other mapping of xarray.DataArrays. See\n        docs on __call__ for an explanation about the targets.\n      forcings: xarray.Dataset of forcing terms.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n        You need not include the loss itself in this dict; it will be added for\n        you.\n    \"\"\"\n    del targets, forcings, optional_kwargs\n    batch_size = inputs.sizes['batch']\n    dummy_loss = xarray_jax.DataArray(jnp.zeros(batch_size), dims=('batch',))\n    return dummy_loss, {}  # pytype: disable=bad-return-type\n\n  def loss_and_predictions(\n      self,\n      inputs: xarray.Dataset,\n      targets: xarray.Dataset,\n      forcings: xarray.Dataset,\n      **optional_kwargs,\n      ) -> Tuple[LossAndDiagnostics, xarray.Dataset]:\n    \"\"\"Like .loss but also returns corresponding predictions.\n\n    Implementing this is optional as it's not used directly by the Experiment,\n    but it is required by autoregressive.Predictor when applying an inner\n    Predictor autoregressively at training time; we need a loss at each step but\n    also predictions to feed back in for the next step.\n\n    Note the loss itself may not be directly regressing the predictions towards\n    targets, the loss may be computed in terms of transformed predictions and\n    targets (or in some other way). For this reason we can't always cleanly\n    separate this into step 1: get predictions, step 2: compute loss from them,\n    hence the need for this combined method.\n\n    Args:\n      inputs:\n      targets:\n      forcings:\n      **optional_kwargs:\n        As for self.loss.\n\n    Returns:\n      (loss, diagnostics)\n        As for self.loss\n      predictions:\n        The predictions which the loss relates to. These should be of the same\n        shape as what you would get from\n        `self.__call__(inputs, targets_template=targets)`, and should be in the\n        same 'domain' as the inputs (i.e. they shouldn't be transformed\n        differently to how the predictor expects its inputs).\n    \"\"\"\n    raise NotImplementedError\n",
    "17": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utils for rolling out models.\"\"\"\n\nfrom typing import Iterator\n\nfrom absl import logging\nimport chex\nimport dask.array\nfrom graphcast import xarray_tree\nimport jax\nimport numpy as np\nimport typing_extensions\nimport xarray\n\n\nclass PredictorFn(typing_extensions.Protocol):\n  \"\"\"Functional version of base.Predictor.__call__ with explicit rng.\"\"\"\n\n  def __call__(\n      self, rng: chex.PRNGKey, inputs: xarray.Dataset,\n      targets_template: xarray.Dataset,\n      forcings: xarray.Dataset,\n      **optional_kwargs,\n      ) -> xarray.Dataset:\n    ...\n\n\ndef chunked_prediction(\n    predictor_fn: PredictorFn,\n    rng: chex.PRNGKey,\n    inputs: xarray.Dataset,\n    targets_template: xarray.Dataset,\n    forcings: xarray.Dataset,\n    num_steps_per_chunk: int = 1,\n    verbose: bool = False,\n) -> xarray.Dataset:\n  \"\"\"Outputs a long trajectory by iteratively concatenating chunked predictions.\n\n  Args:\n    predictor_fn: Function to use to make predictions for each chunk.\n    rng: Random key.\n    inputs: Inputs for the model.\n    targets_template: Template for the target prediction, requires targets\n        equispaced in time.\n    forcings: Optional forcing for the model.\n    num_steps_per_chunk: How many of the steps in `targets_template` to predict\n        at each call of `predictor_fn`. It must evenly divide the number of\n        steps in `targets_template`.\n    verbose: Whether to log the current chunk being predicted.\n\n  Returns:\n    Predictions for the targets template.\n\n  \"\"\"\n  chunks_list = []\n  for prediction_chunk in chunked_prediction_generator(\n      predictor_fn=predictor_fn,\n      rng=rng,\n      inputs=inputs,\n      targets_template=targets_template,\n      forcings=forcings,\n      num_steps_per_chunk=num_steps_per_chunk,\n      verbose=verbose):\n    chunks_list.append(jax.device_get(prediction_chunk))\n  return xarray.concat(chunks_list, dim=\"time\")\n\n\ndef chunked_prediction_generator(\n    predictor_fn: PredictorFn,\n    rng: chex.PRNGKey,\n    inputs: xarray.Dataset,\n    targets_template: xarray.Dataset,\n    forcings: xarray.Dataset,\n    num_steps_per_chunk: int = 1,\n    verbose: bool = False,\n) -> Iterator[xarray.Dataset]:\n  \"\"\"Outputs a long trajectory by yielding chunked predictions.\n\n  Args:\n    predictor_fn: Function to use to make predictions for each chunk.\n    rng: Random key.\n    inputs: Inputs for the model.\n    targets_template: Template for the target prediction, requires targets\n        equispaced in time.\n    forcings: Optional forcing for the model.\n    num_steps_per_chunk: How many of the steps in `targets_template` to predict\n        at each call of `predictor_fn`. It must evenly divide the number of\n        steps in `targets_template`.\n    verbose: Whether to log the current chunk being predicted.\n\n  Yields:\n    The predictions for each chunked step of the chunked rollout, such as\n    if all predictions are concatenated in time this would match the targets\n    template in structure.\n\n  \"\"\"\n\n  # Create copies to avoid mutating inputs.\n  inputs = xarray.Dataset(inputs)\n  targets_template = xarray.Dataset(targets_template)\n  forcings = xarray.Dataset(forcings)\n\n  if \"datetime\" in inputs.coords:\n    del inputs.coords[\"datetime\"]\n\n  if \"datetime\" in targets_template.coords:\n    output_datetime = targets_template.coords[\"datetime\"]\n    del targets_template.coords[\"datetime\"]\n  else:\n    output_datetime = None\n\n  if \"datetime\" in forcings.coords:\n    del forcings.coords[\"datetime\"]\n\n  num_target_steps = targets_template.dims[\"time\"]\n  num_chunks, remainder = divmod(num_target_steps, num_steps_per_chunk)\n  if remainder != 0:\n    raise ValueError(\n        f\"The number of steps per chunk {num_steps_per_chunk} must \"\n        f\"evenly divide the number of target steps {num_target_steps} \")\n\n  if len(np.unique(np.diff(targets_template.coords[\"time\"].data))) > 1:\n    raise ValueError(\"The targets time coordinates must be evenly spaced\")\n\n  # Our template targets will always have a time axis corresponding for the\n  # timedeltas for the first chunk.\n  targets_chunk_time = targets_template.time.isel(\n      time=slice(0, num_steps_per_chunk))\n\n  current_inputs = inputs\n  for chunk_index in range(num_chunks):\n    if verbose:\n      logging.info(\"Chunk %d/%d\", chunk_index, num_chunks)\n      logging.flush()\n\n    # Select targets for the time period that we are predicting for this chunk.\n    target_offset = num_steps_per_chunk * chunk_index\n    target_slice = slice(target_offset, target_offset + num_steps_per_chunk)\n    current_targets_template = targets_template.isel(time=target_slice)\n\n    # Replace the timedelta, by the one corresponding to the first chunk, so we\n    # don't recompile at every iteration, keeping the\n    actual_target_time = current_targets_template.coords[\"time\"]\n    current_targets_template = current_targets_template.assign_coords(\n        time=targets_chunk_time).compute()\n\n    current_forcings = forcings.isel(time=target_slice)\n    current_forcings = current_forcings.assign_coords(time=targets_chunk_time)\n    current_forcings = current_forcings.compute()\n    # Make predictions for the chunk.\n    rng, this_rng = jax.random.split(rng)\n    predictions = predictor_fn(\n        rng=this_rng,\n        inputs=current_inputs,\n        targets_template=current_targets_template,\n        forcings=current_forcings)\n\n    next_frame = xarray.merge([predictions, current_forcings])\n\n    next_inputs = _get_next_inputs(current_inputs, next_frame)\n\n    # Shift timedelta coordinates, so we don't recompile at every iteration.\n    next_inputs = next_inputs.assign_coords(time=current_inputs.coords[\"time\"])\n    current_inputs = next_inputs\n\n    # At this point we can assign the actual targets time coordinates.\n    predictions = predictions.assign_coords(time=actual_target_time)\n    if output_datetime is not None:\n      predictions.coords[\"datetime\"] = output_datetime.isel(\n          time=target_slice)\n    yield predictions\n    del predictions\n\n\ndef _get_next_inputs(\n    prev_inputs: xarray.Dataset, next_frame: xarray.Dataset,\n    ) -> xarray.Dataset:\n  \"\"\"Computes next inputs, from previous inputs and predictions.\"\"\"\n\n  # Make sure are are predicting all inputs with a time axis.\n  non_predicted_or_forced_inputs = list(\n      set(prev_inputs.keys()) - set(next_frame.keys()))\n  if \"time\" in prev_inputs[non_predicted_or_forced_inputs].dims:\n    raise ValueError(\n        \"Found an input with a time index that is not predicted or forced.\")\n\n  # Keys we need to copy from predictions to inputs.\n  next_inputs_keys = list(\n      set(next_frame.keys()).intersection(set(prev_inputs.keys())))\n  next_inputs = next_frame[next_inputs_keys]\n\n  # Apply concatenate next frame with inputs, crop what we don't need.\n  num_inputs = prev_inputs.dims[\"time\"]\n  return (\n      xarray.concat(\n          [prev_inputs, next_inputs], dim=\"time\", data_vars=\"different\")\n      .tail(time=num_inputs))\n\n\ndef extend_targets_template(\n    targets_template: xarray.Dataset,\n    required_num_steps: int) -> xarray.Dataset:\n  \"\"\"Extends `targets_template` to `required_num_steps` with lazy arrays.\n\n  It uses lazy dask arrays of zeros, so it does not require instantiating the\n  array in memory.\n\n  Args:\n    targets_template: Input template to extend.\n    required_num_steps: Number of steps required in the returned template.\n\n  Returns:\n    `xarray.Dataset` identical in variables and timestep to `targets_template`\n    full of `dask.array.zeros` such that the time axis has `required_num_steps`.\n\n  \"\"\"\n\n  # Extend the \"time\" and \"datetime\" coordinates\n  time = targets_template.coords[\"time\"]\n\n  # Assert the first target time corresponds to the timestep.\n  timestep = time[0].data\n  if time.shape[0] > 1:\n    assert np.all(timestep == time[1:] - time[:-1])\n\n  extended_time = (np.arange(required_num_steps) + 1) * timestep\n\n  if \"datetime\" in targets_template.coords:\n    datetime = targets_template.coords[\"datetime\"]\n    extended_datetime = (datetime[0].data - timestep) + extended_time\n  else:\n    extended_datetime = None\n\n  # Replace the values with empty dask arrays extending the time coordinates.\n  datetime = targets_template.coords[\"time\"]\n\n  def extend_time(data_array: xarray.DataArray) -> xarray.DataArray:\n    dims = data_array.dims\n    shape = list(data_array.shape)\n    shape[dims.index(\"time\")] = required_num_steps\n    dask_data = dask.array.zeros(\n        shape=tuple(shape),\n        chunks=-1,  # Will give chunk info directly to `ChunksToZarr``.\n        dtype=data_array.dtype)\n\n    coords = dict(data_array.coords)\n    coords[\"time\"] = extended_time\n\n    if extended_datetime is not None:\n      coords[\"datetime\"] = (\"time\", extended_datetime)\n\n    return xarray.DataArray(\n        dims=dims,\n        data=dask_data,\n        coords=coords)\n\n  return xarray_tree.map_structure(extend_time, targets_template)\n",
    "19": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nimport timeit\nfrom typing import Sequence\n\nfrom absl import logging\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom graphcast import solar_radiation\nimport numpy as np\nimport pandas as pd\nimport xarray as xa\n\n\ndef _get_grid_lat_lon_coords(\n    num_lat: int, num_lon: int\n) -> tuple[np.ndarray, np.ndarray]:\n  \"\"\"Generates a linear latitude-longitude grid of the given size.\n\n  Args:\n    num_lat: Size of the latitude dimension of the grid.\n    num_lon: Size of the longitude dimension of the grid.\n\n  Returns:\n    A tuple `(lat, lon)` containing 1D arrays with the latitude and longitude\n    coordinates in degrees of the generated grid.\n  \"\"\"\n  lat = np.linspace(-90.0, 90.0, num=num_lat, endpoint=True)\n  lon = np.linspace(0.0, 360.0, num=num_lon, endpoint=False)\n  return lat, lon\n\n\nclass SolarRadiationTest(parameterized.TestCase):\n\n  def setUp(self):\n    super().setUp()\n    np.random.seed(0)\n\n  def test_missing_dim_raises_value_error(self):\n    data = xa.DataArray(\n        np.random.randn(2, 2),\n        coords=[np.array([0.1, 0.2]), np.array([0.0, 0.5])],\n        dims=[\"lon\", \"x\"],\n    )\n    with self.assertRaisesRegex(\n        ValueError, r\".* dimensions are missing in `data_array_like`.\"\n    ):\n      solar_radiation.get_toa_incident_solar_radiation_for_xarray(\n          data, integration_period=\"1h\", num_integration_bins=360\n      )\n\n  def test_missing_coordinate_raises_value_error(self):\n    data = xa.Dataset(\n        data_vars={\"var1\": ([\"x\", \"lat\", \"lon\"], np.random.randn(2, 3, 2))},\n        coords={\n            \"lat\": np.array([0.0, 0.1, 0.2]),\n            \"lon\": np.array([0.0, 0.5]),\n        },\n    )\n    with self.assertRaisesRegex(\n        ValueError, r\".* coordinates are missing in `data_array_like`.\"\n    ):\n      solar_radiation.get_toa_incident_solar_radiation_for_xarray(\n          data, integration_period=\"1h\", num_integration_bins=360\n      )\n\n  def test_shape_multiple_timestamps(self):\n    data = xa.Dataset(\n        data_vars={\"var1\": ([\"time\", \"lat\", \"lon\"], np.random.randn(2, 4, 2))},\n        coords={\n            \"lat\": np.array([0.0, 0.1, 0.2, 0.3]),\n            \"lon\": np.array([0.0, 0.5]),\n            \"time\": np.array([100, 200], dtype=\"timedelta64[s]\"),\n            \"datetime\": xa.Variable(\n                \"time\", np.array([10, 20], dtype=\"datetime64[D]\")\n            ),\n        },\n    )\n\n    actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(\n        data, integration_period=\"1h\", num_integration_bins=2\n    )\n\n    self.assertEqual((\"time\", \"lat\", \"lon\"), actual.dims)\n    self.assertEqual((2, 4, 2), actual.shape)\n\n  def test_shape_single_timestamp(self):\n    data = xa.Dataset(\n        data_vars={\"var1\": ([\"lat\", \"lon\"], np.random.randn(4, 2))},\n        coords={\n            \"lat\": np.array([0.0, 0.1, 0.2, 0.3]),\n            \"lon\": np.array([0.0, 0.5]),\n            \"datetime\": np.datetime64(10, \"D\"),\n        },\n    )\n\n    actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(\n        data, integration_period=\"1h\", num_integration_bins=2\n    )\n\n    self.assertEqual((\"lat\", \"lon\"), actual.dims)\n    self.assertEqual((4, 2), actual.shape)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"one_timestamp_jitted\",\n          periods=1,\n          repeats=3,\n          use_jit=True,\n      ),\n      dict(\n          testcase_name=\"one_timestamp_non_jitted\",\n          periods=1,\n          repeats=3,\n          use_jit=False,\n      ),\n      dict(\n          testcase_name=\"ten_timestamps_non_jitted\",\n          periods=10,\n          repeats=1,\n          use_jit=False,\n      ),\n  )\n  def test_full_spatial_resolution(\n      self, periods: int, repeats: int, use_jit: bool\n  ):\n    timestamps = pd.date_range(start=\"2023-09-25\", periods=periods, freq=\"6h\")\n    # Generate a linear grid with 0.25 degrees resolution similar to ERA5.\n    lat, lon = _get_grid_lat_lon_coords(num_lat=721, num_lon=1440)\n\n    def benchmark() -> None:\n      solar_radiation.get_toa_incident_solar_radiation(\n          timestamps,\n          lat,\n          lon,\n          integration_period=\"1h\",\n          num_integration_bins=360,\n          use_jit=use_jit,\n      ).block_until_ready()\n\n    results = timeit.repeat(benchmark, repeat=repeats, number=1)\n\n    logging.info(\n        \"Times to compute `tisr` for input of shape `%d, %d, %d` (seconds): %s\",\n        len(timestamps),\n        len(lat),\n        len(lon),\n        np.array2string(np.array(results), precision=1),\n    )\n\n\nclass GetTsiTest(parameterized.TestCase):\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"reference_tsi_data\",\n          loader=solar_radiation.reference_tsi_data,\n          expected_tsi=np.array([1361.0]),\n      ),\n      dict(\n          testcase_name=\"era5_tsi_data\",\n          loader=solar_radiation.era5_tsi_data,\n          expected_tsi=np.array([1360.9440]),  # 0.9965 * 1365.7240\n      ),\n  )\n  def test_mid_2020_lookup(\n      self, loader: solar_radiation.TsiDataLoader, expected_tsi: np.ndarray\n  ):\n    tsi_data = loader()\n\n    tsi = solar_radiation.get_tsi(\n        [np.datetime64(\"2020-07-02T00:00:00\")], tsi_data\n    )\n\n    np.testing.assert_allclose(expected_tsi, tsi)\n\n  @parameterized.named_parameters(\n      dict(\n          testcase_name=\"beginning_2020_left_boundary\",\n          timestamps=[np.datetime64(\"2020-01-01T00:00:00\")],\n          expected_tsi=np.array([1000.0]),\n      ),\n      dict(\n          testcase_name=\"mid_2020_exact\",\n          timestamps=[np.datetime64(\"2020-07-02T00:00:00\")],\n          expected_tsi=np.array([1000.0]),\n      ),\n      dict(\n          testcase_name=\"beginning_2021_interpolated\",\n          timestamps=[np.datetime64(\"2021-01-01T00:00:00\")],\n          expected_tsi=np.array([1150.0]),\n      ),\n      dict(\n          testcase_name=\"mid_2021_lookup\",\n          timestamps=[np.datetime64(\"2021-07-02T12:00:00\")],\n          expected_tsi=np.array([1300.0]),\n      ),\n      dict(\n          testcase_name=\"beginning_2022_interpolated\",\n          timestamps=[np.datetime64(\"2022-01-01T00:00:00\")],\n          expected_tsi=np.array([1250.0]),\n      ),\n      dict(\n          testcase_name=\"mid_2022_lookup\",\n          timestamps=[np.datetime64(\"2022-07-02T12:00:00\")],\n          expected_tsi=np.array([1200.0]),\n      ),\n      dict(\n          testcase_name=\"beginning_2023_right_boundary\",\n          timestamps=[np.datetime64(\"2023-01-01T00:00:00\")],\n          expected_tsi=np.array([1200.0]),\n      ),\n  )\n  def test_interpolation(\n      self, timestamps: Sequence[np.datetime64], expected_tsi: np.ndarray\n  ):\n    tsi_data = xa.DataArray(\n        np.array([1000.0, 1300.0, 1200.0]),\n        dims=[\"time\"],\n        coords={\"time\": np.array([2020.5, 2021.5, 2022.5])},\n    )\n\n    tsi = solar_radiation.get_tsi(timestamps, tsi_data)\n\n    np.testing.assert_allclose(expected_tsi, tsi)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "20": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Data-structure for storing graphs with typed edges and nodes.\"\"\"\n\nfrom typing import NamedTuple, Any, Union, Tuple, Mapping, TypeVar\n\nArrayLike = Union[Any]  # np.ndarray, jnp.ndarray, tf.tensor\nArrayLikeTree = Union[Any, ArrayLike]  # Nest of ArrayLike\n\n_T = TypeVar('_T')\n\n\n# All tensors have a \"flat_batch_axis\", which is similar to the leading\n# axes of graph_tuples:\n# * In the case of nodes this is simply a shared node and flat batch axis, with\n# size corresponding to the total number of nodes in the flattened batch.\n# * In the case of edges this is simply a shared edge and flat batch axis, with\n# size corresponding to the total number of edges in the flattened batch.\n# * In the case of globals this is simply the number of graphs in the flattened\n# batch.\n\n# All shapes may also have any additional leading shape \"batch_shape\".\n# Options for building batches are:\n# * Use a provided \"flatten\" method that takes a leading `batch_shape` and\n#   it into the flat_batch_axis (this will be useful when using `tf.Dataset`\n#   which supports batching into RaggedTensors, with leading batch shape even\n#   if graphs have different numbers of nodes and edges), so the RaggedBatches\n#   can then be converted into something without ragged dimensions that jax can\n#   use.\n# * Directly build a \"flat batch\" using a provided function for batching a list\n#   of graphs (how it is done in `jraph`).\n\n\nclass NodeSet(NamedTuple):\n  \"\"\"Represents a set of nodes.\"\"\"\n  n_node: ArrayLike  # [num_flat_graphs]\n  features: ArrayLikeTree  # Prev. `nodes`: [num_flat_nodes] + feature_shape\n\n\nclass EdgesIndices(NamedTuple):\n  \"\"\"Represents indices to nodes adjacent to the edges.\"\"\"\n  senders: ArrayLike  # [num_flat_edges]\n  receivers: ArrayLike  # [num_flat_edges]\n\n\nclass EdgeSet(NamedTuple):\n  \"\"\"Represents a set of edges.\"\"\"\n  n_edge: ArrayLike  # [num_flat_graphs]\n  indices: EdgesIndices\n  features: ArrayLikeTree  # Prev. `edges`: [num_flat_edges] + feature_shape\n\n\nclass Context(NamedTuple):\n  # `n_graph` always contains ones but it is useful to query the leading shape\n  # in case of graphs without any nodes or edges sets.\n  n_graph: ArrayLike  # [num_flat_graphs]\n  features: ArrayLikeTree  # Prev. `globals`: [num_flat_graphs] + feature_shape\n\n\nclass EdgeSetKey(NamedTuple):\n  name: str   # Name of the EdgeSet.\n\n  # Sender node set name and receiver node set name connected by the edge set.\n  node_sets: Tuple[str, str]\n\n\nclass TypedGraph(NamedTuple):\n  \"\"\"A graph with typed nodes and edges.\n\n  A typed graph is made of a context, multiple sets of nodes and multiple\n  sets of edges connecting those nodes (as indicated by the EdgeSetKey).\n  \"\"\"\n\n  context: Context\n  nodes: Mapping[str, NodeSet]\n  edges: Mapping[EdgeSetKey, EdgeSet]\n\n  def edge_key_by_name(self, name: str) -> EdgeSetKey:\n    found_key = [k for k in self.edges.keys() if k.name == name]\n    if len(found_key) != 1:\n      raise KeyError(\"invalid edge key '{}'. Available edges: [{}]\".format(\n          name, ', '.join(x.name for x in self.edges.keys())))\n    return found_key[0]\n\n  def edge_by_name(self, name: str) -> EdgeSet:\n    return self.edges[self.edge_key_by_name(name)]\n",
    "21": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"A library of typed Graph Neural Networks.\"\"\"\n\nfrom typing import Callable, Mapping, Optional, Union\n\nfrom graphcast import typed_graph\nimport jax.numpy as jnp\nimport jax.tree_util as tree\nimport jraph\n\n\n# All features will be an ArrayTree.\nNodeFeatures = EdgeFeatures = SenderFeatures = ReceiverFeatures = Globals = (\n    jraph.ArrayTree)\n\n# Signature:\n# (node features, outgoing edge features, incoming edge features,\n#  globals) -> updated node features\nGNUpdateNodeFn = Callable[\n    [NodeFeatures, Mapping[str, SenderFeatures], Mapping[str, ReceiverFeatures],\n     Globals],\n    NodeFeatures]\n\nGNUpdateGlobalFn = Callable[\n    [Mapping[str, NodeFeatures], Mapping[str, EdgeFeatures], Globals],\n    Globals]\n\n\ndef GraphNetwork(  # pylint: disable=invalid-name\n    update_edge_fn: Mapping[str, jraph.GNUpdateEdgeFn],\n    update_node_fn: Mapping[str, GNUpdateNodeFn],\n    update_global_fn: Optional[GNUpdateGlobalFn] = None,\n    aggregate_edges_for_nodes_fn: jraph.AggregateEdgesToNodesFn = jraph\n    .segment_sum,\n    aggregate_nodes_for_globals_fn: jraph.AggregateNodesToGlobalsFn = jraph\n    .segment_sum,\n    aggregate_edges_for_globals_fn: jraph.AggregateEdgesToGlobalsFn = jraph\n    .segment_sum,\n    ):\n  \"\"\"Returns a method that applies a configured GraphNetwork.\n\n  This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n  extended to Typed Graphs with multiple edge sets and node sets and extended to\n  allow aggregating not only edges received by the nodes, but also edges sent by\n  the nodes.\n\n  Example usage::\n\n    gn = GraphNetwork(update_edge_function,\n    update_node_function, **kwargs)\n    # Conduct multiple rounds of message passing with the same parameters:\n    for _ in range(num_message_passing_steps):\n      graph = gn(graph)\n\n  Args:\n    update_edge_fn: mapping of functions used to update a subset of the edge\n      types, indexed by edge type name.\n    update_node_fn: mapping of functions used to update a subset of the node\n      types, indexed by node type name.\n    update_global_fn: function used to update the globals or None to deactivate\n      globals updates.\n    aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n      node.\n    aggregate_nodes_for_globals_fn: function used to aggregate the nodes for the\n      globals.\n    aggregate_edges_for_globals_fn: function used to aggregate the edges for the\n      globals.\n\n  Returns:\n    A method that applies the configured GraphNetwork.\n  \"\"\"\n\n  def _apply_graph_net(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Applies a configured GraphNetwork to a graph.\n\n    This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n    extended to Typed Graphs with multiple edge sets and node sets and extended\n    to allow aggregating not only edges received by the nodes, but also edges\n    sent by the nodes.\n\n    Args:\n      graph: a `TypedGraph` containing the graph.\n\n    Returns:\n      Updated `TypedGraph`.\n    \"\"\"\n\n    updated_graph = graph\n\n    # Edge update.\n    updated_edges = dict(updated_graph.edges)\n    for edge_set_name, edge_fn in update_edge_fn.items():\n      edge_set_key = graph.edge_key_by_name(edge_set_name)\n      updated_edges[edge_set_key] = _edge_update(\n          updated_graph, edge_fn, edge_set_key)\n    updated_graph = updated_graph._replace(edges=updated_edges)\n\n    # Node update.\n    updated_nodes = dict(updated_graph.nodes)\n    for node_set_key, node_fn in update_node_fn.items():\n      updated_nodes[node_set_key] = _node_update(\n          updated_graph, node_fn, node_set_key, aggregate_edges_for_nodes_fn)\n    updated_graph = updated_graph._replace(nodes=updated_nodes)\n\n    # Global update.\n    if update_global_fn:\n      updated_context = _global_update(\n          updated_graph, update_global_fn,\n          aggregate_edges_for_globals_fn,\n          aggregate_nodes_for_globals_fn)\n      updated_graph = updated_graph._replace(context=updated_context)\n\n    return updated_graph\n\n  return _apply_graph_net\n\n\ndef _edge_update(graph, edge_fn, edge_set_key):  # pylint: disable=invalid-name\n  \"\"\"Updates an edge set of a given key.\"\"\"\n\n  sender_nodes = graph.nodes[edge_set_key.node_sets[0]]\n  receiver_nodes = graph.nodes[edge_set_key.node_sets[1]]\n  edge_set = graph.edges[edge_set_key]\n  senders = edge_set.indices.senders  # pytype: disable=attribute-error\n  receivers = edge_set.indices.receivers  # pytype: disable=attribute-error\n\n  sent_attributes = tree.tree_map(\n      lambda n: n[senders], sender_nodes.features)\n  received_attributes = tree.tree_map(\n      lambda n: n[receivers], receiver_nodes.features)\n\n  n_edge = edge_set.n_edge\n  sum_n_edge = senders.shape[0]\n  global_features = tree.tree_map(\n      lambda g: jnp.repeat(g, n_edge, axis=0, total_repeat_length=sum_n_edge),\n      graph.context.features)\n  new_features = edge_fn(\n      edge_set.features, sent_attributes, received_attributes,\n      global_features)\n  return edge_set._replace(features=new_features)\n\n\ndef _node_update(graph, node_fn, node_set_key, aggregation_fn):  # pylint: disable=invalid-name\n  \"\"\"Updates an edge set of a given key.\"\"\"\n  node_set = graph.nodes[node_set_key]\n  sum_n_node = tree.tree_leaves(node_set.features)[0].shape[0]\n\n  sent_features = {}\n  for edge_set_key, edge_set in graph.edges.items():\n    sender_node_set_key = edge_set_key.node_sets[0]\n    if sender_node_set_key == node_set_key:\n      assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n      senders = edge_set.indices.senders\n      sent_features[edge_set_key.name] = tree.tree_map(\n          lambda e: aggregation_fn(e, senders, sum_n_node), edge_set.features)  # pylint: disable=cell-var-from-loop\n\n  received_features = {}\n  for edge_set_key, edge_set in graph.edges.items():\n    receiver_node_set_key = edge_set_key.node_sets[1]\n    if receiver_node_set_key == node_set_key:\n      assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n      receivers = edge_set.indices.receivers\n      received_features[edge_set_key.name] = tree.tree_map(\n          lambda e: aggregation_fn(e, receivers, sum_n_node), edge_set.features)  # pylint: disable=cell-var-from-loop\n\n  n_node = node_set.n_node\n  global_features = tree.tree_map(\n      lambda g: jnp.repeat(g, n_node, axis=0, total_repeat_length=sum_n_node),\n      graph.context.features)\n  new_features = node_fn(\n      node_set.features, sent_features, received_features, global_features)\n  return node_set._replace(features=new_features)\n\n\ndef _global_update(graph, global_fn, edge_aggregation_fn, node_aggregation_fn):  # pylint: disable=invalid-name\n  \"\"\"Updates an edge set of a given key.\"\"\"\n  n_graph = graph.context.n_graph.shape[0]\n  graph_idx = jnp.arange(n_graph)\n\n  edge_features = {}\n  for edge_set_key, edge_set in graph.edges.items():\n    assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n    sum_n_edge = edge_set.indices.senders.shape[0]\n    edge_gr_idx = jnp.repeat(\n        graph_idx, edge_set.n_edge, axis=0, total_repeat_length=sum_n_edge)\n    edge_features[edge_set_key.name] = tree.tree_map(\n        lambda e: edge_aggregation_fn(e, edge_gr_idx, n_graph),   # pylint: disable=cell-var-from-loop\n        edge_set.features)\n\n  node_features = {}\n  for node_set_key, node_set in graph.nodes.items():\n    sum_n_node = tree.tree_leaves(node_set.features)[0].shape[0]\n    node_gr_idx = jnp.repeat(\n        graph_idx, node_set.n_node, axis=0, total_repeat_length=sum_n_node)\n    node_features[node_set_key] = tree.tree_map(\n        lambda n: node_aggregation_fn(n, node_gr_idx, n_graph),   # pylint: disable=cell-var-from-loop\n        node_set.features)\n\n  new_features = global_fn(node_features, edge_features, graph.context.features)\n  return graph.context._replace(features=new_features)\n\n\nInteractionUpdateNodeFn = Callable[\n    [jraph.NodeFeatures,\n     Mapping[str, SenderFeatures],\n     Mapping[str, ReceiverFeatures]],\n    jraph.NodeFeatures]\n\n\nInteractionUpdateNodeFnNoSentEdges = Callable[\n    [jraph.NodeFeatures,\n     Mapping[str, ReceiverFeatures]],\n    jraph.NodeFeatures]\n\n\ndef InteractionNetwork(  # pylint: disable=invalid-name\n    update_edge_fn: Mapping[str, jraph.InteractionUpdateEdgeFn],\n    update_node_fn: Mapping[str, Union[InteractionUpdateNodeFn,\n                                       InteractionUpdateNodeFnNoSentEdges]],\n    aggregate_edges_for_nodes_fn: jraph.AggregateEdgesToNodesFn = jraph\n    .segment_sum,\n    include_sent_messages_in_node_update: bool = False):\n  \"\"\"Returns a method that applies a configured InteractionNetwork.\n\n  An interaction network computes interactions on the edges based on the\n  previous edges features, and on the features of the nodes sending into those\n  edges. It then updates the nodes based on the incoming updated edges.\n  See https://arxiv.org/abs/1612.00222 for more details.\n\n  This implementation extends the behavior to `TypedGraphs` adding an option\n  to include edge features for which a node is a sender in the arguments to\n  the node update function.\n\n  Args:\n    update_edge_fn: mapping of functions used to update a subset of the edge\n      types, indexed by edge type name.\n    update_node_fn: mapping of functions used to update a subset of the node\n      types, indexed by node type name.\n    aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n      node.\n    include_sent_messages_in_node_update: pass edge features for which a node is\n      a sender to the node update function.\n  \"\"\"\n  # An InteractionNetwork is a GraphNetwork without globals features,\n  # so we implement the InteractionNetwork as a configured GraphNetwork.\n\n  # An InteractionNetwork edge function does not have global feature inputs,\n  # so we filter the passed global argument in the GraphNetwork.\n  wrapped_update_edge_fn = tree.tree_map(\n      lambda fn: lambda e, s, r, g: fn(e, s, r), update_edge_fn)\n\n  # Similarly, we wrap the update_node_fn to ensure only the expected\n  # arguments are passed to the Interaction net.\n  if include_sent_messages_in_node_update:\n    wrapped_update_node_fn = tree.tree_map(\n        lambda fn: lambda n, s, r, g: fn(n, s, r), update_node_fn)\n  else:\n    wrapped_update_node_fn = tree.tree_map(\n        lambda fn: lambda n, s, r, g: fn(n, r), update_node_fn)\n  return GraphNetwork(\n      update_edge_fn=wrapped_update_edge_fn,\n      update_node_fn=wrapped_update_node_fn,\n      aggregate_edges_for_nodes_fn=aggregate_edges_for_nodes_fn)\n\n\ndef GraphMapFeatures(  # pylint: disable=invalid-name\n    embed_edge_fn: Optional[Mapping[str, jraph.EmbedEdgeFn]] = None,\n    embed_node_fn: Optional[Mapping[str, jraph.EmbedNodeFn]] = None,\n    embed_global_fn: Optional[jraph.EmbedGlobalFn] = None):\n  \"\"\"Returns function which embeds the components of a graph independently.\n\n  Args:\n    embed_edge_fn: mapping of functions used to embed each edge type,\n      indexed by edge type name.\n    embed_node_fn: mapping of functions used to embed each node type,\n      indexed by node type name.\n    embed_global_fn: function used to embed the globals.\n  \"\"\"\n\n  def _embed(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n\n    updated_edges = dict(graph.edges)\n    if embed_edge_fn:\n      for edge_set_name, embed_fn in embed_edge_fn.items():\n        edge_set_key = graph.edge_key_by_name(edge_set_name)\n        edge_set = graph.edges[edge_set_key]\n        updated_edges[edge_set_key] = edge_set._replace(\n            features=embed_fn(edge_set.features))\n\n    updated_nodes = dict(graph.nodes)\n    if embed_node_fn:\n      for node_set_key, embed_fn in embed_node_fn.items():\n        node_set = graph.nodes[node_set_key]\n        updated_nodes[node_set_key] = node_set._replace(\n            features=embed_fn(node_set.features))\n\n    updated_context = graph.context\n    if embed_global_fn:\n      updated_context = updated_context._replace(\n          features=embed_global_fn(updated_context.features))\n\n    return graph._replace(edges=updated_edges, nodes=updated_nodes,\n                          context=updated_context)\n\n  return _embed\n",
    "22": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Helpers to use xarray.{Variable,DataArray,Dataset} with JAX.\n\nAllows them to be based on JAX arrays without converting to numpy arrays under\nthe hood, so you can start with a JAX array, do some computation with it in\nxarray-land, get a JAX array out the other end and (for example) jax.jit\nthrough the whole thing. You can even jax.jit a function which accepts and\nreturns xarray.Dataset, DataArray and Variable.\n\n## Creating xarray datatypes from jax arrays, and vice-versa.\n\nYou can use the xarray_jax.{Variable,DataArray,Dataset} constructors, which have\nthe same API as the standard xarray constructors but will accept JAX arrays\nwithout converting them to numpy.\n\nIt does this by wrapping the JAX array in a wrapper before passing it to\nxarray; you can also do this manually by calling xarray_jax.wrap on your JAX\narrays before passing them to the standard xarray constructors.\n\nTo get non-wrapped JAX arrays out the other end, you can use e.g.:\n\n  xarray_jax.jax_vars(dataset)\n  xarray_jax.jax_data(dataset.some_var)\n\nwhich will complain if the data isn't actually a JAX array. Use this if you need\nto make sure the computation has gone via JAX, e.g. if it's the output of code\nthat you want to JIT or compute gradients through. If this is not the case and\nyou want to support passing plain numpy arrays through as well as potentially\nJAX arrays, you can use:\n\n  xarray_jax.unwrap_vars(dataset)\n  xarray_jax.unwrap_data(dataset.some_var)\n\nwhich will unwrap the data if it is a wrapped JAX array, but otherwise pass\nit through to you without complaint.\n\nThe wrapped JAX arrays aim to support all the core operations from the numpy\narray API that xarray expects, however there may still be some gaps; if you run\ninto any problems around this, you may need to add a few more proxy methods onto\nthe wrapper class below.\n\nIn future once JAX and xarray support the new  Python array API standard\n(https://data-apis.org/array-api/latest/index.html), we hope to avoid the need\nfor wrapping the JAX arrays like this.\n\n## jax.jit and pmap of functions taking and returning xarray datatypes\n\nWe register xarray datatypes with jax.tree_util, which allows them to be treated\nas generic containers of jax arrays by various parts of jax including jax.jit.\n\nThis allows for, e.g.:\n\n  @jax.jit\n  def foo(input: xarray.Dataset) -> xarray.Dataset:\n    ...\n\nIt will not work out-of-the-box with shape-modifying transformations like\njax.pmap, or e.g. a jax.tree_util.tree_map with some transform that alters array\nshapes or dimension order. That's because we won't know what dimension names\nand/or coordinates to use when unflattening, if the results have a different\nshape to the data that was originally flattened.\n\nYou can work around this using xarray_jax.dims_change_on_unflatten, however,\nand in the case of jax.pmap we provide a wrapper xarray_jax.pmap which allows\nit to be used with functions taking and returning xarrays.\n\n## Treatment of coordinates\n\nWe don't support passing jax arrays as coordinates when constructing a\nDataArray/Dataset. This is because xarray's advanced indexing and slicing is\nunlikely to work with jax arrays (at least when a Tracer is used during\njax.jit), and also because some important datatypes used for coordinates, like\ntimedelta64 and datetime64, are not supported by jax.\n\nFor the purposes of tree_util and jax.jit, coordinates are not treated as leaves\nof the tree (array data 'contained' by a Dataset/DataArray), they are just a\nstatic part of the structure. That means that if a jit'ed function is called\ntwice with Dataset inputs that use different coordinates, it will compile a\nseparate version of the function for each. The coordinates are treated like\nstatic_argnums by jax.jit.\n\nIf you want to use dynamic data for coordinates, we recommend making it a\ndata_var instead of a coord. You won't be able to do indexing and slicing using\nthe coordinate, but that wasn't going to work with a jax array anyway.\n\"\"\"\n\nimport collections\nimport contextlib\nimport contextvars\nfrom typing import Any, Callable, Hashable, Iterator, Mapping, Optional, Union, Tuple, TypeVar, cast\n\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport tree\nimport xarray\n\n\ndef Variable(dims, data, **kwargs) -> xarray.Variable:  # pylint:disable=invalid-name\n  \"\"\"Like xarray.Variable, but can wrap JAX arrays.\"\"\"\n  return xarray.Variable(dims, wrap(data), **kwargs)\n\n\n_JAX_COORD_ATTR_NAME = '_jax_coord'\n\n\ndef DataArray(  # pylint:disable=invalid-name\n    data,\n    coords=None,\n    dims=None,\n    name=None,\n    attrs=None,\n    jax_coords=None,\n    ) -> xarray.DataArray:\n  \"\"\"Like xarray.DataArray, but supports using JAX arrays.\n\n  Args:\n    data: As for xarray.DataArray, except jax arrays are also supported.\n    coords: Coordinates for the array, see xarray.DataArray. These coordinates\n      must be based on plain numpy arrays or something convertible to plain\n      numpy arrays. Their values will form a static part of the data structure\n      from the point of view of jax.tree_util. In particular this means these\n      coordinates will be passed as plain numpy arrays even inside a JIT'd\n      function, and the JIT'd function will be recompiled under the hood if the\n      coordinates of DataArrays passed into it change.\n      If this is not convenient for you, see also jax_coords below.\n    dims: See xarray.DataArray.\n    name: See xarray.DataArray.\n    attrs: See xarray.DataArray.\n    jax_coords: Additional coordinates, which *can* use JAX arrays. These\n      coordinates will be treated as JAX data from the point of view of\n      jax.tree_util, that means when JIT'ing they will be passed as tracers and\n      computation involving them will be JIT'd.\n      Unfortunately a side-effect of this is that they can't be used as index\n      coordinates (because xarray's indexing logic is not JIT-able). If you\n      specify a coordinate with the same name as a dimension here, it will not\n      be set as an index coordinate; this behaviour is different to the default\n      for `coords`, and it means that things like `.sel` based on the jax\n      coordinate will not work.\n      Note we require `jax_coords` to be explicitly specified via a different\n      constructor argument to `coords`, rather than just looking for jax arrays\n      within the `coords` and treating them differently. This is because it\n      affects the way jax.tree_util treats them, which is somewhat orthogonal to\n      whether the value is passed in as numpy or not, and generally needs to be\n      handled consistently so is something we encourage explicit control over.\n\n  Returns:\n    An instance of xarray.DataArray. Where JAX arrays are used as data or\n    coords, they will be wrapped with JaxArrayWrapper and can be unwrapped via\n    `unwrap` and `unwrap_data`.\n  \"\"\"\n  result = xarray.DataArray(\n      wrap(data), dims=dims, name=name, attrs=attrs or {})\n  return assign_coords(result, coords=coords, jax_coords=jax_coords)\n\n\ndef Dataset(  # pylint:disable=invalid-name\n    data_vars,\n    coords=None,\n    attrs=None,\n    jax_coords=None,\n    ) -> xarray.Dataset:\n  \"\"\"Like xarray.Dataset, but can wrap JAX arrays.\n\n  Args:\n    data_vars: As for xarray.Dataset, except jax arrays are also supported.\n    coords: Coordinates for the dataset, see xarray.Dataset. These coordinates\n      must be based on plain numpy arrays or something convertible to plain\n      numpy arrays. Their values will form a static part of the data structure\n      from the point of view of jax.tree_util. In particular this means these\n      coordinates will be passed as plain numpy arrays even inside a JIT'd\n      function, and the JIT'd function will be recompiled under the hood if the\n      coordinates of DataArrays passed into it change.\n      If this is not convenient for you, see also jax_coords below.\n    attrs: See xarray.Dataset.\n    jax_coords: Additional coordinates, which *can* use JAX arrays. These\n      coordinates will be treated as JAX data from the point of view of\n      jax.tree_util, that means when JIT'ing they will be passed as tracers and\n      computation involving them will be JIT'd.\n      Unfortunately a side-effect of this is that they can't be used as index\n      coordinates (because xarray's indexing logic is not JIT-able). If you\n      specify a coordinate with the same name as a dimension here, it will not\n      be set as an index coordinate; this behaviour is different to the default\n      for `coords`, and it means that things like `.sel` based on the jax\n      coordinate will not work.\n      Note we require `jax_coords` to be explicitly specified via a different\n      constructor argument to `coords`, rather than just looking for jax arrays\n      within the `coords` and treating them differently. This is because it\n      affects the way jax.tree_util treats them, which is somewhat orthogonal to\n      whether the value is passed in as numpy or not, and generally needs to be\n      handled consistently so is something we encourage explicit control over.\n\n  Returns:\n    An instance of xarray.Dataset. Where JAX arrays are used as data, they\n    will be wrapped with JaxArrayWrapper.\n  \"\"\"\n  wrapped_data_vars = {}\n  for name, var_like in data_vars.items():\n    # xarray.Dataset accepts a few different formats for data_vars:\n    if isinstance(var_like, jax.Array):\n      wrapped_data_vars[name] = wrap(var_like)\n    elif isinstance(var_like, tuple):\n      # Layout is (dims, data, ...). We wrap data.\n      wrapped_data_vars[name] = (var_like[0], wrap(var_like[1])) + var_like[2:]\n    else:\n      # Could be a plain numpy array or scalar (we don't wrap), or an\n      # xarray.Variable, DataArray etc, which we must assume is already wrapped\n      # if necessary (e.g. if creating using xarray_jax.{Variable,DataArray}).\n      wrapped_data_vars[name] = var_like\n\n  result = xarray.Dataset(\n      data_vars=wrapped_data_vars,\n      attrs=attrs)\n\n  return assign_coords(result, coords=coords, jax_coords=jax_coords)\n\n\nDatasetOrDataArray = TypeVar(\n    'DatasetOrDataArray', xarray.Dataset, xarray.DataArray)\n\n\ndef assign_coords(\n    x: DatasetOrDataArray,\n    *,\n    coords: Optional[Mapping[Hashable, Any]] = None,\n    jax_coords: Optional[Mapping[Hashable, Any]] = None,\n    ) -> DatasetOrDataArray:\n  \"\"\"Replacement for assign_coords which works in presence of jax_coords.\n\n  `jax_coords` allow certain specified coordinates to have their data passed as\n  JAX arrays (including through jax.jit boundaries). The compromise in return is\n  that they are not created as index coordinates and cannot be used for .sel\n  and other coordinate-based indexing operations. See docs for `jax_coords` on\n  xarray_jax.Dataset and xarray_jax.DataArray for more information.\n\n  This function can be used to set jax_coords on an existing DataArray or\n  Dataset, and also to set a mix of jax and non-jax coordinates. It implements\n  some workarounds to prevent xarray trying and failing to create IndexVariables\n  from jax arrays under the hood.\n\n  If you have any jax_coords with the same name as a dimension, you'll need to\n  use this function instead of data_array.assign_coords or dataset.assign_coords\n  in general, to avoid an xarray bug where it tries (and in our case fails) to\n  create indexes for existing jax coords. See\n  https://github.com/pydata/xarray/issues/7885.\n\n  Args:\n    x: An xarray Dataset or DataArray.\n    coords: Dict of (non-JAX) coords, or None if not assigning any.\n    jax_coords: Dict of JAX coords, or None if not assigning any. See docs for\n      xarray_jax.Dataset / DataArray for more information on jax_coords.\n\n  Returns:\n    The Dataset or DataArray with coordinates assigned, similarly to\n    Dataset.assign_coords / DataArray.assign_coords.\n  \"\"\"\n  coords = {} if coords is None else dict(coords)  # Copy before mutating.\n  jax_coords = {} if jax_coords is None else dict(jax_coords)\n\n  # Any existing JAX coords must be dropped and re-added via the workaround\n  # below, since otherwise .assign_coords will trigger an xarray bug where\n  # it tries to recreate the indexes again for the existing coordinates.\n  # Can remove if/when https://github.com/pydata/xarray/issues/7885 fixed.\n  existing_jax_coords = get_jax_coords(x)\n  jax_coords = existing_jax_coords | jax_coords\n  x = x.drop_vars(existing_jax_coords.keys())\n\n  # We need to ensure that xarray doesn't try to create an index for\n  # coordinates with the same name as a dimension, since this will fail if\n  # given a wrapped JAX tracer.\n  # It appears the only way to avoid this is to name them differently to any\n  # dimension name, then rename them back afterwards.\n  renamed_jax_coords = {}\n  for name, coord in jax_coords.items():\n    if isinstance(coord, xarray.DataArray):\n      coord = coord.variable\n    if isinstance(coord, xarray.Variable):\n      coord = coord.copy(deep=False)  # Copy before mutating attrs.\n    else:\n      # Must wrap as Variable with the correct dims first if this has not\n      # already been done, otherwise xarray.Dataset will assume the dimension\n      # name is also __NONINDEX_{n}.\n      coord = Variable((name,), coord)\n\n    # We set an attr on each jax_coord identifying it as such. These attrs on\n    # the coord Variable gets reflected on the coord DataArray exposed too, and\n    # when set on coordinates they generally get preserved under the default\n    # keep_attrs setting.\n    # These attrs are used by jax.tree_util registered flatten/unflatten to\n    # determine which coords need to be treated as leaves of the flattened\n    # structure vs static data.\n    coord.attrs[_JAX_COORD_ATTR_NAME] = True\n    renamed_jax_coords[f'__NONINDEX_{name}'] = coord\n\n  x = x.assign_coords(coords=coords | renamed_jax_coords)\n\n  rename_back_mapping = {f'__NONINDEX_{name}': name for name in jax_coords}\n  if isinstance(x, xarray.Dataset):\n    # Using 'rename' doesn't work if renaming to the same name as a dimension.\n    return x.rename_vars(rename_back_mapping)\n  else:  # DataArray\n    return x.rename(rename_back_mapping)\n\n\ndef get_jax_coords(x: DatasetOrDataArray) -> Mapping[Hashable, Any]:\n  return {\n      name: coord_var\n      for name, coord_var in x.coords.variables.items()\n      if coord_var.attrs.get(_JAX_COORD_ATTR_NAME, False)}\n\n\ndef assign_jax_coords(\n    x: DatasetOrDataArray,\n    jax_coords: Optional[Mapping[Hashable, Any]] = None,\n    **jax_coords_kwargs\n    ) -> DatasetOrDataArray:\n  \"\"\"Assigns only jax_coords, with same API as xarray's assign_coords.\"\"\"\n  return assign_coords(x, jax_coords=jax_coords or jax_coords_kwargs)\n\n\ndef wrap(value):\n  \"\"\"Wraps JAX arrays for use in xarray, passing through other values.\"\"\"\n  if isinstance(value, jax.Array):\n    return JaxArrayWrapper(value)\n  else:\n    return value\n\n\ndef unwrap(value, require_jax=False):\n  \"\"\"Unwraps wrapped JAX arrays used in xarray, passing through other values.\"\"\"\n  if isinstance(value, JaxArrayWrapper):\n    return value.jax_array\n  elif isinstance(value, jax.Array):\n    return value\n  elif require_jax:\n    raise TypeError(f'Expected JAX array, found {type(value)}.')\n  else:\n    return value\n\n\ndef _wrapped(func):\n  \"\"\"Surrounds a function with JAX array unwrapping/wrapping.\"\"\"\n  def wrapped_func(*args, **kwargs):\n    args, kwargs = tree.map_structure(unwrap, (args, kwargs))\n    result = func(*args, **kwargs)\n    return tree.map_structure(wrap, result)\n  return wrapped_func\n\n\ndef unwrap_data(\n    value: Union[xarray.Variable, xarray.DataArray],\n    require_jax: bool = False\n    ) -> Union[jax.Array, np.ndarray]:\n  \"\"\"The unwrapped (see unwrap) data of a an xarray.Variable or DataArray.\"\"\"\n  return unwrap(value.data, require_jax=require_jax)\n\n\ndef unwrap_vars(\n    dataset: Mapping[Hashable, xarray.DataArray],\n    require_jax: bool = False\n    ) -> Mapping[str, Union[jax.Array, np.ndarray]]:\n  \"\"\"The unwrapped data (see unwrap) of the variables in a dataset.\"\"\"\n  # xarray types variable names as Hashable, but in practice they're invariably\n  # strings and we convert to str to allow for a more useful return type.\n  return {str(name): unwrap_data(var, require_jax=require_jax)\n          for name, var in dataset.items()}\n\n\ndef unwrap_coords(\n    dataset: Union[xarray.Dataset, xarray.DataArray],\n    require_jax: bool = False\n    ) -> Mapping[str, Union[jax.Array, np.ndarray]]:\n  \"\"\"The unwrapped data (see unwrap) of the coords in a Dataset or DataArray.\"\"\"\n  return {str(name): unwrap_data(var, require_jax=require_jax)\n          for name, var in dataset.coords.items()}\n\n\ndef jax_data(value: Union[xarray.Variable, xarray.DataArray]) -> jax.Array:\n  \"\"\"Like unwrap_data, but will complain if not a jax array.\"\"\"\n  # Implementing this separately so we can give a more specific return type\n  # for it.\n  return cast(jax.Array, unwrap_data(value, require_jax=True))\n\n\ndef jax_vars(\n    dataset: Mapping[Hashable, xarray.DataArray]) -> Mapping[str, jax.Array]:\n  \"\"\"Like unwrap_vars, but will complain if vars are not all jax arrays.\"\"\"\n  return cast(Mapping[str, jax.Array], unwrap_vars(dataset, require_jax=True))\n\n\nclass JaxArrayWrapper(np.lib.mixins.NDArrayOperatorsMixin):\n  \"\"\"Wraps a JAX array into a duck-typed array suitable for use with xarray.\n\n  This uses an older duck-typed array protocol based on __array_ufunc__ and\n  __array_function__ which works with numpy and xarray. (In newer versions\n  of xarray it implements xarray.namedarray._typing._array_function.)\n\n  This is in the process of being superseded by the Python array API standard\n  (https://data-apis.org/array-api/latest/index.html), but JAX hasn't\n  implemented it yet. Once they have, we should be able to get rid of\n  this wrapper and use JAX arrays directly with xarray.\n\n  \"\"\"\n\n  def __init__(self, jax_array):\n    self.jax_array = jax_array\n\n  def __array_ufunc__(self, ufunc, method, *args, **kwargs):\n    for x in args:\n      if not isinstance(x, (jax.typing.ArrayLike, type(self))):\n        return NotImplemented\n    if method != '__call__':\n      return NotImplemented\n    try:\n      # Get the corresponding jax.numpy function to the NumPy ufunc:\n      func = getattr(jnp, ufunc.__name__)\n    except AttributeError:\n      return NotImplemented\n    # There may be an 'out' kwarg requesting an in-place operation, e.g. when\n    # this is called via __iadd__ (+=), __imul__ (*=) etc. JAX doesn't support\n    # in-place operations so we just remove this argument and have the ufunc\n    # return a fresh JAX array instead.\n    kwargs.pop('out', None)\n    return _wrapped(func)(*args, **kwargs)\n\n  def __array_function__(self, func, types, args, kwargs):\n    try:\n      # Get the corresponding jax.np function to the NumPy function:\n      func = getattr(jnp, func.__name__)\n    except AttributeError:\n      return NotImplemented\n    return _wrapped(func)(*args, **kwargs)\n\n  def __repr__(self):\n    return f'xarray_jax.JaxArrayWrapper({repr(self.jax_array)})'\n\n  # NDArrayOperatorsMixin already proxies most __dunder__ operator methods.\n  # We need to proxy through a few more methods in a similar way:\n\n  # Essential array properties:\n\n  @property\n  def shape(self):\n    return self.jax_array.shape\n\n  @property\n  def dtype(self):\n    return self.jax_array.dtype\n\n  @property\n  def ndim(self):\n    return self.jax_array.ndim\n\n  @property\n  def size(self):\n    return self.jax_array.size\n\n  @property\n  def real(self):\n    return self.jax_array.real\n\n  @property\n  def imag(self):\n    return self.jax_array.imag\n\n  # Array methods not covered by NDArrayOperatorsMixin:\n\n  # Allows conversion to numpy array using np.asarray etc. Warning: doing this\n  # will fail in a jax.jit-ed function.\n  def __array__(self, dtype=None, context=None):\n    return np.asarray(self.jax_array, dtype=dtype)\n\n  __getitem__ = _wrapped(lambda array, *args: array.__getitem__(*args))\n  # We drop the kwargs on this as they are not supported by JAX, but xarray\n  # uses at least one of them (the copy arg).\n  astype = _wrapped(lambda array, *args, **kwargs: array.astype(*args))\n\n  # There are many more methods which are more canonically available via (j)np\n  # functions, e.g. .sum() available via jnp.sum, and also mean, max, min,\n  # argmax, argmin etc. We don't attempt to proxy through all of these as\n  # methods, since this doesn't appear to be expected from a duck-typed array\n  # implementation. But there are a few which xarray calls as methods, so we\n  # proxy those:\n  transpose = _wrapped(jnp.transpose)\n  reshape = _wrapped(jnp.reshape)\n  all = _wrapped(jnp.all)\n\n\ndef apply_ufunc(func, *args, require_jax=False, **apply_ufunc_kwargs):\n  \"\"\"Like xarray.apply_ufunc but for jax-specific ufuncs.\n\n  Many numpy ufuncs will work fine out of the box with xarray_jax and\n  JaxArrayWrapper, since JaxArrayWrapper quacks (mostly) like a numpy array and\n  will convert many numpy operations to jax ops under the hood. For these\n  situations, xarray.apply_ufunc should work fine.\n\n  But sometimes you need a jax-specific ufunc which needs to be given a\n  jax array as input or return a jax array as output. In that case you should\n  use this helper as it will remove any JaxArrayWrapper before calling the func,\n  and wrap the result afterwards before handing it back to xarray.\n\n  Args:\n    func: A function that works with jax arrays (e.g. using functions from\n      jax.numpy) but otherwise meets the spec for the func argument to\n      xarray.apply_ufunc.\n    *args: xarray arguments to be mapped to arguments for func\n      (see xarray.apply_ufunc).\n    require_jax: Whether to require that inputs are based on jax arrays or allow\n      those based on plain numpy arrays too.\n    **apply_ufunc_kwargs: See xarray.apply_ufunc.\n\n  Returns:\n    Corresponding xarray results (see xarray.apply_ufunc).\n  \"\"\"\n  def wrapped_func(*maybe_wrapped_args):\n    unwrapped_args = [unwrap(a, require_jax) for a in maybe_wrapped_args]\n    result = func(*unwrapped_args)\n    # Result can be an array or a tuple of arrays, this handles both:\n    return jax.tree_util.tree_map(wrap, result)\n  return xarray.apply_ufunc(wrapped_func, *args, **apply_ufunc_kwargs)\n\n\ndef pmap(fn: Callable[..., Any],\n         dim: str,\n         axis_name: Optional[str] = None,\n         devices: ... = None,\n         backend: ... = None) -> Callable[..., Any]:\n  \"\"\"Wraps a subset of jax.pmap functionality to handle xarray input/output.\n\n  Constraints:\n    * Any Dataset or DataArray passed to the function must have `dim` as the\n      first dimension. This will be checked. You can ensure this if necessary\n      by calling `.transpose(dim, ...)` beforehand.\n    * All args and return values will be mapped over the first dimension,\n      it will use in_axes=0, out_axes=0.\n    * No support for static_broadcasted_argnums, donate_argnums etc.\n\n  Args:\n    fn: Function to be pmap'd which takes and returns trees which may contain\n      xarray Dataset/DataArray. Any Dataset/DataArrays passed as input must use\n      `dim` as the first dimension on all arrays.\n    dim: The xarray dimension name corresponding to the first dimension that is\n      pmapped over (pmap is called with in_axes=0, out_axes=0).\n    axis_name: Used by jax to identify the mapped axis so that parallel\n      collectives can be applied. Defaults to same as `dim`.\n    devices:\n    backend:\n      See jax.pmap.\n\n  Returns:\n    A pmap'd version of `fn`, which takes and returns Dataset/DataArray with an\n    extra leading dimension `dim` relative to what the original `fn` sees.\n  \"\"\"\n  input_treedef = None\n  output_treedef = None\n\n  def fn_passed_to_pmap(*flat_args):\n    assert input_treedef is not None\n    # Inside the pmap the original first dimension will no longer be present:\n    def check_and_remove_leading_dim(dims):\n      try:\n        index = dims.index(dim)\n      except ValueError:\n        index = None\n      if index != 0:\n        raise ValueError(f'Expected dim {dim} at index 0, found at {index}.')\n      return dims[1:]\n    with dims_change_on_unflatten(check_and_remove_leading_dim):\n      args = jax.tree_util.tree_unflatten(input_treedef, flat_args)\n    result = fn(*args)\n    nonlocal output_treedef\n    flat_result, output_treedef = jax.tree_util.tree_flatten(result)\n    return flat_result\n\n  pmapped_fn = jax.pmap(\n      fn_passed_to_pmap,\n      axis_name=axis_name or dim,\n      in_axes=0,\n      out_axes=0,\n      devices=devices,\n      backend=backend)\n\n  def result_fn(*args):\n    nonlocal input_treedef\n    flat_args, input_treedef = jax.tree_util.tree_flatten(args)\n    flat_result = pmapped_fn(*flat_args)\n    assert output_treedef is not None\n    # After the pmap an extra leading axis will be present, we need to add an\n    # xarray dimension for this when unflattening the result:\n    with dims_change_on_unflatten(lambda dims: (dim,) + dims):\n      return jax.tree_util.tree_unflatten(output_treedef, flat_result)\n\n  return result_fn\n\n\n# Register xarray datatypes with jax.tree_util.\n\n\nDimsChangeFn = Callable[[Tuple[Hashable, ...]], Tuple[Hashable, ...]]\n_DIMS_CHANGE_ON_UNFLATTEN_FN: contextvars.ContextVar[DimsChangeFn] = (\n    contextvars.ContextVar('dims_change_on_unflatten_fn'))\n\n\n@contextlib.contextmanager\ndef dims_change_on_unflatten(dims_change_fn: DimsChangeFn):\n  \"\"\"Can be used to change the dims used when unflattening arrays into xarrays.\n\n  This is useful when some axes were added to / removed from the underlying jax\n  arrays after they were flattened using jax.tree_util.tree_flatten, and you\n  want to unflatten them again afterwards using the original treedef but\n  adjusted for the added/removed dimensions.\n\n  It can also be used with jax.tree_util.tree_map, when it's called with a\n  function that adds/removes axes or otherwise changes the axis order.\n\n  When dimensions are removed, any coordinates using those removed dimensions\n  will also be removed on unflatten.\n\n  This is implemented as a context manager that sets some thread-local state\n  affecting the behaviour of our unflatten functions, because it's not possible\n  to directly modify the treedef to change the dims/coords in it (and with\n  tree_map, the treedef isn't exposed to you anyway).\n\n  Args:\n    dims_change_fn: Maps a tuple of dimension names for the original\n      Variable/DataArray/Dataset that was flattened, to an updated tuple of\n      dimensions which should be used when unflattening.\n\n  Yields:\n    To a context manager in whose scope jax.tree_util.tree_unflatten and\n    jax.tree_util.tree_map will apply the dims_change_fn before reconstructing\n    xarrays from jax arrays.\n  \"\"\"\n  token = _DIMS_CHANGE_ON_UNFLATTEN_FN.set(dims_change_fn)\n  try:\n    yield\n  finally:\n    _DIMS_CHANGE_ON_UNFLATTEN_FN.reset(token)\n\n\ndef _flatten_variable(v: xarray.Variable) -> Tuple[\n    Tuple[jax.typing.ArrayLike], Tuple[Hashable, ...]]:\n  \"\"\"Flattens a Variable for jax.tree_util.\"\"\"\n  children = (unwrap_data(v),)\n  aux = v.dims\n  return children, aux\n\n\ndef _unflatten_variable(\n    aux: Tuple[Hashable, ...],\n    children: Tuple[jax.typing.ArrayLike]) -> xarray.Variable:\n  \"\"\"Unflattens a Variable for jax.tree_util.\"\"\"\n  dims = aux\n  dims_change_fn = _DIMS_CHANGE_ON_UNFLATTEN_FN.get(None)\n  if dims_change_fn: dims = dims_change_fn(dims)\n  return Variable(dims=dims, data=children[0])\n\n\ndef _split_static_and_jax_coords(\n    coords: xarray.core.coordinates.Coordinates) -> Tuple[\n        Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]]:\n  static_coord_vars = {}\n  jax_coord_vars = {}\n  for name, coord in coords.items():\n    if coord.attrs.get(_JAX_COORD_ATTR_NAME, False):\n      jax_coord_vars[name] = coord.variable\n    else:\n      assert not isinstance(coord, (jax.Array, JaxArrayWrapper))\n      static_coord_vars[name] = coord.variable\n  return static_coord_vars, jax_coord_vars\n\n\ndef _drop_with_none_of_dims(\n    coord_vars: Mapping[Hashable, xarray.Variable],\n    dims: Tuple[Hashable]) -> Mapping[Hashable, xarray.Variable]:\n  return {name: var for name, var in coord_vars.items()\n          if set(var.dims) <= set(dims)}\n\n\nclass _HashableCoords(collections.abc.Mapping):\n  \"\"\"Wraps a dict of xarray Variables as hashable, used for static coordinates.\n\n  This needs to be hashable so that when an xarray.Dataset is passed to a\n  jax.jit'ed function, jax can check whether it's seen an array with the\n  same static coordinates(*) before or whether it needs to recompile the\n  function for the new values of the static coordinates.\n\n  (*) note jax_coords are not included in this; their value can be different\n  on different calls without triggering a recompile.\n  \"\"\"\n\n  def __init__(self, coord_vars: Mapping[Hashable, xarray.Variable]):\n    self._variables = coord_vars\n\n  def __repr__(self) -> str:\n    return f'_HashableCoords({repr(self._variables)})'\n\n  def __getitem__(self, key: Hashable) -> xarray.Variable:\n    return self._variables[key]\n\n  def __len__(self) -> int:\n    return len(self._variables)\n\n  def __iter__(self) -> Iterator[Hashable]:\n    return iter(self._variables)\n\n  def __hash__(self):\n    if not hasattr(self, '_hash'):\n      self._hash = hash(frozenset((name, var.data.tobytes())\n                                  for name, var in self._variables.items()))\n    return self._hash\n\n  def __eq__(self, other):\n    if self is other:\n      return True\n    elif not isinstance(other, type(self)):\n      return NotImplemented\n    elif self._variables is other._variables:\n      return True\n    else:\n      return self._variables.keys() == other._variables.keys() and all(\n          variable.equals(other._variables[name])\n          for name, variable in self._variables.items())\n\n\ndef _flatten_data_array(v: xarray.DataArray) -> Tuple[\n    # Children (data variable, jax_coord_vars):\n    Tuple[xarray.Variable, Mapping[Hashable, xarray.Variable]],\n    # Static auxiliary data (name, static_coord_vars):\n    Tuple[Optional[Hashable], _HashableCoords]]:\n  \"\"\"Flattens a DataArray for jax.tree_util.\"\"\"\n  static_coord_vars, jax_coord_vars = _split_static_and_jax_coords(v.coords)\n  children = (v.variable, jax_coord_vars)\n  aux = (v.name, _HashableCoords(static_coord_vars))\n  return children, aux\n\n\ndef _unflatten_data_array(\n    aux: Tuple[Optional[Hashable], _HashableCoords],\n    children: Tuple[xarray.Variable, Mapping[Hashable, xarray.Variable]],\n) -> xarray.DataArray:\n  \"\"\"Unflattens a DataArray for jax.tree_util.\"\"\"\n  variable, jax_coord_vars = children\n  name, static_coord_vars = aux\n  # Drop static coords which have dims not present in any of the data_vars.\n  # These would generally be dims that were dropped by a dims_change_fn, but\n  # because static coordinates don't go through dims_change_fn on unflatten, we\n  # just drop them where this causes a problem.\n  # Since jax_coords go through the dims_change_fn on unflatten we don't need\n  # to do this for jax_coords.\n  static_coord_vars = _drop_with_none_of_dims(static_coord_vars, variable.dims)\n  return DataArray(\n      variable, name=name, coords=static_coord_vars, jax_coords=jax_coord_vars)\n\n\ndef _flatten_dataset(dataset: xarray.Dataset) -> Tuple[\n    # Children (data variables, jax_coord_vars):\n    Tuple[Mapping[Hashable, xarray.Variable],\n          Mapping[Hashable, xarray.Variable]],\n    # Static auxiliary data (static_coord_vars):\n    _HashableCoords]:\n  \"\"\"Flattens a Dataset for jax.tree_util.\"\"\"\n  variables = {name: data_array.variable\n               for name, data_array in dataset.data_vars.items()}\n  static_coord_vars, jax_coord_vars = _split_static_and_jax_coords(\n      dataset.coords)\n  children = (variables, jax_coord_vars)\n  aux = _HashableCoords(static_coord_vars)\n  return children, aux\n\n\ndef _unflatten_dataset(\n    aux: _HashableCoords,\n    children: Tuple[Mapping[Hashable, xarray.Variable],\n                    Mapping[Hashable, xarray.Variable]],\n    ) -> xarray.Dataset:\n  \"\"\"Unflattens a Dataset for jax.tree_util.\"\"\"\n  data_vars, jax_coord_vars = children\n  static_coord_vars = aux\n  dataset = xarray.Dataset(data_vars)\n  # Drop static coords which have dims not present in any of the data_vars.\n  # See corresponding comment in _unflatten_data_array.\n  static_coord_vars = _drop_with_none_of_dims(static_coord_vars, dataset.dims)  # pytype: disable=wrong-arg-types\n  return assign_coords(\n      dataset, coords=static_coord_vars, jax_coords=jax_coord_vars)\n\n\njax.tree_util.register_pytree_node(\n    xarray.Variable, _flatten_variable, _unflatten_variable)\n# This is a subclass of Variable but still needs registering separately.\n# Flatten/unflatten for IndexVariable is a bit of a corner case but we do\n# need to support it.\njax.tree_util.register_pytree_node(\n    xarray.IndexVariable, _flatten_variable, _unflatten_variable)\njax.tree_util.register_pytree_node(\n    xarray.DataArray, _flatten_data_array, _unflatten_data_array)\njax.tree_util.register_pytree_node(\n    xarray.Dataset, _flatten_dataset, _unflatten_dataset)\n",
    "23": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for xarray_jax.\"\"\"\n\nfrom absl.testing import absltest\nimport chex\nfrom graphcast import xarray_jax\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nimport xarray\n\n\nclass XarrayJaxTest(absltest.TestCase):\n\n  def test_jax_array_wrapper_with_numpy_api(self):\n    # This is just a side benefit of making things work with xarray, but the\n    # JaxArrayWrapper does allow you to manipulate JAX arrays using the\n    # standard numpy API, without converting them to numpy in the process:\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    x = xarray_jax.JaxArrayWrapper(ones)\n    x = np.abs((x + 2) * (x - 3))\n    x = x[:-1, 1:3]\n    x = np.concatenate([x, x + 1], axis=0)\n    x = np.transpose(x, (1, 0))\n    x = np.reshape(x, (-1,))\n    x = x.astype(np.int32)\n    self.assertIsInstance(x, xarray_jax.JaxArrayWrapper)\n    # An explicit conversion gets us out of JAX-land however:\n    self.assertIsInstance(np.asarray(x), np.ndarray)\n\n  def test_jax_xarray_variable(self):\n    def ops_via_xarray(inputs):\n      x = xarray_jax.Variable(('lat', 'lon'), inputs)\n      # We'll apply a sequence of operations just to test that the end result is\n      # still a JAX array, i.e. we haven't converted to numpy at any point.\n      x = np.abs((x + 2) * (x - 3))\n      x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n      x = xarray.Variable.concat([x, x + 1], dim='lat')\n      x = x.transpose('lon', 'lat')\n      x = x.stack(channels=('lon', 'lat'))\n      x = x.sum()\n      return xarray_jax.jax_data(x)\n\n    # Check it doesn't leave jax-land when passed concrete values:\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    result = ops_via_xarray(ones)\n    self.assertIsInstance(result, jax.Array)\n\n    # And that you can JIT it and compute gradients through it. These will\n    # involve passing jax tracers through the xarray computation:\n    jax.jit(ops_via_xarray)(ones)\n    jax.grad(ops_via_xarray)(ones)\n\n  def test_jax_xarray_data_array(self):\n    def ops_via_xarray(inputs):\n      x = xarray_jax.DataArray(dims=('lat', 'lon'),\n                               data=inputs,\n                               coords={'lat': np.arange(3) * 10,\n                                       'lon': np.arange(4) * 10})\n      x = np.abs((x + 2) * (x - 3))\n      x = x.sel({'lat': slice(0, 20)})\n      y = xarray_jax.DataArray(dims=('lat', 'lon'),\n                               data=ones,\n                               coords={'lat': np.arange(3, 6) * 10,\n                                       'lon': np.arange(4) * 10})\n      x = xarray.concat([x, y], dim='lat')\n      x = x.transpose('lon', 'lat')\n      x = x.stack(channels=('lon', 'lat'))\n      x = x.unstack()\n      x = x.sum()\n      return xarray_jax.jax_data(x)\n\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    result = ops_via_xarray(ones)\n    self.assertIsInstance(result, jax.Array)\n\n    jax.jit(ops_via_xarray)(ones)\n    jax.grad(ops_via_xarray)(ones)\n\n  def test_jax_xarray_dataset(self):\n    def ops_via_xarray(foo, bar):\n      x = xarray_jax.Dataset(\n          data_vars={'foo': (('lat', 'lon'), foo),\n                     'bar': (('time', 'lat', 'lon'), bar)},\n          coords={\n              'time': np.arange(2),\n              'lat': np.arange(3) * 10,\n              'lon': np.arange(4) * 10})\n      x = np.abs((x + 2) * (x - 3))\n      x = x.sel({'lat': slice(0, 20)})\n      y = xarray_jax.Dataset(\n          data_vars={'foo': (('lat', 'lon'), foo),\n                     'bar': (('time', 'lat', 'lon'), bar)},\n          coords={\n              'time': np.arange(2),\n              'lat': np.arange(3, 6) * 10,\n              'lon': np.arange(4) * 10})\n      x = xarray.concat([x, y], dim='lat')\n      x = x.transpose('lon', 'lat', 'time')\n      x = x.stack(channels=('lon', 'lat'))\n      x = (x.foo + x.bar).sum()\n      return xarray_jax.jax_data(x)\n\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    result = ops_via_xarray(foo, bar)\n    self.assertIsInstance(result, jax.Array)\n\n    jax.jit(ops_via_xarray)(foo, bar)\n    jax.grad(ops_via_xarray)(foo, bar)\n\n  def test_jit_function_with_xarray_variable_arguments_and_return(self):\n    function = jax.jit(lambda v: v + 1)\n    with self.subTest('jax input'):\n      inputs = xarray_jax.Variable(\n          ('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n      _ = function(inputs)\n      # We test running the jitted function a second time, to exercise logic in\n      # jax which checks if the structure of the inputs (including dimension\n      # names and coordinates) is the same as it was for the previous call and\n      # so whether it needs to re-trace-and-compile a new version of the\n      # function or not. This can run into problems if the 'aux' structure\n      # returned by the registered flatten function is not hashable/comparable.\n      outputs = function(inputs)\n      self.assertEqual(outputs.dims, inputs.dims)\n    with self.subTest('numpy input'):\n      inputs = xarray.Variable(\n          ('lat', 'lon'), np.ones((3, 4), dtype=np.float32))\n      _ = function(inputs)\n      outputs = function(inputs)\n      self.assertEqual(outputs.dims, inputs.dims)\n\n  def test_jit_problem_if_convert_to_plain_numpy_array(self):\n    inputs = xarray_jax.DataArray(\n        data=jnp.ones((2,), dtype=np.float32), dims=('foo',))\n    with self.assertRaises(jax.errors.TracerArrayConversionError):\n      # Calling .values on a DataArray converts its values to numpy:\n      jax.jit(lambda data_array: data_array.values)(inputs)\n\n  def test_grad_function_with_xarray_variable_arguments(self):\n    x = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n    # For grad we still need a JAX scalar as the output:\n    jax.grad(lambda v: xarray_jax.jax_data(v.sum()))(x)\n\n  def test_jit_function_with_xarray_data_array_arguments_and_return(self):\n    inputs = xarray_jax.DataArray(\n        data=jnp.ones((3, 4), dtype=np.float32),\n        dims=('lat', 'lon'),\n        coords={'lat': np.arange(3),\n                'lon': np.arange(4) * 10})\n    fn = jax.jit(lambda v: v + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords, inputs.coords)\n\n  def test_jit_function_with_data_array_and_jax_coords(self):\n    inputs = xarray_jax.DataArray(\n        data=jnp.ones((3, 4), dtype=np.float32),\n        dims=('lat', 'lon'),\n        coords={'lat': np.arange(3)},\n        jax_coords={'lon': jnp.arange(4) * 10})\n    # Verify the jax_coord 'lon' retains jax data, and has not been created\n    # as an index coordinate:\n    self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', inputs.indexes)\n\n    @jax.jit\n    def fn(v):\n      # The non-JAX coord is passed with numpy array data and an index:\n      self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n      self.assertIn('lat', v.indexes)\n\n      # The jax_coord is passed with JAX array data:\n      self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n      self.assertNotIn('lon', v.indexes)\n\n      # Use the jax coord in the computation:\n      v = v + v.coords['lon']\n\n      # Return with an updated jax coord:\n      return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n\n    _ = fn(inputs)\n    outputs = fn(inputs)\n\n    # Verify the jax_coord 'lon' has jax data in the output too:\n    self.assertIsInstance(\n        outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', outputs.indexes)\n\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n    # Check our computations with the coordinate values worked:\n    chex.assert_trees_all_equal(\n        outputs.coords['lon'].data, (inputs.coords['lon']+1).data)\n    chex.assert_trees_all_equal(\n        outputs.data, (inputs + inputs.coords['lon']).data)\n\n  def test_jit_function_with_xarray_dataset_arguments_and_return(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    inputs = xarray_jax.Dataset(\n        data_vars={'foo': (('lat', 'lon'), foo),\n                   'bar': (('time', 'lat', 'lon'), bar)},\n        coords={\n            'time': np.arange(2),\n            'lat': np.arange(3) * 10,\n            'lon': np.arange(4) * 10})\n    fn = jax.jit(lambda v: v + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertEqual({'foo', 'bar'}, outputs.data_vars.keys())\n    self.assertEqual(inputs.foo.dims, outputs.foo.dims)\n    self.assertEqual(inputs.bar.dims, outputs.bar.dims)\n    chex.assert_trees_all_equal(outputs.coords, inputs.coords)\n\n  def test_jit_function_with_dataset_and_jax_coords(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    inputs = xarray_jax.Dataset(\n        data_vars={'foo': (('lat', 'lon'), foo),\n                   'bar': (('time', 'lat', 'lon'), bar)},\n        coords={\n            'time': np.arange(2),\n            'lat': np.arange(3) * 10,\n        },\n        jax_coords={'lon': jnp.arange(4) * 10}\n    )\n    # Verify the jax_coord 'lon' retains jax data, and has not been created\n    # as an index coordinate:\n    self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', inputs.indexes)\n\n    @jax.jit\n    def fn(v):\n      # The non-JAX coords are passed with numpy array data and an index:\n      self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n      self.assertIn('lat', v.indexes)\n\n      # The jax_coord is passed with JAX array data:\n      self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n      self.assertNotIn('lon', v.indexes)\n\n      # Use the jax coord in the computation:\n      v = v + v.coords['lon']\n\n      # Return with an updated jax coord:\n      return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n\n    _ = fn(inputs)\n    outputs = fn(inputs)\n\n    # Verify the jax_coord 'lon' has jax data in the output too:\n    self.assertIsInstance(\n        outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', outputs.indexes)\n\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n    # Check our computations with the coordinate values worked:\n    chex.assert_trees_all_equal(\n        (outputs.coords['lon']).data,\n        (inputs.coords['lon']+1).data,\n    )\n    outputs_dict = {key: outputs[key].data for key in outputs}\n    inputs_and_inputs_coords_dict = {\n        key: (inputs + inputs.coords['lon'])[key].data\n        for key in inputs + inputs.coords['lon']\n    }\n    chex.assert_trees_all_equal(outputs_dict, inputs_and_inputs_coords_dict)\n\n  def test_flatten_unflatten_variable(self):\n    variable = xarray_jax.Variable(\n        ('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n    children, aux = xarray_jax._flatten_variable(variable)\n    # Check auxiliary info is hashable/comparable (important for jax.jit):\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_variable(aux, children)\n    self.assertTrue(variable.equals(roundtrip))\n\n  def test_flatten_unflatten_data_array(self):\n    data_array = xarray_jax.DataArray(\n        data=jnp.ones((3, 4), dtype=np.float32),\n        dims=('lat', 'lon'),\n        coords={'lat': np.arange(3)},\n        jax_coords={'lon': np.arange(4) * 10},\n    )\n    children, aux = xarray_jax._flatten_data_array(data_array)\n    # Check auxiliary info is hashable/comparable (important for jax.jit):\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_data_array(aux, children)\n    self.assertTrue(data_array.equals(roundtrip))\n\n  def test_flatten_unflatten_dataset(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset(\n        data_vars={'foo': (('lat', 'lon'), foo),\n                   'bar': (('time', 'lat', 'lon'), bar)},\n        coords={\n            'time': np.arange(2),\n            'lat': np.arange(3) * 10},\n        jax_coords={\n            'lon': np.arange(4) * 10})\n    children, aux = xarray_jax._flatten_dataset(dataset)\n    # Check auxiliary info is hashable/comparable (important for jax.jit):\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_dataset(aux, children)\n    self.assertTrue(dataset.equals(roundtrip))\n\n  def test_flatten_unflatten_added_dim(self):\n    data_array = xarray_jax.DataArray(\n        data=jnp.ones((3, 4), dtype=np.float32),\n        dims=('lat', 'lon'),\n        coords={'lat': np.arange(3),\n                'lon': np.arange(4) * 10})\n    leaves, treedef = jax.tree_util.tree_flatten(data_array)\n    leaves = [jnp.expand_dims(x, 0) for x in leaves]\n    with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n      with_new_dim = jax.tree_util.tree_unflatten(treedef, leaves)\n    self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n    xarray.testing.assert_identical(\n        jax.device_get(data_array),\n        jax.device_get(with_new_dim.isel(new=0)))\n\n  def test_map_added_dim(self):\n    data_array = xarray_jax.DataArray(\n        data=jnp.ones((3, 4), dtype=np.float32),\n        dims=('lat', 'lon'),\n        coords={'lat': np.arange(3),\n                'lon': np.arange(4) * 10})\n    with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n      with_new_dim = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, 0),\n                                            data_array)\n    self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n    xarray.testing.assert_identical(\n        jax.device_get(data_array),\n        jax.device_get(with_new_dim.isel(new=0)))\n\n  def test_map_remove_dim(self):\n    foo = jnp.ones((1, 3, 4), dtype=np.float32)\n    bar = jnp.ones((1, 2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset(\n        data_vars={'foo': (('batch', 'lat', 'lon'), foo),\n                   'bar': (('batch', 'time', 'lat', 'lon'), bar)},\n        coords={\n            'batch': np.array([123]),\n            'time': np.arange(2),\n            'lat': np.arange(3) * 10,\n            'lon': np.arange(4) * 10})\n    with xarray_jax.dims_change_on_unflatten(lambda dims: dims[1:]):\n      with_removed_dim = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, 0),\n                                                dataset)\n    self.assertEqual(('lat', 'lon'), with_removed_dim['foo'].dims)\n    self.assertEqual(('time', 'lat', 'lon'), with_removed_dim['bar'].dims)\n    self.assertNotIn('batch', with_removed_dim.dims)\n    self.assertNotIn('batch', with_removed_dim.coords)\n    xarray.testing.assert_identical(\n        jax.device_get(dataset.isel(batch=0, drop=True)),\n        jax.device_get(with_removed_dim))\n\n  def test_pmap(self):\n    devices = jax.local_device_count()\n    foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n    bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset({\n        'foo': (('device', 'lat', 'lon'), foo),\n        'bar': (('device', 'time', 'lat', 'lon'), bar)})\n\n    def func(d):\n      self.assertNotIn('device', d.dims)\n      return d + 1\n    func = xarray_jax.pmap(func, dim='device')\n\n    result = func(dataset)\n    xarray.testing.assert_identical(\n        jax.device_get(dataset + 1),\n        jax.device_get(result))\n\n    # Can call it again with a different argument structure (it will recompile\n    # under the hood but should work):\n    dataset = dataset.drop_vars('foo')\n    result = func(dataset)\n    xarray.testing.assert_identical(\n        jax.device_get(dataset + 1),\n        jax.device_get(result))\n\n  def test_pmap_with_jax_coords(self):\n    devices = jax.local_device_count()\n    foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n    bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n    time = jnp.zeros((devices, 2), dtype=np.float32)\n    dataset = xarray_jax.Dataset(\n        {'foo': (('device', 'lat', 'lon'), foo),\n         'bar': (('device', 'time', 'lat', 'lon'), bar)},\n        coords={\n            'lat': np.arange(3),\n            'lon': np.arange(4),\n        },\n        jax_coords={\n            # Currently any jax_coords need a leading device dimension to use\n            # with pmap, same as for data_vars.\n            # TODO(matthjw): have pmap automatically broadcast to all devices\n            # where the device dimension not present.\n            'time': xarray_jax.Variable(('device', 'time'), time),\n        }\n    )\n\n    def func(d):\n      self.assertNotIn('device', d.dims)\n      self.assertNotIn('device', d.coords['time'].dims)\n\n      # The jax_coord 'time' should be passed in backed by a JAX array, but\n      # not as an index coordinate.\n      self.assertIsInstance(d.coords['time'].data, xarray_jax.JaxArrayWrapper)\n      self.assertNotIn('time', d.indexes)\n\n      return d + 1\n    func = xarray_jax.pmap(func, dim='device')\n\n    result = func(dataset)\n    xarray.testing.assert_identical(\n        jax.device_get(dataset + 1),\n        jax.device_get(result))\n\n    # Can call it again with a different argument structure (it will recompile\n    # under the hood but should work):\n    dataset = dataset.drop_vars('foo')\n    result = func(dataset)\n    xarray.testing.assert_identical(\n        jax.device_get(dataset + 1),\n        jax.device_get(result))\n\n  def test_pmap_with_tree_mix_of_xarray_and_jax_array(self):\n    devices = jax.local_device_count()\n    data_array = xarray_jax.DataArray(\n        data=jnp.ones((devices, 3, 4), dtype=np.float32),\n        dims=('device', 'lat', 'lon'))\n    plain_array = jnp.ones((devices, 2), dtype=np.float32)\n    inputs = {'foo': data_array,\n              'bar': plain_array}\n\n    def func(x):\n      return x['foo'] + 1, x['bar'] + 1\n\n    func = xarray_jax.pmap(func, dim='device')\n    result_foo, result_bar = func(inputs)\n    xarray.testing.assert_identical(\n        jax.device_get(inputs['foo'] + 1),\n        jax.device_get(result_foo))\n    np.testing.assert_array_equal(\n        jax.device_get(inputs['bar'] + 1),\n        jax.device_get(result_bar))\n\n  def test_pmap_complains_when_dim_not_first(self):\n    devices = jax.local_device_count()\n    data_array = xarray_jax.DataArray(\n        data=jnp.ones((3, devices, 4), dtype=np.float32),\n        dims=('lat', 'device', 'lon'))\n\n    func = xarray_jax.pmap(lambda x: x+1, dim='device')\n    with self.assertRaisesRegex(\n        ValueError, 'Expected dim device at index 0, found at 1'):\n      func(data_array)\n\n  def test_apply_ufunc(self):\n    inputs = xarray_jax.DataArray(\n        data=jnp.asarray([[1, 2], [3, 4]]),\n        dims=('x', 'y'),\n        coords={'x': [0, 1],\n                'y': [2, 3]})\n    result = xarray_jax.apply_ufunc(\n        lambda x: jnp.sum(x, axis=-1),\n        inputs,\n        input_core_dims=[['x']])\n    expected_result = xarray_jax.DataArray(\n        data=[4, 6],\n        dims=('y',),\n        coords={'y': [2, 3]})\n    xarray.testing.assert_identical(expected_result, jax.device_get(result))\n\n  def test_apply_ufunc_multiple_return_values(self):\n    def ufunc(array):\n      return jnp.min(array, axis=-1), jnp.max(array, axis=-1)\n    inputs = xarray_jax.DataArray(\n        data=jnp.asarray([[1, 4], [3, 2]]),\n        dims=('x', 'y'),\n        coords={'x': [0, 1],\n                'y': [2, 3]})\n    result = xarray_jax.apply_ufunc(\n        ufunc, inputs, input_core_dims=[['x']], output_core_dims=[[], []])\n    expected = (\n        # Mins:\n        xarray_jax.DataArray(\n            data=[1, 2],\n            dims=('y',),\n            coords={'y': [2, 3]}\n        ),\n        # Maxes:\n        xarray_jax.DataArray(\n            data=[3, 4],\n            dims=('y',),\n            coords={'y': [2, 3]}\n        )\n    )\n    xarray.testing.assert_identical(expected[0], jax.device_get(result[0]))\n    xarray.testing.assert_identical(expected[1], jax.device_get(result[1]))\n\nif __name__ == '__main__':\n  absltest.main()\n",
    "24": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Utilities for working with trees of xarray.DataArray (including Datasets).\n\nNote that xarray.Dataset doesn't work out-of-the-box with the `tree` library;\nit won't work as a leaf node since it implements Mapping, but also won't work\nas an internal node since tree doesn't know how to re-create it properly.\n\nTo fix this, we reimplement a subset of `map_structure`, exposing its\nconstituent DataArrays as leaf nodes. This means it can be mapped over as a\ngeneric container of DataArrays, while still preserving the result as a Dataset\nwhere possible.\n\nThis is useful because in a few places we need to handle a general\nMapping[str, DataArray] (where the coordinates might not be compatible across\nthe constituent DataArrays) but also the special case of a Dataset nicely.\n\nFor the result e.g. of a tree.map_structure(fn, dataset), if fn returns None for\nsome of the child DataArrays, they will be omitted from the returned dataset. If\nany values other than DataArrays or None are returned, then we don't attempt to\nreturn a Dataset and just return a plain dict of the results. Similarly if\nDataArrays are returned but with non-matching coordinates, it will just return a\nplain dict of DataArrays.\n\nNote xarray datatypes are registered with `jax.tree_util` by xarray_jax.py,\nbut `jax.tree_util.tree_map` is distinct from the `xarray_tree.map_structure`.\nas the former exposes the underlying JAX/numpy arrays as leaf nodes, while the\nlatter exposes DataArrays as leaf nodes.\n\"\"\"\n\nfrom typing import Any, Callable\n\nimport xarray\n\n\ndef map_structure(func: Callable[..., Any], *structures: Any) -> Any:\n  \"\"\"Maps func through given structures with xarrays. See tree.map_structure.\"\"\"\n  if not callable(func):\n    raise TypeError(f'func must be callable, got: {func}')\n  if not structures:\n    raise ValueError('Must provide at least one structure')\n\n  first = structures[0]\n  if isinstance(first, xarray.Dataset):\n    data = {k: func(*[s[k] for s in structures]) for k in first.keys()}\n    if all(isinstance(a, (type(None), xarray.DataArray))\n           for a in data.values()):\n      data_arrays = [v.rename(k) for k, v in data.items() if v is not None]\n      try:\n        return xarray.merge(data_arrays, join='exact')\n      except ValueError:  # Exact join not possible.\n        pass\n    return data\n  if isinstance(first, dict):\n    return {k: map_structure(func, *[s[k] for s in structures])\n            for k in first.keys()}\n  if isinstance(first, (list, tuple, set)):\n    return type(first)(map_structure(func, *s) for s in zip(*structures))\n  return func(*structures)\n",
    "25": "# Copyright 2023 DeepMind Technologies Limited.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS-IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Tests for xarray_tree.\"\"\"\n\nfrom absl.testing import absltest\nfrom graphcast import xarray_tree\nimport numpy as np\nimport xarray\n\n\nTEST_DATASET = xarray.Dataset(\n    data_vars={\n        \"foo\": ((\"x\", \"y\"), np.zeros((2, 3))),\n        \"bar\": ((\"x\",), np.zeros((2,))),\n    },\n    coords={\n        \"x\": [1, 2],\n        \"y\": [10, 20, 30],\n    }\n)\n\n\nclass XarrayTreeTest(absltest.TestCase):\n\n  def test_map_structure_maps_over_leaves_but_preserves_dataset_type(self):\n    def fn(leaf):\n      self.assertIsInstance(leaf, xarray.DataArray)\n      result = leaf + 1\n      # Removing the name from the returned DataArray to test that we don't rely\n      # on it being present to restore the correct names in the result:\n      result = result.rename(None)\n      return result\n\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, xarray.Dataset)\n    self.assertSameElements({\"foo\", \"bar\"}, result.keys())\n\n  def test_map_structure_on_data_arrays(self):\n    data_arrays = dict(TEST_DATASET)\n    result = xarray_tree.map_structure(lambda x: x+1, data_arrays)\n    self.assertIsInstance(result, dict)\n    self.assertSameElements({\"foo\", \"bar\"}, result.keys())\n\n  def test_map_structure_on_dataset_plain_dict_when_coords_incompatible(self):\n    def fn(leaf):\n      # Returns DataArrays that can't be exactly merged back into a Dataset\n      # due to the coordinates not matching:\n      if leaf.name == \"foo\":\n        return xarray.DataArray(\n            data=np.zeros(2), dims=(\"x\",), coords={\"x\": [1, 2]})\n      else:\n        return xarray.DataArray(\n            data=np.zeros(2), dims=(\"x\",), coords={\"x\": [3, 4]})\n\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, dict)\n    self.assertSameElements({\"foo\", \"bar\"}, result.keys())\n\n  def test_map_structure_on_dataset_drops_vars_with_none_return_values(self):\n    def fn(leaf):\n      return leaf if leaf.name == \"foo\" else None\n\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, xarray.Dataset)\n    self.assertSameElements({\"foo\"}, result.keys())\n\n  def test_map_structure_on_dataset_returns_plain_dict_other_return_types(self):\n    def fn(leaf):\n      self.assertIsInstance(leaf, xarray.DataArray)\n      return \"not a DataArray\"\n\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertEqual({\"foo\": \"not a DataArray\",\n                      \"bar\": \"not a DataArray\"}, result)\n\n  def test_map_structure_two_args_different_variable_orders(self):\n    dataset_different_order = TEST_DATASET[[\"bar\", \"foo\"]]\n    def fn(arg1, arg2):\n      self.assertEqual(arg1.name, arg2.name)\n    xarray_tree.map_structure(fn, TEST_DATASET, dataset_different_order)\n\n\nif __name__ == \"__main__\":\n  absltest.main()\n",
    "27": "def _get_flat_arrays_and_single_timestep_treedef(variables):\n    flat_arrays = jax.tree_util.tree_leaves(variables.transpose('time', ...))\n    (_, treedef) = jax.tree_util.tree_flatten(variables.isel(time=0, drop=True))\n    return (flat_arrays, treedef)",
    "28": "class Predictor(predictor_base.Predictor):\n    \"\"\"Wraps a one-step Predictor to make multi-step predictions autoregressively.\n\n  The wrapped Predictor will be used to predict a single timestep conditional\n  on the inputs passed to the outer Predictor. Its predictions are then\n  passed back in as inputs at the next timestep, for as many timesteps as are\n  requested in the targets_template. (When multiple timesteps of input are\n  used, a rolling window of inputs is maintained with new predictions\n  concatenated onto the end).\n\n  You may ask for additional variables to be predicted as targets which aren't\n  used as inputs. These will be predicted as output variables only and not fed\n  back in autoregressively. All target variables must be time-dependent however.\n\n  You may also specify static (non-time-dependent) inputs which will be passed\n  in at each timestep but are not predicted.\n\n  At present, any time-dependent inputs must also be present as targets so they\n  can be passed in autoregressively.\n\n  The loss of the wrapped one-step Predictor is averaged over all timesteps to\n  give a loss for the autoregressive Predictor.\n  \"\"\"\n\n    def __init__(self, predictor: predictor_base.Predictor, noise_level: Optional[float]=None, gradient_checkpointing: bool=False):\n        \"\"\"Initializes an autoregressive predictor wrapper.\n\n    Args:\n      predictor: A predictor to wrap in an auto-regressive way.\n      noise_level: Optional value that multiplies the standard normal noise\n        added to the time-dependent variables of the predictor inputs. In\n        particular, no noise is added to the predictions that are fed back\n        auto-regressively. Defaults to not adding noise.\n      gradient_checkpointing: If True, gradient checkpointing will be\n        used at each step of the computation to save on memory. Roughtly this\n        should make the backwards pass two times more expensive, and the time\n        per step counting the forward pass, should only increase by about 50%.\n        Note this parameter will be ignored with a warning if the scan sequence\n        length is 1.\n    \"\"\"\n        self._predictor = predictor\n        self._noise_level = noise_level\n        self._gradient_checkpointing = gradient_checkpointing\n\n    def _get_and_validate_constant_inputs(self, inputs, targets, forcings):\n        constant_inputs = inputs.drop_vars(targets.keys(), errors='ignore')\n        constant_inputs = constant_inputs.drop_vars(forcings.keys(), errors='ignore')\n        for (name, var) in constant_inputs.items():\n            if 'time' in var.dims:\n                raise ValueError(f'Time-dependent input variable {name} must either be a forcing variable, or a target variable to allow for auto-regressive feedback.')\n        return constant_inputs\n\n    def _validate_targets_and_forcings(self, targets, forcings):\n        for (name, var) in targets.items():\n            if 'time' not in var.dims:\n                raise ValueError(f'Target variable {name} must be time-dependent.')\n        for (name, var) in forcings.items():\n            if 'time' not in var.dims:\n                raise ValueError(f'Forcing variable {name} must be time-dependent.')\n        overlap = forcings.keys() & targets.keys()\n        if overlap:\n            raise ValueError(f\"The following were specified as both targets and forcings, which isn't allowed: {overlap}\")\n\n    def _update_inputs(self, inputs, next_frame):\n        num_inputs = inputs.dims['time']\n        predicted_or_forced_inputs = next_frame[list(inputs.keys())]\n        return xarray.concat([inputs, predicted_or_forced_inputs], dim='time').tail(time=num_inputs).assign_coords(time=inputs.coords['time'])\n\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n        \"\"\"Calls the Predictor.\n\n    Args:\n      inputs: input variable used to make predictions. Inputs can include both\n        time-dependent and time independent variables. Any time-dependent\n        input variables must also be present in the targets_template or the\n        forcings.\n      targets_template: A target template containing informations about which\n        variables should be predicted and the time alignment of the predictions.\n        All target variables must be time-dependent.\n        The number of time frames is used to set the number of unroll of the AR\n        predictor (e.g. multiple unroll of the inner predictor for one time step\n        in the targets is not supported yet).\n      forcings: Variables that will be fed to the model. The variables\n        should not overlap with the target ones. The time coordinates of the\n        forcing variables should match the target ones.\n        Forcing variables which are also present in the inputs, will be used to\n        supply ground-truth values for those inputs when they are passed to the\n        underlying predictor at timesteps beyond the first timestep.\n      **kwargs: Additional arguments passed along to the inner Predictor.\n\n    Returns:\n      predictions: the model predictions matching the target template.\n\n    Raise:\n      ValueError: if the time coordinates of the inputs and targets are not\n        different by a constant time step.\n    \"\"\"\n        constant_inputs = self._get_and_validate_constant_inputs(inputs, targets_template, forcings)\n        self._validate_targets_and_forcings(targets_template, forcings)\n        inputs = inputs.drop_vars(constant_inputs.keys())\n        target_template = targets_template.isel(time=[0])\n        (flat_forcings, forcings_treedef) = _get_flat_arrays_and_single_timestep_treedef(forcings)\n        scan_variables = flat_forcings\n\n        def one_step_prediction(inputs, scan_variables):\n            flat_forcings = scan_variables\n            forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, target_template.coords['time'])\n            all_inputs = xarray.merge([constant_inputs, inputs])\n            predictions: xarray.Dataset = self._predictor(all_inputs, target_template, forcings=forcings, **kwargs)\n            next_frame = xarray.merge([predictions, forcings])\n            next_inputs = self._update_inputs(inputs, next_frame)\n            predictions = predictions.squeeze('time', drop=True)\n            flat_pred = jax.tree_util.tree_leaves(predictions)\n            return (next_inputs, flat_pred)\n        if self._gradient_checkpointing:\n            scan_length = targets_template.dims['time']\n            if scan_length <= 1:\n                logging.warning('Skipping gradient checkpointing for sequence length of 1')\n            else:\n                one_step_prediction = hk.remat(one_step_prediction)\n        (_, flat_preds) = hk.scan(one_step_prediction, inputs, scan_variables)\n        scan_result_template = target_template.squeeze('time', drop=True).expand_dims(time=targets_template.coords['time'], axis=0)\n        (_, scan_result_treedef) = jax.tree_util.tree_flatten(scan_result_template)\n        predictions = jax.tree_util.tree_unflatten(scan_result_treedef, flat_preds)\n        return predictions\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n        \"\"\"The mean of the per-timestep losses of the underlying predictor.\"\"\"\n        if targets.sizes['time'] == 1:\n            return self._predictor.loss(inputs, targets, forcings, **kwargs)\n        constant_inputs = self._get_and_validate_constant_inputs(inputs, targets, forcings)\n        self._validate_targets_and_forcings(targets, forcings)\n        inputs = inputs.drop_vars(constant_inputs.keys())\n        if self._noise_level:\n\n            def add_noise(x):\n                return x + self._noise_level * jax.random.normal(hk.next_rng_key(), shape=x.shape)\n            inputs = jax.tree_map(add_noise, inputs)\n        (flat_targets, target_treedef) = _get_flat_arrays_and_single_timestep_treedef(targets)\n        scan_variables = flat_targets\n        (flat_forcings, forcings_treedef) = _get_flat_arrays_and_single_timestep_treedef(forcings)\n        scan_variables = (flat_targets, flat_forcings)\n\n        def one_step_loss(inputs, scan_variables):\n            (flat_target, flat_forcings) = scan_variables\n            forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, targets.coords['time'][:1])\n            target = _unflatten_and_expand_time(flat_target, target_treedef, targets.coords['time'][:1])\n            all_inputs = xarray.merge([constant_inputs, inputs])\n            ((loss, diagnostics), predictions) = self._predictor.loss_and_predictions(all_inputs, target, forcings=forcings, **kwargs)\n            (loss, diagnostics) = xarray_tree.map_structure(xarray_jax.unwrap_data, (loss, diagnostics))\n            predictions = cast(xarray.Dataset, predictions)\n            next_frame = xarray.merge([predictions, forcings])\n            next_inputs = self._update_inputs(inputs, next_frame)\n            return (next_inputs, (loss, diagnostics))\n        if self._gradient_checkpointing:\n            scan_length = targets.dims['time']\n            if scan_length <= 1:\n                logging.warning('Skipping gradient checkpointing for sequence length of 1')\n            else:\n                one_step_loss = hk.remat(one_step_loss)\n        (_, (per_timestep_losses, per_timestep_diagnostics)) = hk.scan(one_step_loss, inputs, scan_variables)\n        (loss, diagnostics) = jax.tree_util.tree_map(lambda x: xarray_jax.DataArray(x, dims=('time', 'batch')).mean('time', skipna=False), (per_timestep_losses, per_timestep_diagnostics))\n        return (loss, diagnostics)",
    "29": "def __init__(self, predictor: predictor_base.Predictor, noise_level: Optional[float]=None, gradient_checkpointing: bool=False):\n    \"\"\"Initializes an autoregressive predictor wrapper.\n\n    Args:\n      predictor: A predictor to wrap in an auto-regressive way.\n      noise_level: Optional value that multiplies the standard normal noise\n        added to the time-dependent variables of the predictor inputs. In\n        particular, no noise is added to the predictions that are fed back\n        auto-regressively. Defaults to not adding noise.\n      gradient_checkpointing: If True, gradient checkpointing will be\n        used at each step of the computation to save on memory. Roughtly this\n        should make the backwards pass two times more expensive, and the time\n        per step counting the forward pass, should only increase by about 50%.\n        Note this parameter will be ignored with a warning if the scan sequence\n        length is 1.\n    \"\"\"\n    self._predictor = predictor\n    self._noise_level = noise_level\n    self._gradient_checkpointing = gradient_checkpointing",
    "30": "def _get_and_validate_constant_inputs(self, inputs, targets, forcings):\n    constant_inputs = inputs.drop_vars(targets.keys(), errors='ignore')\n    constant_inputs = constant_inputs.drop_vars(forcings.keys(), errors='ignore')\n    for (name, var) in constant_inputs.items():\n        if 'time' in var.dims:\n            raise ValueError(f'Time-dependent input variable {name} must either be a forcing variable, or a target variable to allow for auto-regressive feedback.')\n    return constant_inputs",
    "31": "def _validate_targets_and_forcings(self, targets, forcings):\n    for (name, var) in targets.items():\n        if 'time' not in var.dims:\n            raise ValueError(f'Target variable {name} must be time-dependent.')\n    for (name, var) in forcings.items():\n        if 'time' not in var.dims:\n            raise ValueError(f'Forcing variable {name} must be time-dependent.')\n    overlap = forcings.keys() & targets.keys()\n    if overlap:\n        raise ValueError(f\"The following were specified as both targets and forcings, which isn't allowed: {overlap}\")",
    "33": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n    \"\"\"Calls the Predictor.\n\n    Args:\n      inputs: input variable used to make predictions. Inputs can include both\n        time-dependent and time independent variables. Any time-dependent\n        input variables must also be present in the targets_template or the\n        forcings.\n      targets_template: A target template containing informations about which\n        variables should be predicted and the time alignment of the predictions.\n        All target variables must be time-dependent.\n        The number of time frames is used to set the number of unroll of the AR\n        predictor (e.g. multiple unroll of the inner predictor for one time step\n        in the targets is not supported yet).\n      forcings: Variables that will be fed to the model. The variables\n        should not overlap with the target ones. The time coordinates of the\n        forcing variables should match the target ones.\n        Forcing variables which are also present in the inputs, will be used to\n        supply ground-truth values for those inputs when they are passed to the\n        underlying predictor at timesteps beyond the first timestep.\n      **kwargs: Additional arguments passed along to the inner Predictor.\n\n    Returns:\n      predictions: the model predictions matching the target template.\n\n    Raise:\n      ValueError: if the time coordinates of the inputs and targets are not\n        different by a constant time step.\n    \"\"\"\n    constant_inputs = self._get_and_validate_constant_inputs(inputs, targets_template, forcings)\n    self._validate_targets_and_forcings(targets_template, forcings)\n    inputs = inputs.drop_vars(constant_inputs.keys())\n    target_template = targets_template.isel(time=[0])\n    (flat_forcings, forcings_treedef) = _get_flat_arrays_and_single_timestep_treedef(forcings)\n    scan_variables = flat_forcings\n\n    def one_step_prediction(inputs, scan_variables):\n        flat_forcings = scan_variables\n        forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, target_template.coords['time'])\n        all_inputs = xarray.merge([constant_inputs, inputs])\n        predictions: xarray.Dataset = self._predictor(all_inputs, target_template, forcings=forcings, **kwargs)\n        next_frame = xarray.merge([predictions, forcings])\n        next_inputs = self._update_inputs(inputs, next_frame)\n        predictions = predictions.squeeze('time', drop=True)\n        flat_pred = jax.tree_util.tree_leaves(predictions)\n        return (next_inputs, flat_pred)\n    if self._gradient_checkpointing:\n        scan_length = targets_template.dims['time']\n        if scan_length <= 1:\n            logging.warning('Skipping gradient checkpointing for sequence length of 1')\n        else:\n            one_step_prediction = hk.remat(one_step_prediction)\n    (_, flat_preds) = hk.scan(one_step_prediction, inputs, scan_variables)\n    scan_result_template = target_template.squeeze('time', drop=True).expand_dims(time=targets_template.coords['time'], axis=0)\n    (_, scan_result_treedef) = jax.tree_util.tree_flatten(scan_result_template)\n    predictions = jax.tree_util.tree_unflatten(scan_result_treedef, flat_preds)\n    return predictions",
    "34": "def one_step_prediction(inputs, scan_variables):\n    flat_forcings = scan_variables\n    forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, target_template.coords['time'])\n    all_inputs = xarray.merge([constant_inputs, inputs])\n    predictions: xarray.Dataset = self._predictor(all_inputs, target_template, forcings=forcings, **kwargs)\n    next_frame = xarray.merge([predictions, forcings])\n    next_inputs = self._update_inputs(inputs, next_frame)\n    predictions = predictions.squeeze('time', drop=True)\n    flat_pred = jax.tree_util.tree_leaves(predictions)\n    return (next_inputs, flat_pred)",
    "35": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n    \"\"\"The mean of the per-timestep losses of the underlying predictor.\"\"\"\n    if targets.sizes['time'] == 1:\n        return self._predictor.loss(inputs, targets, forcings, **kwargs)\n    constant_inputs = self._get_and_validate_constant_inputs(inputs, targets, forcings)\n    self._validate_targets_and_forcings(targets, forcings)\n    inputs = inputs.drop_vars(constant_inputs.keys())\n    if self._noise_level:\n\n        def add_noise(x):\n            return x + self._noise_level * jax.random.normal(hk.next_rng_key(), shape=x.shape)\n        inputs = jax.tree_map(add_noise, inputs)\n    (flat_targets, target_treedef) = _get_flat_arrays_and_single_timestep_treedef(targets)\n    scan_variables = flat_targets\n    (flat_forcings, forcings_treedef) = _get_flat_arrays_and_single_timestep_treedef(forcings)\n    scan_variables = (flat_targets, flat_forcings)\n\n    def one_step_loss(inputs, scan_variables):\n        (flat_target, flat_forcings) = scan_variables\n        forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, targets.coords['time'][:1])\n        target = _unflatten_and_expand_time(flat_target, target_treedef, targets.coords['time'][:1])\n        all_inputs = xarray.merge([constant_inputs, inputs])\n        ((loss, diagnostics), predictions) = self._predictor.loss_and_predictions(all_inputs, target, forcings=forcings, **kwargs)\n        (loss, diagnostics) = xarray_tree.map_structure(xarray_jax.unwrap_data, (loss, diagnostics))\n        predictions = cast(xarray.Dataset, predictions)\n        next_frame = xarray.merge([predictions, forcings])\n        next_inputs = self._update_inputs(inputs, next_frame)\n        return (next_inputs, (loss, diagnostics))\n    if self._gradient_checkpointing:\n        scan_length = targets.dims['time']\n        if scan_length <= 1:\n            logging.warning('Skipping gradient checkpointing for sequence length of 1')\n        else:\n            one_step_loss = hk.remat(one_step_loss)\n    (_, (per_timestep_losses, per_timestep_diagnostics)) = hk.scan(one_step_loss, inputs, scan_variables)\n    (loss, diagnostics) = jax.tree_util.tree_map(lambda x: xarray_jax.DataArray(x, dims=('time', 'batch')).mean('time', skipna=False), (per_timestep_losses, per_timestep_diagnostics))\n    return (loss, diagnostics)",
    "37": "def one_step_loss(inputs, scan_variables):\n    (flat_target, flat_forcings) = scan_variables\n    forcings = _unflatten_and_expand_time(flat_forcings, forcings_treedef, targets.coords['time'][:1])\n    target = _unflatten_and_expand_time(flat_target, target_treedef, targets.coords['time'][:1])\n    all_inputs = xarray.merge([constant_inputs, inputs])\n    ((loss, diagnostics), predictions) = self._predictor.loss_and_predictions(all_inputs, target, forcings=forcings, **kwargs)\n    (loss, diagnostics) = xarray_tree.map_structure(xarray_jax.unwrap_data, (loss, diagnostics))\n    predictions = cast(xarray.Dataset, predictions)\n    next_frame = xarray.merge([predictions, forcings])\n    next_inputs = self._update_inputs(inputs, next_frame)\n    return (next_inputs, (loss, diagnostics))",
    "38": "class Bfloat16Cast(predictor_base.Predictor):\n    \"\"\"Wrapper that casts all inputs to bfloat16 and outputs to targets dtype.\"\"\"\n\n    def __init__(self, predictor: predictor_base.Predictor, enabled: bool=True):\n        \"\"\"Inits the wrapper.\n\n    Args:\n      predictor: predictor being wrapped.\n      enabled: disables the wrapper if False, for simpler hyperparameter scans.\n\n    \"\"\"\n        self._enabled = enabled\n        self._predictor = predictor\n\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n        if not self._enabled:\n            return self._predictor(inputs, targets_template, forcings, **kwargs)\n        with bfloat16_variable_view():\n            predictions = self._predictor(*_all_inputs_to_bfloat16(inputs, targets_template, forcings), **kwargs)\n        predictions_dtype = infer_floating_dtype(predictions)\n        if predictions_dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n        targets_dtype = infer_floating_dtype(targets_template)\n        return tree_map_cast(predictions, input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n        if not self._enabled:\n            return self._predictor.loss(inputs, targets, forcings, **kwargs)\n        with bfloat16_variable_view():\n            (loss, scalars) = self._predictor.loss(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n        if loss.dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n        targets_dtype = infer_floating_dtype(targets)\n        return tree_map_cast((loss, scalars), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)\n\n    def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n        if not self._enabled:\n            return self._predictor.loss_and_predictions(inputs, targets, forcings, **kwargs)\n        with bfloat16_variable_view():\n            ((loss, scalars), predictions) = self._predictor.loss_and_predictions(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n        if loss.dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n        predictions_dtype = infer_floating_dtype(predictions)\n        if predictions_dtype != jnp.bfloat16:\n            raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n        targets_dtype = infer_floating_dtype(targets)\n        return tree_map_cast(((loss, scalars), predictions), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "39": "def __init__(self, predictor: predictor_base.Predictor, enabled: bool=True):\n    \"\"\"Inits the wrapper.\n\n    Args:\n      predictor: predictor being wrapped.\n      enabled: disables the wrapper if False, for simpler hyperparameter scans.\n\n    \"\"\"\n    self._enabled = enabled\n    self._predictor = predictor",
    "40": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n    if not self._enabled:\n        return self._predictor(inputs, targets_template, forcings, **kwargs)\n    with bfloat16_variable_view():\n        predictions = self._predictor(*_all_inputs_to_bfloat16(inputs, targets_template, forcings), **kwargs)\n    predictions_dtype = infer_floating_dtype(predictions)\n    if predictions_dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n    targets_dtype = infer_floating_dtype(targets_template)\n    return tree_map_cast(predictions, input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "41": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n    if not self._enabled:\n        return self._predictor.loss(inputs, targets, forcings, **kwargs)\n    with bfloat16_variable_view():\n        (loss, scalars) = self._predictor.loss(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n    if loss.dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n    targets_dtype = infer_floating_dtype(targets)\n    return tree_map_cast((loss, scalars), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "42": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    if not self._enabled:\n        return self._predictor.loss_and_predictions(inputs, targets, forcings, **kwargs)\n    with bfloat16_variable_view():\n        ((loss, scalars), predictions) = self._predictor.loss_and_predictions(*_all_inputs_to_bfloat16(inputs, targets, forcings), **kwargs)\n    if loss.dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 loss, got {loss.dtype}')\n    predictions_dtype = infer_floating_dtype(predictions)\n    if predictions_dtype != jnp.bfloat16:\n        raise ValueError(f'Expected bfloat16 output, got {predictions_dtype}')\n    targets_dtype = infer_floating_dtype(targets)\n    return tree_map_cast(((loss, scalars), predictions), input_dtype=jnp.bfloat16, output_dtype=targets_dtype)",
    "43": "def infer_floating_dtype(data_vars: Mapping[str, chex.Array]) -> np.dtype:\n    \"\"\"Infers a floating dtype from an input mapping of data.\"\"\"\n    dtypes = {v.dtype for (k, v) in data_vars.items() if jnp.issubdtype(v.dtype, np.floating)}\n    if len(dtypes) != 1:\n        dtypes_and_shapes = {k: (v.dtype, v.shape) for (k, v) in data_vars.items() if jnp.issubdtype(v.dtype, np.floating)}\n        raise ValueError(f'Did not found exactly one floating dtype {dtypes} in input variables:{dtypes_and_shapes}')\n    return list(dtypes)[0]",
    "45": "def tree_map_cast(inputs: PyTree, input_dtype: np.dtype, output_dtype: np.dtype) -> PyTree:\n\n    def cast_fn(x):\n        if x.dtype == input_dtype:\n            return x.astype(output_dtype)\n    return jax.tree_map(cast_fn, inputs)",
    "47": "@contextlib.contextmanager\ndef bfloat16_variable_view(enabled: bool=True):\n    \"\"\"Context for Haiku modules with float32 params, but bfloat16 activations.\n\n  It works as follows:\n  * Every time a variable is requested to be created/set as np.bfloat16,\n    it will create an underlying float32 variable, instead.\n  * Every time a variable a variable is requested as bfloat16, it will check the\n    variable is of float32 type, and cast the variable to bfloat16.\n\n  Note the gradients are still computed and accumulated as float32, because\n  the params returned by init are float32, so the gradient function with\n  respect to the params will already include an implicit casting to float32.\n\n  Args:\n    enabled: Only enables bfloat16 behavior if True.\n\n  Yields:\n    None\n  \"\"\"\n    if enabled:\n        with hk.custom_creator(_bfloat16_creator, state=True), hk.custom_getter(_bfloat16_getter, state=True), hk.custom_setter(_bfloat16_setter):\n            yield\n    else:\n        yield",
    "49": "def _bfloat16_getter(next_getter, value, context):\n    \"\"\"Casts float32 to bfloat16 when bfloat16 was originally requested.\"\"\"\n    if context.original_dtype == jnp.bfloat16:\n        assert value.dtype == jnp.float32\n        value = value.astype(jnp.bfloat16)\n    return next_getter(value)",
    "51": "def dump(dest: BinaryIO, value: Any) -> None:\n    \"\"\"Dump a tree of dicts/dataclasses to a file object.\n\n  Args:\n    dest: a file object to write to.\n    value: A tree of dicts, lists, tuples and dataclasses of numpy arrays and\n      other basic types. Unions are not supported, other than Optional/None\n      which is only supported in dataclasses, not in dicts, lists or tuples.\n      All leaves must be coercible to a numpy array, and recoverable as a single\n      arg to a type.\n  \"\"\"\n    buffer = io.BytesIO()\n    np.savez(buffer, **_flatten(value))\n    dest.write(buffer.getvalue())",
    "52": "def load(source: BinaryIO, typ: type[_T]) -> _T:\n    \"\"\"Load from a file object and convert it to the specified type.\n\n  Args:\n    source: a file object to read from.\n    typ: a type object that acts as a schema for deserialization. It must match\n      what was serialized. If a type is Any, it will be returned however numpy\n      serialized it, which is what you want for a tree of numpy arrays.\n\n  Returns:\n    the deserialized value as the specified type.\n  \"\"\"\n    return _convert_types(typ, _unflatten(np.load(source)))",
    "53": "def _flatten(tree: Any) -> dict[str, Any]:\n    \"\"\"Flatten a tree of dicts/dataclasses/lists/tuples to a single dict.\"\"\"\n    if dataclasses.is_dataclass(tree):\n        tree = {f.name: v for f in dataclasses.fields(tree) if (v := getattr(tree, f.name)) is not None}\n    elif isinstance(tree, (list, tuple)):\n        tree = dict(enumerate(tree))\n    assert isinstance(tree, dict)\n    flat = {}\n    for (k, v) in tree.items():\n        k = str(k)\n        assert _SEP not in k\n        if dataclasses.is_dataclass(v) or isinstance(v, (dict, list, tuple)):\n            for (a, b) in _flatten(v).items():\n                flat[f'{k}{_SEP}{a}'] = b\n        else:\n            assert v is not None\n            flat[k] = v\n    return flat",
    "54": "def _unflatten(flat: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Unflatten a dict to a tree of dicts.\"\"\"\n    tree = {}\n    for (flat_key, v) in flat.items():\n        node = tree\n        keys = flat_key.split(_SEP)\n        for k in keys[:-1]:\n            if k not in node:\n                node[k] = {}\n            node = node[k]\n        node[keys[-1]] = v\n    return tree",
    "55": "def _convert_types(typ: type[_T], value: Any) -> _T:\n    \"\"\"Convert some structure into the given type. The structures must match.\"\"\"\n    if typ in (Any, ...):\n        return value\n    if typ in (int, float, str, bool):\n        return typ(value)\n    if typ is np.ndarray:\n        assert isinstance(value, np.ndarray)\n        return value\n    if dataclasses.is_dataclass(typ):\n        kwargs = {}\n        for f in dataclasses.fields(typ):\n            if isinstance(f.type, (types.UnionType, type(Optional[int]))):\n                constructors = [t for t in f.type.__args__ if t is not types.NoneType]\n                if len(constructors) != 1:\n                    raise TypeError(\"Optional works, Union with anything except None doesn't\")\n                if f.name not in value:\n                    kwargs[f.name] = None\n                    continue\n                constructor = constructors[0]\n            else:\n                constructor = f.type\n            if f.name in value:\n                kwargs[f.name] = _convert_types(constructor, value[f.name])\n            else:\n                raise ValueError(f'Missing value: {f.name}')\n        return typ(**kwargs)\n    base_type = getattr(typ, '__origin__', None)\n    if base_type is dict:\n        assert len(typ.__args__) == 2\n        (key_type, value_type) = typ.__args__\n        return {_convert_types(key_type, k): _convert_types(value_type, v) for (k, v) in value.items()}\n    if base_type is list:\n        assert len(typ.__args__) == 1\n        value_type = typ.__args__[0]\n        return [_convert_types(value_type, v) for (_, v) in sorted(value.items(), key=lambda x: int(x[0]))]\n    if base_type is tuple:\n        if len(typ.__args__) == 2 and typ.__args__[1] == ...:\n            value_type = typ.__args__[0]\n            return tuple((_convert_types(value_type, v) for (_, v) in sorted(value.items(), key=lambda x: int(x[0]))))\n        else:\n            assert len(typ.__args__) == len(value)\n            return tuple((_convert_types(t, v) for (t, (_, v)) in zip(typ.__args__, sorted(value.items(), key=lambda x: int(x[0])))))\n    try:\n        return typ(value)\n    except TypeError as e:\n        raise TypeError(\"_convert_types expects the type argument to be a dataclass defined with types that are valid constructors (eg tuple is fine, Tuple isn't), and accept a numpy array as the sole argument.\") from e",
    "57": "@dataclasses.dataclass\nclass Config:\n    bt: bool\n    bf: bool\n    i: int\n    f: float\n    o1: Optional[int]\n    o2: Optional[int]\n    o3: Union[int, None]\n    o4: Union[int, None]\n    o5: int | None\n    o6: int | None\n    li: list[int]\n    ls: list[str]\n    ldc: list[SubConfig]\n    tf: tuple[float, ...]\n    ts: tuple[str, ...]\n    t: tuple[str, int, SubConfig]\n    tdc: tuple[SubConfig, ...]\n    dsi: dict[str, int]\n    dss: dict[str, str]\n    dis: dict[int, str]\n    dsdis: dict[str, dict[int, str]]\n    dc: SubConfig\n    dco: Optional[SubConfig]\n    ddc: dict[str, SubConfig]",
    "59": "class DataclassTest(absltest.TestCase):\n\n    def test_serialize_dataclass(self):\n        ckpt = Checkpoint(params={'layer1': {'w': np.arange(10).reshape(2, 5), 'b': np.array([2, 6])}, 'layer2': {'w': np.arange(8).reshape(2, 4), 'b': np.array([2, 6])}, 'blah': np.array([3, 9])}, config=Config(bt=True, bf=False, i=42, f=3.14, o1=1, o2=None, o3=2, o4=None, o5=3, o6=None, li=[12, 9, 7, 15, 16, 14, 1, 6, 11, 4, 10, 5, 13, 3, 8, 2], ls=list('qhjfdxtpzgemryoikwvblcaus'), ldc=[SubConfig(1, 'hello'), SubConfig(2, 'world')], tf=(1, 4, 2, 10, 5, 9, 13, 16, 15, 8, 12, 7, 11, 14, 3, 6), ts=('hello', 'world'), t=('foo', 42, SubConfig(1, 'bar')), tdc=(SubConfig(1, 'hello'), SubConfig(2, 'world')), dsi={'a': 1, 'b': 2, 'c': 3}, dss={'d': 'e', 'f': 'g'}, dis={1: 'a', 2: 'b', 3: 'c'}, dsdis={'a': {1: 'hello', 2: 'world'}, 'b': {1: 'world'}}, dc=SubConfig(1, 'hello'), dco=None, ddc={'a': SubConfig(1, 'hello'), 'b': SubConfig(2, 'world')}))\n        buffer = io.BytesIO()\n        checkpoint.dump(buffer, ckpt)\n        buffer.seek(0)\n        ckpt2 = checkpoint.load(buffer, Checkpoint)\n        np.testing.assert_array_equal(ckpt.params['layer1']['w'], ckpt2.params['layer1']['w'])\n        np.testing.assert_array_equal(ckpt.params['layer1']['b'], ckpt2.params['layer1']['b'])\n        np.testing.assert_array_equal(ckpt.params['layer2']['w'], ckpt2.params['layer2']['w'])\n        np.testing.assert_array_equal(ckpt.params['layer2']['b'], ckpt2.params['layer2']['b'])\n        np.testing.assert_array_equal(ckpt.params['blah'], ckpt2.params['blah'])\n        self.assertEqual(ckpt.config, ckpt2.config)",
    "60": "def test_serialize_dataclass(self):\n    ckpt = Checkpoint(params={'layer1': {'w': np.arange(10).reshape(2, 5), 'b': np.array([2, 6])}, 'layer2': {'w': np.arange(8).reshape(2, 4), 'b': np.array([2, 6])}, 'blah': np.array([3, 9])}, config=Config(bt=True, bf=False, i=42, f=3.14, o1=1, o2=None, o3=2, o4=None, o5=3, o6=None, li=[12, 9, 7, 15, 16, 14, 1, 6, 11, 4, 10, 5, 13, 3, 8, 2], ls=list('qhjfdxtpzgemryoikwvblcaus'), ldc=[SubConfig(1, 'hello'), SubConfig(2, 'world')], tf=(1, 4, 2, 10, 5, 9, 13, 16, 15, 8, 12, 7, 11, 14, 3, 6), ts=('hello', 'world'), t=('foo', 42, SubConfig(1, 'bar')), tdc=(SubConfig(1, 'hello'), SubConfig(2, 'world')), dsi={'a': 1, 'b': 2, 'c': 3}, dss={'d': 'e', 'f': 'g'}, dis={1: 'a', 2: 'b', 3: 'c'}, dsdis={'a': {1: 'hello', 2: 'world'}, 'b': {1: 'world'}}, dc=SubConfig(1, 'hello'), dco=None, ddc={'a': SubConfig(1, 'hello'), 'b': SubConfig(2, 'world')}))\n    buffer = io.BytesIO()\n    checkpoint.dump(buffer, ckpt)\n    buffer.seek(0)\n    ckpt2 = checkpoint.load(buffer, Checkpoint)\n    np.testing.assert_array_equal(ckpt.params['layer1']['w'], ckpt2.params['layer1']['w'])\n    np.testing.assert_array_equal(ckpt.params['layer1']['b'], ckpt2.params['layer1']['b'])\n    np.testing.assert_array_equal(ckpt.params['layer2']['w'], ckpt2.params['layer2']['w'])\n    np.testing.assert_array_equal(ckpt.params['layer2']['b'], ckpt2.params['layer2']['b'])\n    np.testing.assert_array_equal(ckpt.params['blah'], ckpt2.params['blah'])\n    self.assertEqual(ckpt.config, ckpt2.config)",
    "61": "def get_year_progress(seconds_since_epoch: np.ndarray) -> np.ndarray:\n    \"\"\"Computes year progress for times in seconds.\n\n  Args:\n    seconds_since_epoch: Times in seconds since the \"epoch\" (the point at which\n      UNIX time starts).\n\n  Returns:\n    Year progress normalized to be in the [0, 1) interval for each time point.\n  \"\"\"\n    years_since_epoch = seconds_since_epoch / SEC_PER_DAY / np.float64(_AVG_DAY_PER_YEAR)\n    return np.mod(years_since_epoch, 1.0).astype(np.float32)",
    "62": "def get_day_progress(seconds_since_epoch: np.ndarray, longitude: np.ndarray) -> np.ndarray:\n    \"\"\"Computes day progress for times in seconds at each longitude.\n\n  Args:\n    seconds_since_epoch: 1D array of times in seconds since the 'epoch' (the\n      point at which UNIX time starts).\n    longitude: 1D array of longitudes at which day progress is computed.\n\n  Returns:\n    2D array of day progress values normalized to be in the [0, 1) inverval\n      for each time point at each longitude.\n  \"\"\"\n    day_progress_greenwich = np.mod(seconds_since_epoch, SEC_PER_DAY) / SEC_PER_DAY\n    longitude_offsets = np.deg2rad(longitude) / (2 * np.pi)\n    day_progress = np.mod(day_progress_greenwich[..., np.newaxis] + longitude_offsets, 1.0)\n    return day_progress.astype(np.float32)",
    "63": "def featurize_progress(name: str, dims: Sequence[str], progress: np.ndarray) -> Mapping[str, xarray.Variable]:\n    \"\"\"Derives features used by ML models from the `progress` variable.\n\n  Args:\n    name: Base variable name from which features are derived.\n    dims: List of the output feature dimensions, e.g. (\"day\", \"lon\").\n    progress: Progress variable values.\n\n  Returns:\n    Dictionary of xarray variables derived from the `progress` values. It\n    includes the original `progress` variable along with its sin and cos\n    transformations.\n\n  Raises:\n    ValueError if the number of feature dimensions is not equal to the number\n      of data dimensions.\n  \"\"\"\n    if len(dims) != progress.ndim:\n        raise ValueError(f'Number of feature dimensions ({len(dims)}) must be equal to the number of data dimensions: {progress.ndim}.')\n    progress_phase = progress * (2 * np.pi)\n    return {name: xarray.Variable(dims, progress), name + '_sin': xarray.Variable(dims, np.sin(progress_phase)), name + '_cos': xarray.Variable(dims, np.cos(progress_phase))}",
    "64": "def add_derived_vars(data: xarray.Dataset) -> None:\n    \"\"\"Adds year and day progress features to `data` in place if missing.\n\n  Args:\n    data: Xarray dataset to which derived features will be added.\n\n  Raises:\n    ValueError if `datetime` or `lon` are not in `data` coordinates.\n  \"\"\"\n    for coord in ('datetime', 'lon'):\n        if coord not in data.coords:\n            raise ValueError(f\"'{coord}' must be in `data` coordinates.\")\n    seconds_since_epoch = data.coords['datetime'].data.astype('datetime64[s]').astype(np.int64)\n    batch_dim = ('batch',) if 'batch' in data.dims else ()\n    if YEAR_PROGRESS not in data.data_vars:\n        year_progress = get_year_progress(seconds_since_epoch)\n        data.update(featurize_progress(name=YEAR_PROGRESS, dims=batch_dim + ('time',), progress=year_progress))\n    if DAY_PROGRESS not in data.data_vars:\n        longitude_coord = data.coords['lon']\n        day_progress = get_day_progress(seconds_since_epoch, longitude_coord.data)\n        data.update(featurize_progress(name=DAY_PROGRESS, dims=batch_dim + ('time',) + longitude_coord.dims, progress=day_progress))",
    "65": "def add_tisr_var(data: xarray.Dataset) -> None:\n    \"\"\"Adds TISR feature to `data` in place if missing.\n\n  Args:\n    data: Xarray dataset to which TISR feature will be added.\n\n  Raises:\n    ValueError if `datetime`, 'lat', or `lon` are not in `data` coordinates.\n  \"\"\"\n    if TISR in data.data_vars:\n        return\n    for coord in ('datetime', 'lat', 'lon'):\n        if coord not in data.coords:\n            raise ValueError(f\"'{coord}' must be in `data` coordinates.\")\n    data_no_batch = data.squeeze('batch') if 'batch' in data.dims else data\n    tisr = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data_no_batch, use_jit=True)\n    if 'batch' in data.dims:\n        tisr = tisr.expand_dims('batch', axis=0)\n    data.update({TISR: tisr})",
    "66": "def extract_input_target_times(dataset: xarray.Dataset, input_duration: TimedeltaLike, target_lead_times: TargetLeadTimes) -> Tuple[xarray.Dataset, xarray.Dataset]:\n    \"\"\"Extracts inputs and targets for prediction, from a Dataset with a time dim.\n\n  The input period is assumed to be contiguous (specified by a duration), but\n  the targets can be a list of arbitrary lead times.\n\n  Examples:\n\n    # Use 18 hours of data as inputs, and two specific lead times as targets:\n    # 3 days and 5 days after the final input.\n    extract_inputs_targets(\n        dataset,\n        input_duration='18h',\n        target_lead_times=('3d', '5d')\n    )\n\n    # Use 1 day of data as input, and all lead times between 6 hours and\n    # 24 hours inclusive as targets. Demonstrates a friendlier supported string\n    # syntax.\n    extract_inputs_targets(\n        dataset,\n        input_duration='1 day',\n        target_lead_times=slice('6 hours', '24 hours')\n    )\n\n    # Just use a single target lead time of 3 days:\n    extract_inputs_targets(\n        dataset,\n        input_duration='24h',\n        target_lead_times='3d'\n    )\n\n  Args:\n    dataset: An xarray.Dataset with a 'time' dimension whose coordinates are\n      timedeltas. It's assumed that the time coordinates have a fixed offset /\n      time resolution, and that the input_duration and target_lead_times are\n      multiples of this.\n    input_duration: pandas.Timedelta or something convertible to it (e.g. a\n      shorthand string like '6h' or '5d12h').\n    target_lead_times: Either a single lead time, a slice with start and stop\n      (inclusive) lead times, or a sequence of lead times. Lead times should be\n      Timedeltas (or something convertible to). They are given relative to the\n      final input timestep, and should be positive.\n\n  Returns:\n    inputs:\n    targets:\n      Two datasets with the same shape as the input dataset except that a\n      selection has been made from the time axis, and the origin of the\n      time coordinate will be shifted to refer to lead times relative to the\n      final input timestep. So for inputs the times will end at lead time 0,\n      for targets the time coordinates will refer to the lead times requested.\n  \"\"\"\n    (target_lead_times, target_duration) = _process_target_lead_times_and_get_duration(target_lead_times)\n    time = dataset.coords['time']\n    dataset = dataset.assign_coords(time=time + target_duration - time[-1])\n    targets = dataset.sel({'time': target_lead_times})\n    input_duration = pd.Timedelta(input_duration)\n    zero = pd.Timedelta(0)\n    epsilon = pd.Timedelta(1, 'ns')\n    inputs = dataset.sel({'time': slice(-input_duration + epsilon, zero)})\n    return (inputs, targets)",
    "67": "def _process_target_lead_times_and_get_duration(target_lead_times: TargetLeadTimes) -> TimedeltaLike:\n    \"\"\"Returns the minimum duration for the target lead times.\"\"\"\n    if isinstance(target_lead_times, slice):\n        if target_lead_times.start is None:\n            target_lead_times = slice(pd.Timedelta(1, 'ns'), target_lead_times.stop, target_lead_times.step)\n        target_duration = pd.Timedelta(target_lead_times.stop)\n    else:\n        if not isinstance(target_lead_times, (list, tuple, set)):\n            target_lead_times = [target_lead_times]\n        target_lead_times = [pd.Timedelta(x) for x in target_lead_times]\n        target_lead_times.sort()\n        target_duration = target_lead_times[-1]\n    return (target_lead_times, target_duration)",
    "68": "def extract_inputs_targets_forcings(dataset: xarray.Dataset, *, input_variables: Tuple[str, ...], target_variables: Tuple[str, ...], forcing_variables: Tuple[str, ...], pressure_levels: Tuple[int, ...], input_duration: TimedeltaLike, target_lead_times: TargetLeadTimes) -> Tuple[xarray.Dataset, xarray.Dataset, xarray.Dataset]:\n    \"\"\"Extracts inputs, targets and forcings according to requirements.\"\"\"\n    dataset = dataset.sel(level=list(pressure_levels))\n    if set(forcing_variables) & _DERIVED_VARS:\n        add_derived_vars(dataset)\n    if set(forcing_variables) & {TISR}:\n        add_tisr_var(dataset)\n    dataset = dataset.drop_vars('datetime')\n    (inputs, targets) = extract_input_target_times(dataset, input_duration=input_duration, target_lead_times=target_lead_times)\n    if set(forcing_variables) & set(target_variables):\n        raise ValueError(f'Forcing variables {forcing_variables} should not overlap with target variables {target_variables}.')\n    inputs = inputs[list(input_variables)]\n    forcings = targets[list(forcing_variables)]\n    targets = targets[list(target_variables)]\n    return (inputs, targets, forcings)",
    "69": "class DataUtilsTest(parameterized.TestCase):\n\n    def setUp(self):\n        super().setUp()\n        np.random.seed(0)\n\n    def test_year_progress_is_zero_at_year_start_or_end(self):\n        year_progress = data_utils.get_year_progress(np.array([0, data_utils.AVG_SEC_PER_YEAR, data_utils.AVG_SEC_PER_YEAR * 42]))\n        np.testing.assert_array_equal(year_progress, np.zeros(year_progress.shape))\n\n    def test_year_progress_is_almost_one_before_year_ends(self):\n        year_progress = data_utils.get_year_progress(np.array([data_utils.AVG_SEC_PER_YEAR - 1, (data_utils.AVG_SEC_PER_YEAR - 1) * 42]))\n        with self.subTest('Year progress values are close to 1'):\n            self.assertTrue(np.all(year_progress > 0.999))\n        with self.subTest('Year progress values != 1'):\n            self.assertTrue(np.all(year_progress < 1.0))\n\n    def test_day_progress_computes_for_all_times_and_longitudes(self):\n        times = np.random.randint(low=0, high=10000000000.0, size=10)\n        longitudes = np.arange(0, 360.0, 1.0)\n        day_progress = data_utils.get_day_progress(times, longitudes)\n        with self.subTest('Day progress is computed for all times and longinutes'):\n            self.assertSequenceEqual(day_progress.shape, (len(times), len(longitudes)))\n\n    @parameterized.named_parameters(dict(testcase_name='random_date_1', year=1988, month=11, day=7, hour=2, minute=45, second=34), dict(testcase_name='random_date_2', year=2022, month=3, day=12, hour=7, minute=1, second=0))\n    def test_day_progress_is_in_between_zero_and_one(self, year, month, day, hour, minute, second):\n        dt = datetime.datetime(year, month, day, hour, minute, second)\n        epoch_time = datetime.datetime(1970, 1, 1)\n        seconds_since_epoch = np.array([(dt - epoch_time).total_seconds()])\n        longitudes = np.arange(0, 360.0, 1.0)\n        day_progress = data_utils.get_day_progress(seconds_since_epoch, longitudes)\n        with self.subTest('Day progress >= 0'):\n            self.assertTrue(np.all(day_progress >= 0.0))\n        with self.subTest('Day progress < 1'):\n            self.assertTrue(np.all(day_progress < 1.0))\n\n    def test_day_progress_is_zero_at_day_start_or_end(self):\n        day_progress = data_utils.get_day_progress(seconds_since_epoch=np.array([0, data_utils.SEC_PER_DAY, data_utils.SEC_PER_DAY * 42]), longitude=np.array([0.0]))\n        np.testing.assert_array_equal(day_progress, np.zeros(day_progress.shape))\n\n    def test_day_progress_specific_value(self):\n        day_progress = data_utils.get_day_progress(seconds_since_epoch=np.array([123]), longitude=np.array([0.0]))\n        np.testing.assert_array_almost_equal(day_progress, np.array([[0.00142361]]), decimal=6)\n\n    def test_featurize_progress_valid_values_and_dimensions(self):\n        day_progress = np.array([0.0, 0.45, 0.213])\n        feature_dimensions = ('time',)\n        progress_features = data_utils.featurize_progress(name='day_progress', dims=feature_dimensions, progress=day_progress)\n        for feature in progress_features.values():\n            with self.subTest(f'Valid dimensions for {feature}'):\n                self.assertSequenceEqual(feature.dims, feature_dimensions)\n        with self.subTest('Valid values for day_progress'):\n            np.testing.assert_array_equal(day_progress, progress_features['day_progress'].values)\n        with self.subTest('Valid values for day_progress_sin'):\n            np.testing.assert_array_almost_equal(np.array([0.0, 0.30901699, 0.97309851]), progress_features['day_progress_sin'].values, decimal=6)\n        with self.subTest('Valid values for day_progress_cos'):\n            np.testing.assert_array_almost_equal(np.array([1.0, -0.95105652, 0.23038943]), progress_features['day_progress_cos'].values, decimal=6)\n\n    def test_featurize_progress_invalid_dimensions(self):\n        year_progress = np.array([0.0, 0.45, 0.213])\n        feature_dimensions = ('time', 'longitude')\n        with self.assertRaises(ValueError):\n            data_utils.featurize_progress(name='year_progress', dims=feature_dimensions, progress=year_progress)\n\n    def test_add_derived_vars_variables_added(self):\n        data = xa.Dataset(data_vars={'var1': (['x', 'lon', 'datetime'], 8 * np.random.randn(2, 2, 3))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n        data_utils.add_derived_vars(data)\n        all_variables = set(data.variables)\n        with self.subTest('Original value was not removed'):\n            self.assertIn('var1', all_variables)\n        with self.subTest('Year progress feature was added'):\n            self.assertIn(data_utils.YEAR_PROGRESS, all_variables)\n        with self.subTest('Day progress feature was added'):\n            self.assertIn(data_utils.DAY_PROGRESS, all_variables)\n\n    def test_add_derived_vars_existing_vars_not_overridden(self):\n        dims = ['x', 'lon', 'datetime']\n        data = xa.Dataset(data_vars={'var1': (dims, 8 * np.random.randn(2, 2, 3)), data_utils.YEAR_PROGRESS: (dims, np.full((2, 2, 3), 0.111)), data_utils.DAY_PROGRESS: (dims, np.full((2, 2, 3), 0.222))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n        data_utils.add_derived_vars(data)\n        with self.subTest('Year progress feature was not overridden'):\n            np.testing.assert_allclose(data[data_utils.YEAR_PROGRESS], 0.111)\n        with self.subTest('Day progress feature was not overridden'):\n            np.testing.assert_allclose(data[data_utils.DAY_PROGRESS], 0.222)\n\n    @parameterized.named_parameters(dict(testcase_name='missing_datetime', coord_name='lon'), dict(testcase_name='missing_lon', coord_name='datetime'))\n    def test_add_derived_vars_missing_coordinate_raises_value_error(self, coord_name):\n        with self.subTest(f'Missing {coord_name} coordinate'):\n            data = xa.Dataset(data_vars={'var1': (['x', coord_name], 8 * np.random.randn(2, 2))}, coords={coord_name: np.array([0.0, 0.5])})\n            with self.assertRaises(ValueError):\n                data_utils.add_derived_vars(data)\n\n    def test_add_tisr_var_variable_added(self):\n        data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.full((2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n        data_utils.add_tisr_var(data)\n        self.assertIn(data_utils.TISR, set(data.variables))\n\n    def test_add_tisr_var_existing_var_not_overridden(self):\n        dims = ['time', 'lat', 'lon']\n        data = xa.Dataset(data_vars={'var1': (dims, np.full((2, 2, 2), 8.0)), data_utils.TISR: (dims, np.full((2, 2, 2), 1200.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n        data_utils.add_derived_vars(data)\n        np.testing.assert_allclose(data[data_utils.TISR], 1200.0)\n\n    def test_add_tisr_var_works_with_batch_dim_size_one(self):\n        data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((1, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20]], dtype='datetime64[D]'))})\n        data_utils.add_tisr_var(data)\n        self.assertIn(data_utils.TISR, set(data.variables))\n\n    def test_add_tisr_var_fails_with_batch_dim_size_greater_than_one(self):\n        data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((2, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20], [100, 200]], dtype='datetime64[D]'))})\n        with self.assertRaisesRegex(ValueError, 'cannot select a dimension'):\n            data_utils.add_tisr_var(data)",
    "71": "def test_year_progress_is_zero_at_year_start_or_end(self):\n    year_progress = data_utils.get_year_progress(np.array([0, data_utils.AVG_SEC_PER_YEAR, data_utils.AVG_SEC_PER_YEAR * 42]))\n    np.testing.assert_array_equal(year_progress, np.zeros(year_progress.shape))",
    "72": "def test_year_progress_is_almost_one_before_year_ends(self):\n    year_progress = data_utils.get_year_progress(np.array([data_utils.AVG_SEC_PER_YEAR - 1, (data_utils.AVG_SEC_PER_YEAR - 1) * 42]))\n    with self.subTest('Year progress values are close to 1'):\n        self.assertTrue(np.all(year_progress > 0.999))\n    with self.subTest('Year progress values != 1'):\n        self.assertTrue(np.all(year_progress < 1.0))",
    "73": "def test_day_progress_computes_for_all_times_and_longitudes(self):\n    times = np.random.randint(low=0, high=10000000000.0, size=10)\n    longitudes = np.arange(0, 360.0, 1.0)\n    day_progress = data_utils.get_day_progress(times, longitudes)\n    with self.subTest('Day progress is computed for all times and longinutes'):\n        self.assertSequenceEqual(day_progress.shape, (len(times), len(longitudes)))",
    "74": "@parameterized.named_parameters(dict(testcase_name='random_date_1', year=1988, month=11, day=7, hour=2, minute=45, second=34), dict(testcase_name='random_date_2', year=2022, month=3, day=12, hour=7, minute=1, second=0))\ndef test_day_progress_is_in_between_zero_and_one(self, year, month, day, hour, minute, second):\n    dt = datetime.datetime(year, month, day, hour, minute, second)\n    epoch_time = datetime.datetime(1970, 1, 1)\n    seconds_since_epoch = np.array([(dt - epoch_time).total_seconds()])\n    longitudes = np.arange(0, 360.0, 1.0)\n    day_progress = data_utils.get_day_progress(seconds_since_epoch, longitudes)\n    with self.subTest('Day progress >= 0'):\n        self.assertTrue(np.all(day_progress >= 0.0))\n    with self.subTest('Day progress < 1'):\n        self.assertTrue(np.all(day_progress < 1.0))",
    "76": "def test_day_progress_specific_value(self):\n    day_progress = data_utils.get_day_progress(seconds_since_epoch=np.array([123]), longitude=np.array([0.0]))\n    np.testing.assert_array_almost_equal(day_progress, np.array([[0.00142361]]), decimal=6)",
    "77": "def test_featurize_progress_valid_values_and_dimensions(self):\n    day_progress = np.array([0.0, 0.45, 0.213])\n    feature_dimensions = ('time',)\n    progress_features = data_utils.featurize_progress(name='day_progress', dims=feature_dimensions, progress=day_progress)\n    for feature in progress_features.values():\n        with self.subTest(f'Valid dimensions for {feature}'):\n            self.assertSequenceEqual(feature.dims, feature_dimensions)\n    with self.subTest('Valid values for day_progress'):\n        np.testing.assert_array_equal(day_progress, progress_features['day_progress'].values)\n    with self.subTest('Valid values for day_progress_sin'):\n        np.testing.assert_array_almost_equal(np.array([0.0, 0.30901699, 0.97309851]), progress_features['day_progress_sin'].values, decimal=6)\n    with self.subTest('Valid values for day_progress_cos'):\n        np.testing.assert_array_almost_equal(np.array([1.0, -0.95105652, 0.23038943]), progress_features['day_progress_cos'].values, decimal=6)",
    "79": "def test_add_derived_vars_variables_added(self):\n    data = xa.Dataset(data_vars={'var1': (['x', 'lon', 'datetime'], 8 * np.random.randn(2, 2, 3))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n    data_utils.add_derived_vars(data)\n    all_variables = set(data.variables)\n    with self.subTest('Original value was not removed'):\n        self.assertIn('var1', all_variables)\n    with self.subTest('Year progress feature was added'):\n        self.assertIn(data_utils.YEAR_PROGRESS, all_variables)\n    with self.subTest('Day progress feature was added'):\n        self.assertIn(data_utils.DAY_PROGRESS, all_variables)",
    "80": "def test_add_derived_vars_existing_vars_not_overridden(self):\n    dims = ['x', 'lon', 'datetime']\n    data = xa.Dataset(data_vars={'var1': (dims, 8 * np.random.randn(2, 2, 3)), data_utils.YEAR_PROGRESS: (dims, np.full((2, 2, 3), 0.111)), data_utils.DAY_PROGRESS: (dims, np.full((2, 2, 3), 0.222))}, coords={'lon': np.array([0.0, 0.5]), 'datetime': np.array([datetime.datetime(2021, 1, 1), datetime.datetime(2023, 1, 1), datetime.datetime(2023, 1, 3)])})\n    data_utils.add_derived_vars(data)\n    with self.subTest('Year progress feature was not overridden'):\n        np.testing.assert_allclose(data[data_utils.YEAR_PROGRESS], 0.111)\n    with self.subTest('Day progress feature was not overridden'):\n        np.testing.assert_allclose(data[data_utils.DAY_PROGRESS], 0.222)",
    "81": "@parameterized.named_parameters(dict(testcase_name='missing_datetime', coord_name='lon'), dict(testcase_name='missing_lon', coord_name='datetime'))\ndef test_add_derived_vars_missing_coordinate_raises_value_error(self, coord_name):\n    with self.subTest(f'Missing {coord_name} coordinate'):\n        data = xa.Dataset(data_vars={'var1': (['x', coord_name], 8 * np.random.randn(2, 2))}, coords={coord_name: np.array([0.0, 0.5])})\n        with self.assertRaises(ValueError):\n            data_utils.add_derived_vars(data)",
    "82": "def test_add_tisr_var_variable_added(self):\n    data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.full((2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n    data_utils.add_tisr_var(data)\n    self.assertIn(data_utils.TISR, set(data.variables))",
    "83": "def test_add_tisr_var_existing_var_not_overridden(self):\n    dims = ['time', 'lat', 'lon']\n    data = xa.Dataset(data_vars={'var1': (dims, np.full((2, 2, 2), 8.0)), data_utils.TISR: (dims, np.full((2, 2, 2), 1200.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n    data_utils.add_derived_vars(data)\n    np.testing.assert_allclose(data[data_utils.TISR], 1200.0)",
    "84": "def test_add_tisr_var_works_with_batch_dim_size_one(self):\n    data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((1, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20]], dtype='datetime64[D]'))})\n    data_utils.add_tisr_var(data)\n    self.assertIn(data_utils.TISR, set(data.variables))",
    "85": "def test_add_tisr_var_fails_with_batch_dim_size_greater_than_one(self):\n    data = xa.Dataset(data_vars={'var1': (['batch', 'time', 'lat', 'lon'], np.full((2, 2, 2, 2), 8.0))}, coords={'lat': np.array([2.0, 1.0]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable(('batch', 'time'), np.array([[10, 20], [100, 200]], dtype='datetime64[D]'))})\n    with self.assertRaisesRegex(ValueError, 'cannot select a dimension'):\n        data_utils.add_tisr_var(data)",
    "86": "class DeepTypedGraphNet(hk.Module):\n    \"\"\"Deep Graph Neural Network.\n\n  It works with TypedGraphs with typed nodes and edges. It runs message\n  passing on all of the node sets and all of the edge sets in the graph. For\n  each message passing step a `typed_graph_net.InteractionNetwork` is used to\n  update the full TypedGraph by using different MLPs for each of the node sets\n  and each of the edge sets.\n\n  If embed_{nodes,edges} is specified the node/edge features will be embedded\n  into a fixed dimensionality before running the first step of message passing.\n\n  If {node,edge}_output_size the final node/edge features will be embedded into\n  the specified output size.\n\n  This class may be used for shared or unshared message passing:\n  * num_message_passing_steps = N, num_processor_repetitions = 1, gives\n    N layers of message passing with fully unshared weights:\n    [W_1, W_2, ... , W_M] (default)\n  * num_message_passing_steps = 1, num_processor_repetitions = M, gives\n    N layers of message passing with fully shared weights:\n    [W_1] * M\n  * num_message_passing_steps = N, num_processor_repetitions = M, gives\n    M*N layers of message passing with both shared and unshared message passing\n    such that the weights used at each iteration are:\n    [W_1, W_2, ... , W_N] * M\n\n  \"\"\"\n\n    def __init__(self, *, node_latent_size: Mapping[str, int], edge_latent_size: Mapping[str, int], mlp_hidden_size: int, mlp_num_hidden_layers: int, num_message_passing_steps: int, num_processor_repetitions: int=1, embed_nodes: bool=True, embed_edges: bool=True, node_output_size: Optional[Mapping[str, int]]=None, edge_output_size: Optional[Mapping[str, int]]=None, include_sent_messages_in_node_update: bool=False, use_layer_norm: bool=True, activation: str='relu', f32_aggregation: bool=False, aggregate_edges_for_nodes_fn: str='segment_sum', aggregate_normalization: Optional[float]=None, name: str='DeepTypedGraphNet'):\n        \"\"\"Inits the model.\n\n    Args:\n      node_latent_size: Size of the node latent representations.\n      edge_latent_size: Size of the edge latent representations.\n      mlp_hidden_size: Hidden layer size for all MLPs.\n      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n      num_message_passing_steps: Number of unshared message passing steps\n         in the processor steps.\n      num_processor_repetitions: Number of times that the same processor is\n         applied sequencially.\n      embed_nodes: If False, the node embedder will be omitted.\n      embed_edges: If False, the edge embedder will be omitted.\n      node_output_size: Size of the output node representations for\n         each node type. For node types not specified here, the latent node\n         representation from the output of the processor will be returned.\n      edge_output_size: Size of the output edge representations for\n         each edge type. For edge types not specified here, the latent edge\n         representation from the output of the processor will be returned.\n      include_sent_messages_in_node_update: Whether to include pooled sent\n          messages from each node in the node update.\n      use_layer_norm: Whether it uses layer norm or not.\n      activation: name of activation function.\n      f32_aggregation: Use float32 in the edge aggregation.\n      aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n        node.\n      aggregate_normalization: An optional constant that normalizes the output\n        of aggregate_edges_for_nodes_fn. For context, this can be used to\n        reduce the shock the model undergoes when switching resolution, which\n        increase the number of edges connected to a node. In particular, this is\n        useful when using segment_sum, but should not be combined with\n        segment_mean.\n      name: Name of the model.\n    \"\"\"\n        super().__init__(name=name)\n        self._node_latent_size = node_latent_size\n        self._edge_latent_size = edge_latent_size\n        self._mlp_hidden_size = mlp_hidden_size\n        self._mlp_num_hidden_layers = mlp_num_hidden_layers\n        self._num_message_passing_steps = num_message_passing_steps\n        self._num_processor_repetitions = num_processor_repetitions\n        self._embed_nodes = embed_nodes\n        self._embed_edges = embed_edges\n        self._node_output_size = node_output_size\n        self._edge_output_size = edge_output_size\n        self._include_sent_messages_in_node_update = include_sent_messages_in_node_update\n        self._use_layer_norm = use_layer_norm\n        self._activation = _get_activation_fn(activation)\n        self._initialized = False\n        self._f32_aggregation = f32_aggregation\n        self._aggregate_edges_for_nodes_fn = _get_aggregate_edges_for_nodes_fn(aggregate_edges_for_nodes_fn)\n        self._aggregate_normalization = aggregate_normalization\n        if aggregate_normalization:\n            assert aggregate_edges_for_nodes_fn == 'segment_sum'\n\n    def __call__(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n        self._networks_builder(input_graph)\n        latent_graph_0 = self._embed(input_graph)\n        latent_graph_m = self._process(latent_graph_0)\n        return self._output(latent_graph_m)\n\n    def _networks_builder(self, graph_template):\n        if self._initialized:\n            return\n        self._initialized = True\n\n        def build_mlp(name, output_size):\n            mlp = hk.nets.MLP(output_sizes=[self._mlp_hidden_size] * self._mlp_num_hidden_layers + [output_size], name=name + '_mlp', activation=self._activation)\n            return jraph.concatenated_args(mlp)\n\n        def build_mlp_with_maybe_layer_norm(name, output_size):\n            network = build_mlp(name, output_size)\n            if self._use_layer_norm:\n                layer_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True, name=name + '_layer_norm')\n                network = hk.Sequential([network, layer_norm])\n            return jraph.concatenated_args(network)\n        if self._embed_edges:\n            embed_edge_fn = _build_update_fns_for_edge_types(build_mlp_with_maybe_layer_norm, graph_template, 'encoder_edges_', output_sizes=self._edge_latent_size)\n        else:\n            embed_edge_fn = None\n        if self._embed_nodes:\n            embed_node_fn = _build_update_fns_for_node_types(build_mlp_with_maybe_layer_norm, graph_template, 'encoder_nodes_', output_sizes=self._node_latent_size)\n        else:\n            embed_node_fn = None\n        embedder_kwargs = dict(embed_edge_fn=embed_edge_fn, embed_node_fn=embed_node_fn)\n        self._embedder_network = typed_graph_net.GraphMapFeatures(**embedder_kwargs)\n        if self._f32_aggregation:\n\n            def aggregate_fn(data, *args, **kwargs):\n                dtype = data.dtype\n                data = data.astype(jnp.float32)\n                output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n                if self._aggregate_normalization:\n                    output = output / self._aggregate_normalization\n                output = output.astype(dtype)\n                return output\n        else:\n\n            def aggregate_fn(data, *args, **kwargs):\n                output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n                if self._aggregate_normalization:\n                    output = output / self._aggregate_normalization\n                return output\n        self._processor_networks = []\n        for step_i in range(self._num_message_passing_steps):\n            self._processor_networks.append(typed_graph_net.InteractionNetwork(update_edge_fn=_build_update_fns_for_edge_types(build_mlp_with_maybe_layer_norm, graph_template, f'processor_edges_{step_i}_', output_sizes=self._edge_latent_size), update_node_fn=_build_update_fns_for_node_types(build_mlp_with_maybe_layer_norm, graph_template, f'processor_nodes_{step_i}_', output_sizes=self._node_latent_size), aggregate_edges_for_nodes_fn=aggregate_fn, include_sent_messages_in_node_update=self._include_sent_messages_in_node_update))\n        output_kwargs = dict(embed_edge_fn=_build_update_fns_for_edge_types(build_mlp, graph_template, 'decoder_edges_', self._edge_output_size) if self._edge_output_size else None, embed_node_fn=_build_update_fns_for_node_types(build_mlp, graph_template, 'decoder_nodes_', self._node_output_size) if self._node_output_size else None)\n        self._output_network = typed_graph_net.GraphMapFeatures(**output_kwargs)\n\n    def _embed(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Embeds the input graph features into a latent graph.\"\"\"\n        context_features = input_graph.context.features\n        if jax.tree_util.tree_leaves(context_features):\n            assert len(jax.tree_util.tree_leaves(context_features)) == 1\n            new_nodes = {}\n            for (node_set_name, node_set) in input_graph.nodes.items():\n                node_features = node_set.features\n                broadcasted_context = jnp.repeat(context_features, node_set.n_node, axis=0, total_repeat_length=node_features.shape[0])\n                new_nodes[node_set_name] = node_set._replace(features=jnp.concatenate([node_features, broadcasted_context], axis=-1))\n            input_graph = input_graph._replace(nodes=new_nodes, context=input_graph.context._replace(features=()))\n        latent_graph_0 = self._embedder_network(input_graph)\n        return latent_graph_0\n\n    def _process(self, latent_graph_0: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n        latent_graph = latent_graph_0\n        for unused_repetition_i in range(self._num_processor_repetitions):\n            for processor_network in self._processor_networks:\n                latent_graph = self._process_step(processor_network, latent_graph)\n        return latent_graph\n\n    def _process_step(self, processor_network_k, latent_graph_prev_k: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n        latent_graph_k = processor_network_k(latent_graph_prev_k)\n        nodes_with_residuals = {}\n        for (k, prev_set) in latent_graph_prev_k.nodes.items():\n            nodes_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.nodes[k].features)\n        edges_with_residuals = {}\n        for (k, prev_set) in latent_graph_prev_k.edges.items():\n            edges_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.edges[k].features)\n        latent_graph_k = latent_graph_k._replace(nodes=nodes_with_residuals, edges=edges_with_residuals)\n        return latent_graph_k\n\n    def _output(self, latent_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Produces the output from the latent graph.\"\"\"\n        return self._output_network(latent_graph)",
    "87": "def __init__(self, *, node_latent_size: Mapping[str, int], edge_latent_size: Mapping[str, int], mlp_hidden_size: int, mlp_num_hidden_layers: int, num_message_passing_steps: int, num_processor_repetitions: int=1, embed_nodes: bool=True, embed_edges: bool=True, node_output_size: Optional[Mapping[str, int]]=None, edge_output_size: Optional[Mapping[str, int]]=None, include_sent_messages_in_node_update: bool=False, use_layer_norm: bool=True, activation: str='relu', f32_aggregation: bool=False, aggregate_edges_for_nodes_fn: str='segment_sum', aggregate_normalization: Optional[float]=None, name: str='DeepTypedGraphNet'):\n    \"\"\"Inits the model.\n\n    Args:\n      node_latent_size: Size of the node latent representations.\n      edge_latent_size: Size of the edge latent representations.\n      mlp_hidden_size: Hidden layer size for all MLPs.\n      mlp_num_hidden_layers: Number of hidden layers in all MLPs.\n      num_message_passing_steps: Number of unshared message passing steps\n         in the processor steps.\n      num_processor_repetitions: Number of times that the same processor is\n         applied sequencially.\n      embed_nodes: If False, the node embedder will be omitted.\n      embed_edges: If False, the edge embedder will be omitted.\n      node_output_size: Size of the output node representations for\n         each node type. For node types not specified here, the latent node\n         representation from the output of the processor will be returned.\n      edge_output_size: Size of the output edge representations for\n         each edge type. For edge types not specified here, the latent edge\n         representation from the output of the processor will be returned.\n      include_sent_messages_in_node_update: Whether to include pooled sent\n          messages from each node in the node update.\n      use_layer_norm: Whether it uses layer norm or not.\n      activation: name of activation function.\n      f32_aggregation: Use float32 in the edge aggregation.\n      aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n        node.\n      aggregate_normalization: An optional constant that normalizes the output\n        of aggregate_edges_for_nodes_fn. For context, this can be used to\n        reduce the shock the model undergoes when switching resolution, which\n        increase the number of edges connected to a node. In particular, this is\n        useful when using segment_sum, but should not be combined with\n        segment_mean.\n      name: Name of the model.\n    \"\"\"\n    super().__init__(name=name)\n    self._node_latent_size = node_latent_size\n    self._edge_latent_size = edge_latent_size\n    self._mlp_hidden_size = mlp_hidden_size\n    self._mlp_num_hidden_layers = mlp_num_hidden_layers\n    self._num_message_passing_steps = num_message_passing_steps\n    self._num_processor_repetitions = num_processor_repetitions\n    self._embed_nodes = embed_nodes\n    self._embed_edges = embed_edges\n    self._node_output_size = node_output_size\n    self._edge_output_size = edge_output_size\n    self._include_sent_messages_in_node_update = include_sent_messages_in_node_update\n    self._use_layer_norm = use_layer_norm\n    self._activation = _get_activation_fn(activation)\n    self._initialized = False\n    self._f32_aggregation = f32_aggregation\n    self._aggregate_edges_for_nodes_fn = _get_aggregate_edges_for_nodes_fn(aggregate_edges_for_nodes_fn)\n    self._aggregate_normalization = aggregate_normalization\n    if aggregate_normalization:\n        assert aggregate_edges_for_nodes_fn == 'segment_sum'",
    "88": "def __call__(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Forward pass of the learnable dynamics model.\"\"\"\n    self._networks_builder(input_graph)\n    latent_graph_0 = self._embed(input_graph)\n    latent_graph_m = self._process(latent_graph_0)\n    return self._output(latent_graph_m)",
    "89": "def _networks_builder(self, graph_template):\n    if self._initialized:\n        return\n    self._initialized = True\n\n    def build_mlp(name, output_size):\n        mlp = hk.nets.MLP(output_sizes=[self._mlp_hidden_size] * self._mlp_num_hidden_layers + [output_size], name=name + '_mlp', activation=self._activation)\n        return jraph.concatenated_args(mlp)\n\n    def build_mlp_with_maybe_layer_norm(name, output_size):\n        network = build_mlp(name, output_size)\n        if self._use_layer_norm:\n            layer_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True, name=name + '_layer_norm')\n            network = hk.Sequential([network, layer_norm])\n        return jraph.concatenated_args(network)\n    if self._embed_edges:\n        embed_edge_fn = _build_update_fns_for_edge_types(build_mlp_with_maybe_layer_norm, graph_template, 'encoder_edges_', output_sizes=self._edge_latent_size)\n    else:\n        embed_edge_fn = None\n    if self._embed_nodes:\n        embed_node_fn = _build_update_fns_for_node_types(build_mlp_with_maybe_layer_norm, graph_template, 'encoder_nodes_', output_sizes=self._node_latent_size)\n    else:\n        embed_node_fn = None\n    embedder_kwargs = dict(embed_edge_fn=embed_edge_fn, embed_node_fn=embed_node_fn)\n    self._embedder_network = typed_graph_net.GraphMapFeatures(**embedder_kwargs)\n    if self._f32_aggregation:\n\n        def aggregate_fn(data, *args, **kwargs):\n            dtype = data.dtype\n            data = data.astype(jnp.float32)\n            output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n            if self._aggregate_normalization:\n                output = output / self._aggregate_normalization\n            output = output.astype(dtype)\n            return output\n    else:\n\n        def aggregate_fn(data, *args, **kwargs):\n            output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n            if self._aggregate_normalization:\n                output = output / self._aggregate_normalization\n            return output\n    self._processor_networks = []\n    for step_i in range(self._num_message_passing_steps):\n        self._processor_networks.append(typed_graph_net.InteractionNetwork(update_edge_fn=_build_update_fns_for_edge_types(build_mlp_with_maybe_layer_norm, graph_template, f'processor_edges_{step_i}_', output_sizes=self._edge_latent_size), update_node_fn=_build_update_fns_for_node_types(build_mlp_with_maybe_layer_norm, graph_template, f'processor_nodes_{step_i}_', output_sizes=self._node_latent_size), aggregate_edges_for_nodes_fn=aggregate_fn, include_sent_messages_in_node_update=self._include_sent_messages_in_node_update))\n    output_kwargs = dict(embed_edge_fn=_build_update_fns_for_edge_types(build_mlp, graph_template, 'decoder_edges_', self._edge_output_size) if self._edge_output_size else None, embed_node_fn=_build_update_fns_for_node_types(build_mlp, graph_template, 'decoder_nodes_', self._node_output_size) if self._node_output_size else None)\n    self._output_network = typed_graph_net.GraphMapFeatures(**output_kwargs)",
    "91": "def build_mlp_with_maybe_layer_norm(name, output_size):\n    network = build_mlp(name, output_size)\n    if self._use_layer_norm:\n        layer_norm = hk.LayerNorm(axis=-1, create_scale=True, create_offset=True, name=name + '_layer_norm')\n        network = hk.Sequential([network, layer_norm])\n    return jraph.concatenated_args(network)",
    "92": "def aggregate_fn(data, *args, **kwargs):\n    dtype = data.dtype\n    data = data.astype(jnp.float32)\n    output = self._aggregate_edges_for_nodes_fn(data, *args, **kwargs)\n    if self._aggregate_normalization:\n        output = output / self._aggregate_normalization\n    output = output.astype(dtype)\n    return output",
    "94": "def _embed(self, input_graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Embeds the input graph features into a latent graph.\"\"\"\n    context_features = input_graph.context.features\n    if jax.tree_util.tree_leaves(context_features):\n        assert len(jax.tree_util.tree_leaves(context_features)) == 1\n        new_nodes = {}\n        for (node_set_name, node_set) in input_graph.nodes.items():\n            node_features = node_set.features\n            broadcasted_context = jnp.repeat(context_features, node_set.n_node, axis=0, total_repeat_length=node_features.shape[0])\n            new_nodes[node_set_name] = node_set._replace(features=jnp.concatenate([node_features, broadcasted_context], axis=-1))\n        input_graph = input_graph._replace(nodes=new_nodes, context=input_graph.context._replace(features=()))\n    latent_graph_0 = self._embedder_network(input_graph)\n    return latent_graph_0",
    "95": "def _process(self, latent_graph_0: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Processes the latent graph with several steps of message passing.\"\"\"\n    latent_graph = latent_graph_0\n    for unused_repetition_i in range(self._num_processor_repetitions):\n        for processor_network in self._processor_networks:\n            latent_graph = self._process_step(processor_network, latent_graph)\n    return latent_graph",
    "96": "def _process_step(self, processor_network_k, latent_graph_prev_k: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Single step of message passing with node/edge residual connections.\"\"\"\n    latent_graph_k = processor_network_k(latent_graph_prev_k)\n    nodes_with_residuals = {}\n    for (k, prev_set) in latent_graph_prev_k.nodes.items():\n        nodes_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.nodes[k].features)\n    edges_with_residuals = {}\n    for (k, prev_set) in latent_graph_prev_k.edges.items():\n        edges_with_residuals[k] = prev_set._replace(features=prev_set.features + latent_graph_k.edges[k].features)\n    latent_graph_k = latent_graph_k._replace(nodes=nodes_with_residuals, edges=edges_with_residuals)\n    return latent_graph_k",
    "98": "def _build_update_fns_for_node_types(builder_fn, graph_template, prefix, output_sizes=None):\n    \"\"\"Builds an update function for all node types or a subset of them.\"\"\"\n    output_fns = {}\n    for node_set_name in graph_template.nodes.keys():\n        if output_sizes is None:\n            output_size = None\n        elif node_set_name in output_sizes:\n            output_size = output_sizes[node_set_name]\n        else:\n            continue\n        output_fns[node_set_name] = builder_fn(f'{prefix}{node_set_name}', output_size)\n    return output_fns",
    "99": "def _build_update_fns_for_edge_types(builder_fn, graph_template, prefix, output_sizes=None):\n    \"\"\"Builds an edge function for all node types or a subset of them.\"\"\"\n    output_fns = {}\n    for edge_set_key in graph_template.edges.keys():\n        edge_set_name = edge_set_key.name\n        if output_sizes is None:\n            output_size = None\n        elif edge_set_name in output_sizes:\n            output_size = output_sizes[edge_set_name]\n        else:\n            continue\n        output_fns[edge_set_name] = builder_fn(f'{prefix}{edge_set_name}', output_size)\n    return output_fns",
    "100": "def _get_activation_fn(name):\n    \"\"\"Return activation function corresponding to function_name.\"\"\"\n    if name == 'identity':\n        return lambda x: x\n    if hasattr(jax.nn, name):\n        return getattr(jax.nn, name)\n    if hasattr(jnp, name):\n        return getattr(jnp, name)\n    raise ValueError(f'Unknown activation function {name} specified.')",
    "102": "@chex.dataclass(frozen=True, eq=True)\nclass TaskConfig:\n    \"\"\"Defines inputs and targets on which a model is trained and/or evaluated.\"\"\"\n    input_variables: tuple[str, ...]\n    target_variables: tuple[str, ...]\n    forcing_variables: tuple[str, ...]\n    pressure_levels: tuple[int, ...]\n    input_duration: str",
    "103": "@chex.dataclass(frozen=True, eq=True)\nclass ModelConfig:\n    \"\"\"Defines the architecture of the GraphCast neural network architecture.\n\n  Properties:\n    resolution: The resolution of the data, in degrees (e.g. 0.25 or 1.0).\n    mesh_size: How many refinements to do on the multi-mesh.\n    gnn_msg_steps: How many Graph Network message passing steps to do.\n    latent_size: How many latent features to include in the various MLPs.\n    hidden_layers: How many hidden layers for each MLP.\n    radius_query_fraction_edge_length: Scalar that will be multiplied by the\n        length of the longest edge of the finest mesh to define the radius of\n        connectivity to use in the Grid2Mesh graph. Reasonable values are\n        between 0.6 and 1. 0.6 reduces the number of grid points feeding into\n        multiple mesh nodes and therefore reduces edge count and memory use, but\n        1 gives better predictions.\n    mesh2grid_edge_normalization_factor: Allows explicitly controlling edge\n        normalization for mesh2grid edges. If None, defaults to max edge length.\n        This supports using pre-trained model weights with a different graph\n        structure to what it was trained on.\n  \"\"\"\n    resolution: float\n    mesh_size: int\n    latent_size: int\n    gnn_msg_steps: int\n    hidden_layers: int\n    radius_query_fraction_edge_length: float\n    mesh2grid_edge_normalization_factor: Optional[float] = None",
    "105": "class GraphCast(predictor_base.Predictor):\n    \"\"\"GraphCast Predictor.\n\n  The model works on graphs that take into account:\n  * Mesh nodes: nodes for the vertices of the mesh.\n  * Grid nodes: nodes for the points of the grid.\n  * Nodes: When referring to just \"nodes\", this means the joint set of\n    both mesh nodes, concatenated with grid nodes.\n\n  The model works with 3 graphs:\n  * Grid2Mesh graph: Graph that contains all nodes. This graph is strictly\n    bipartite with edges going from grid nodes to mesh nodes using a\n    fixed radius query. The grid2mesh_gnn will operate in this graph. The output\n    of this stage will be a latent representation for the mesh nodes, and a\n    latent representation for the grid nodes.\n  * Mesh graph: Graph that contains mesh nodes only. The mesh_gnn will\n    operate in this graph. It will update the latent state of the mesh nodes\n    only.\n  * Mesh2Grid graph: Graph that contains all nodes. This graph is strictly\n    bipartite with edges going from mesh nodes to grid nodes such that each grid\n    nodes is connected to 3 nodes of the mesh triangular face that contains\n    the grid points. The mesh2grid_gnn will operate in this graph. It will\n    process the updated latent state of the mesh nodes, and the latent state\n    of the grid nodes, to produce the final output for the grid nodes.\n\n  The model is built on top of `TypedGraph`s so the different types of nodes and\n  edges can be stored and treated separately.\n\n  \"\"\"\n\n    def __init__(self, model_config: ModelConfig, task_config: TaskConfig):\n        \"\"\"Initializes the predictor.\"\"\"\n        self._spatial_features_kwargs = dict(add_node_positions=False, add_node_latitude=True, add_node_longitude=True, add_relative_positions=True, relative_longitude_local_coordinates=True, relative_latitude_local_coordinates=True)\n        self._meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=model_config.mesh_size)\n        self._grid2mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(embed_nodes=True, embed_edges=True, edge_latent_size=dict(grid2mesh=model_config.latent_size), node_latent_size=dict(mesh_nodes=model_config.latent_size, grid_nodes=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=1, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=True, aggregate_normalization=None, name='grid2mesh_gnn')\n        self._mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(embed_nodes=False, embed_edges=True, node_latent_size=dict(mesh_nodes=model_config.latent_size), edge_latent_size=dict(mesh=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=model_config.gnn_msg_steps, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=False, name='mesh_gnn')\n        num_surface_vars = len(set(task_config.target_variables) - set(ALL_ATMOSPHERIC_VARS))\n        num_atmospheric_vars = len(set(task_config.target_variables) & set(ALL_ATMOSPHERIC_VARS))\n        num_outputs = num_surface_vars + len(task_config.pressure_levels) * num_atmospheric_vars\n        self._mesh2grid_gnn = deep_typed_graph_net.DeepTypedGraphNet(node_output_size=dict(grid_nodes=num_outputs), embed_nodes=False, embed_edges=True, edge_latent_size=dict(mesh2grid=model_config.latent_size), node_latent_size=dict(mesh_nodes=model_config.latent_size, grid_nodes=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=1, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=False, name='mesh2grid_gnn')\n        self._query_radius = _get_max_edge_distance(self._finest_mesh) * model_config.radius_query_fraction_edge_length\n        self._mesh2grid_edge_normalization_factor = model_config.mesh2grid_edge_normalization_factor\n        self._initialized = False\n        self._num_mesh_nodes = None\n        self._mesh_nodes_lat = None\n        self._mesh_nodes_lon = None\n        self._grid_lat = None\n        self._grid_lon = None\n        self._num_grid_nodes = None\n        self._grid_nodes_lat = None\n        self._grid_nodes_lon = None\n        self._grid2mesh_graph_structure = None\n        self._mesh_graph_structure = None\n        self._mesh2grid_graph_structure = None\n\n    @property\n    def _finest_mesh(self):\n        return self._meshes[-1]\n\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, is_training: bool=False) -> xarray.Dataset:\n        self._maybe_init(inputs)\n        grid_node_features = self._inputs_to_grid_node_features(inputs, forcings)\n        (latent_mesh_nodes, latent_grid_nodes) = self._run_grid2mesh_gnn(grid_node_features)\n        updated_latent_mesh_nodes = self._run_mesh_gnn(latent_mesh_nodes)\n        output_grid_nodes = self._run_mesh2grid_gnn(updated_latent_mesh_nodes, latent_grid_nodes)\n        return self._grid_node_outputs_to_prediction(output_grid_nodes, targets_template)\n\n    def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset) -> tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n        predictions = self(inputs, targets_template=targets, forcings=forcings, is_training=True)\n        loss = losses.weighted_mse_per_level(predictions, targets, per_variable_weights={'2m_temperature': 1.0, '10m_u_component_of_wind': 0.1, '10m_v_component_of_wind': 0.1, 'mean_sea_level_pressure': 0.1, 'total_precipitation_6hr': 0.1})\n        return (loss, predictions)\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset) -> predictor_base.LossAndDiagnostics:\n        (loss, _) = self.loss_and_predictions(inputs, targets, forcings)\n        return loss\n\n    def _maybe_init(self, sample_inputs: xarray.Dataset):\n        \"\"\"Inits everything that has a dependency on the input coordinates.\"\"\"\n        if not self._initialized:\n            self._init_mesh_properties()\n            self._init_grid_properties(grid_lat=sample_inputs.lat, grid_lon=sample_inputs.lon)\n            self._grid2mesh_graph_structure = self._init_grid2mesh_graph()\n            self._mesh_graph_structure = self._init_mesh_graph()\n            self._mesh2grid_graph_structure = self._init_mesh2grid_graph()\n            self._initialized = True\n\n    def _init_mesh_properties(self):\n        \"\"\"Inits static properties that have to do with mesh nodes.\"\"\"\n        self._num_mesh_nodes = self._finest_mesh.vertices.shape[0]\n        (mesh_phi, mesh_theta) = model_utils.cartesian_to_spherical(self._finest_mesh.vertices[:, 0], self._finest_mesh.vertices[:, 1], self._finest_mesh.vertices[:, 2])\n        (mesh_nodes_lat, mesh_nodes_lon) = model_utils.spherical_to_lat_lon(phi=mesh_phi, theta=mesh_theta)\n        self._mesh_nodes_lat = mesh_nodes_lat.astype(np.float32)\n        self._mesh_nodes_lon = mesh_nodes_lon.astype(np.float32)\n\n    def _init_grid_properties(self, grid_lat: np.ndarray, grid_lon: np.ndarray):\n        \"\"\"Inits static properties that have to do with grid nodes.\"\"\"\n        self._grid_lat = grid_lat.astype(np.float32)\n        self._grid_lon = grid_lon.astype(np.float32)\n        self._num_grid_nodes = grid_lat.shape[0] * grid_lon.shape[0]\n        (grid_nodes_lon, grid_nodes_lat) = np.meshgrid(grid_lon, grid_lat)\n        self._grid_nodes_lon = grid_nodes_lon.reshape([-1]).astype(np.float32)\n        self._grid_nodes_lat = grid_nodes_lat.reshape([-1]).astype(np.float32)\n\n    def _init_grid2mesh_graph(self) -> typed_graph.TypedGraph:\n        \"\"\"Build Grid2Mesh graph.\"\"\"\n        assert self._grid_lat is not None and self._grid_lon is not None\n        (grid_indices, mesh_indices) = grid_mesh_connectivity.radius_query_indices(grid_latitude=self._grid_lat, grid_longitude=self._grid_lon, mesh=self._finest_mesh, radius=self._query_radius)\n        senders = grid_indices\n        receivers = mesh_indices\n        (senders_node_features, receivers_node_features, edge_features) = model_utils.get_bipartite_graph_spatial_features(senders_node_lat=self._grid_nodes_lat, senders_node_lon=self._grid_nodes_lon, receivers_node_lat=self._mesh_nodes_lat, receivers_node_lon=self._mesh_nodes_lon, senders=senders, receivers=receivers, edge_normalization_factor=None, **self._spatial_features_kwargs)\n        n_grid_node = np.array([self._num_grid_nodes])\n        n_mesh_node = np.array([self._num_mesh_nodes])\n        n_edge = np.array([mesh_indices.shape[0]])\n        grid_node_set = typed_graph.NodeSet(n_node=n_grid_node, features=senders_node_features)\n        mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=receivers_node_features)\n        edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n        nodes = {'grid_nodes': grid_node_set, 'mesh_nodes': mesh_node_set}\n        edges = {typed_graph.EdgeSetKey('grid2mesh', ('grid_nodes', 'mesh_nodes')): edge_set}\n        grid2mesh_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n        return grid2mesh_graph\n\n    def _init_mesh_graph(self) -> typed_graph.TypedGraph:\n        \"\"\"Build Mesh graph.\"\"\"\n        merged_mesh = icosahedral_mesh.merge_meshes(self._meshes)\n        (senders, receivers) = icosahedral_mesh.faces_to_edges(merged_mesh.faces)\n        assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n        (node_features, edge_features) = model_utils.get_graph_spatial_features(node_lat=self._mesh_nodes_lat, node_lon=self._mesh_nodes_lon, senders=senders, receivers=receivers, **self._spatial_features_kwargs)\n        n_mesh_node = np.array([self._num_mesh_nodes])\n        n_edge = np.array([senders.shape[0]])\n        assert n_mesh_node == len(node_features)\n        mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=node_features)\n        edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n        nodes = {'mesh_nodes': mesh_node_set}\n        edges = {typed_graph.EdgeSetKey('mesh', ('mesh_nodes', 'mesh_nodes')): edge_set}\n        mesh_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n        return mesh_graph\n\n    def _init_mesh2grid_graph(self) -> typed_graph.TypedGraph:\n        \"\"\"Build Mesh2Grid graph.\"\"\"\n        (grid_indices, mesh_indices) = grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=self._grid_lat, grid_longitude=self._grid_lon, mesh=self._finest_mesh)\n        senders = mesh_indices\n        receivers = grid_indices\n        assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n        (senders_node_features, receivers_node_features, edge_features) = model_utils.get_bipartite_graph_spatial_features(senders_node_lat=self._mesh_nodes_lat, senders_node_lon=self._mesh_nodes_lon, receivers_node_lat=self._grid_nodes_lat, receivers_node_lon=self._grid_nodes_lon, senders=senders, receivers=receivers, edge_normalization_factor=self._mesh2grid_edge_normalization_factor, **self._spatial_features_kwargs)\n        n_grid_node = np.array([self._num_grid_nodes])\n        n_mesh_node = np.array([self._num_mesh_nodes])\n        n_edge = np.array([senders.shape[0]])\n        grid_node_set = typed_graph.NodeSet(n_node=n_grid_node, features=receivers_node_features)\n        mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=senders_node_features)\n        edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n        nodes = {'grid_nodes': grid_node_set, 'mesh_nodes': mesh_node_set}\n        edges = {typed_graph.EdgeSetKey('mesh2grid', ('mesh_nodes', 'grid_nodes')): edge_set}\n        mesh2grid_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n        return mesh2grid_graph\n\n    def _run_grid2mesh_gnn(self, grid_node_features: chex.Array) -> tuple[chex.Array, chex.Array]:\n        \"\"\"Runs the grid2mesh_gnn, extracting latent mesh and grid nodes.\"\"\"\n        batch_size = grid_node_features.shape[1]\n        grid2mesh_graph = self._grid2mesh_graph_structure\n        assert grid2mesh_graph is not None\n        grid_nodes = grid2mesh_graph.nodes['grid_nodes']\n        mesh_nodes = grid2mesh_graph.nodes['mesh_nodes']\n        new_grid_nodes = grid_nodes._replace(features=jnp.concatenate([grid_node_features, _add_batch_second_axis(grid_nodes.features.astype(grid_node_features.dtype), batch_size)], axis=-1))\n        dummy_mesh_node_features = jnp.zeros((self._num_mesh_nodes,) + grid_node_features.shape[1:], dtype=grid_node_features.dtype)\n        new_mesh_nodes = mesh_nodes._replace(features=jnp.concatenate([dummy_mesh_node_features, _add_batch_second_axis(mesh_nodes.features.astype(dummy_mesh_node_features.dtype), batch_size)], axis=-1))\n        grid2mesh_edges_key = grid2mesh_graph.edge_key_by_name('grid2mesh')\n        edges = grid2mesh_graph.edges[grid2mesh_edges_key]\n        new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(dummy_mesh_node_features.dtype), batch_size))\n        input_graph = self._grid2mesh_graph_structure._replace(edges={grid2mesh_edges_key: new_edges}, nodes={'grid_nodes': new_grid_nodes, 'mesh_nodes': new_mesh_nodes})\n        grid2mesh_out = self._grid2mesh_gnn(input_graph)\n        latent_mesh_nodes = grid2mesh_out.nodes['mesh_nodes'].features\n        latent_grid_nodes = grid2mesh_out.nodes['grid_nodes'].features\n        return (latent_mesh_nodes, latent_grid_nodes)\n\n    def _run_mesh_gnn(self, latent_mesh_nodes: chex.Array) -> chex.Array:\n        \"\"\"Runs the mesh_gnn, extracting updated latent mesh nodes.\"\"\"\n        batch_size = latent_mesh_nodes.shape[1]\n        mesh_graph = self._mesh_graph_structure\n        assert mesh_graph is not None\n        mesh_edges_key = mesh_graph.edge_key_by_name('mesh')\n        edges = mesh_graph.edges[mesh_edges_key]\n        msg = 'The setup currently requires to only have one kind of edge in the mesh GNN.'\n        assert len(mesh_graph.edges) == 1, msg\n        new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(latent_mesh_nodes.dtype), batch_size))\n        nodes = mesh_graph.nodes['mesh_nodes']\n        nodes = nodes._replace(features=latent_mesh_nodes)\n        input_graph = mesh_graph._replace(edges={mesh_edges_key: new_edges}, nodes={'mesh_nodes': nodes})\n        return self._mesh_gnn(input_graph).nodes['mesh_nodes'].features\n\n    def _run_mesh2grid_gnn(self, updated_latent_mesh_nodes: chex.Array, latent_grid_nodes: chex.Array) -> chex.Array:\n        \"\"\"Runs the mesh2grid_gnn, extracting the output grid nodes.\"\"\"\n        batch_size = updated_latent_mesh_nodes.shape[1]\n        mesh2grid_graph = self._mesh2grid_graph_structure\n        assert mesh2grid_graph is not None\n        mesh_nodes = mesh2grid_graph.nodes['mesh_nodes']\n        grid_nodes = mesh2grid_graph.nodes['grid_nodes']\n        new_mesh_nodes = mesh_nodes._replace(features=updated_latent_mesh_nodes)\n        new_grid_nodes = grid_nodes._replace(features=latent_grid_nodes)\n        mesh2grid_key = mesh2grid_graph.edge_key_by_name('mesh2grid')\n        edges = mesh2grid_graph.edges[mesh2grid_key]\n        new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(latent_grid_nodes.dtype), batch_size))\n        input_graph = mesh2grid_graph._replace(edges={mesh2grid_key: new_edges}, nodes={'mesh_nodes': new_mesh_nodes, 'grid_nodes': new_grid_nodes})\n        output_graph = self._mesh2grid_gnn(input_graph)\n        output_grid_nodes = output_graph.nodes['grid_nodes'].features\n        return output_grid_nodes\n\n    def _inputs_to_grid_node_features(self, inputs: xarray.Dataset, forcings: xarray.Dataset) -> chex.Array:\n        \"\"\"xarrays -> [num_grid_nodes, batch, num_channels].\"\"\"\n        stacked_inputs = model_utils.dataset_to_stacked(inputs)\n        stacked_forcings = model_utils.dataset_to_stacked(forcings)\n        stacked_inputs = xarray.concat([stacked_inputs, stacked_forcings], dim='channels')\n        grid_xarray_lat_lon_leading = model_utils.lat_lon_to_leading_axes(stacked_inputs)\n        return xarray_jax.unwrap(grid_xarray_lat_lon_leading.data).reshape((-1,) + grid_xarray_lat_lon_leading.data.shape[2:])\n\n    def _grid_node_outputs_to_prediction(self, grid_node_outputs: chex.Array, targets_template: xarray.Dataset) -> xarray.Dataset:\n        \"\"\"[num_grid_nodes, batch, num_outputs] -> xarray.\"\"\"\n        assert self._grid_lat is not None and self._grid_lon is not None\n        grid_shape = (self._grid_lat.shape[0], self._grid_lon.shape[0])\n        grid_outputs_lat_lon_leading = grid_node_outputs.reshape(grid_shape + grid_node_outputs.shape[1:])\n        dims = ('lat', 'lon', 'batch', 'channels')\n        grid_xarray_lat_lon_leading = xarray_jax.DataArray(data=grid_outputs_lat_lon_leading, dims=dims)\n        grid_xarray = model_utils.restore_leading_axes(grid_xarray_lat_lon_leading)\n        return model_utils.stacked_to_dataset(grid_xarray.variable, targets_template)",
    "106": "def __init__(self, model_config: ModelConfig, task_config: TaskConfig):\n    \"\"\"Initializes the predictor.\"\"\"\n    self._spatial_features_kwargs = dict(add_node_positions=False, add_node_latitude=True, add_node_longitude=True, add_relative_positions=True, relative_longitude_local_coordinates=True, relative_latitude_local_coordinates=True)\n    self._meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=model_config.mesh_size)\n    self._grid2mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(embed_nodes=True, embed_edges=True, edge_latent_size=dict(grid2mesh=model_config.latent_size), node_latent_size=dict(mesh_nodes=model_config.latent_size, grid_nodes=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=1, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=True, aggregate_normalization=None, name='grid2mesh_gnn')\n    self._mesh_gnn = deep_typed_graph_net.DeepTypedGraphNet(embed_nodes=False, embed_edges=True, node_latent_size=dict(mesh_nodes=model_config.latent_size), edge_latent_size=dict(mesh=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=model_config.gnn_msg_steps, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=False, name='mesh_gnn')\n    num_surface_vars = len(set(task_config.target_variables) - set(ALL_ATMOSPHERIC_VARS))\n    num_atmospheric_vars = len(set(task_config.target_variables) & set(ALL_ATMOSPHERIC_VARS))\n    num_outputs = num_surface_vars + len(task_config.pressure_levels) * num_atmospheric_vars\n    self._mesh2grid_gnn = deep_typed_graph_net.DeepTypedGraphNet(node_output_size=dict(grid_nodes=num_outputs), embed_nodes=False, embed_edges=True, edge_latent_size=dict(mesh2grid=model_config.latent_size), node_latent_size=dict(mesh_nodes=model_config.latent_size, grid_nodes=model_config.latent_size), mlp_hidden_size=model_config.latent_size, mlp_num_hidden_layers=model_config.hidden_layers, num_message_passing_steps=1, use_layer_norm=True, include_sent_messages_in_node_update=False, activation='swish', f32_aggregation=False, name='mesh2grid_gnn')\n    self._query_radius = _get_max_edge_distance(self._finest_mesh) * model_config.radius_query_fraction_edge_length\n    self._mesh2grid_edge_normalization_factor = model_config.mesh2grid_edge_normalization_factor\n    self._initialized = False\n    self._num_mesh_nodes = None\n    self._mesh_nodes_lat = None\n    self._mesh_nodes_lon = None\n    self._grid_lat = None\n    self._grid_lon = None\n    self._num_grid_nodes = None\n    self._grid_nodes_lat = None\n    self._grid_nodes_lon = None\n    self._grid2mesh_graph_structure = None\n    self._mesh_graph_structure = None\n    self._mesh2grid_graph_structure = None",
    "108": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, is_training: bool=False) -> xarray.Dataset:\n    self._maybe_init(inputs)\n    grid_node_features = self._inputs_to_grid_node_features(inputs, forcings)\n    (latent_mesh_nodes, latent_grid_nodes) = self._run_grid2mesh_gnn(grid_node_features)\n    updated_latent_mesh_nodes = self._run_mesh_gnn(latent_mesh_nodes)\n    output_grid_nodes = self._run_mesh2grid_gnn(updated_latent_mesh_nodes, latent_grid_nodes)\n    return self._grid_node_outputs_to_prediction(output_grid_nodes, targets_template)",
    "109": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset) -> tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    predictions = self(inputs, targets_template=targets, forcings=forcings, is_training=True)\n    loss = losses.weighted_mse_per_level(predictions, targets, per_variable_weights={'2m_temperature': 1.0, '10m_u_component_of_wind': 0.1, '10m_v_component_of_wind': 0.1, 'mean_sea_level_pressure': 0.1, 'total_precipitation_6hr': 0.1})\n    return (loss, predictions)",
    "111": "def _maybe_init(self, sample_inputs: xarray.Dataset):\n    \"\"\"Inits everything that has a dependency on the input coordinates.\"\"\"\n    if not self._initialized:\n        self._init_mesh_properties()\n        self._init_grid_properties(grid_lat=sample_inputs.lat, grid_lon=sample_inputs.lon)\n        self._grid2mesh_graph_structure = self._init_grid2mesh_graph()\n        self._mesh_graph_structure = self._init_mesh_graph()\n        self._mesh2grid_graph_structure = self._init_mesh2grid_graph()\n        self._initialized = True",
    "112": "def _init_mesh_properties(self):\n    \"\"\"Inits static properties that have to do with mesh nodes.\"\"\"\n    self._num_mesh_nodes = self._finest_mesh.vertices.shape[0]\n    (mesh_phi, mesh_theta) = model_utils.cartesian_to_spherical(self._finest_mesh.vertices[:, 0], self._finest_mesh.vertices[:, 1], self._finest_mesh.vertices[:, 2])\n    (mesh_nodes_lat, mesh_nodes_lon) = model_utils.spherical_to_lat_lon(phi=mesh_phi, theta=mesh_theta)\n    self._mesh_nodes_lat = mesh_nodes_lat.astype(np.float32)\n    self._mesh_nodes_lon = mesh_nodes_lon.astype(np.float32)",
    "113": "def _init_grid_properties(self, grid_lat: np.ndarray, grid_lon: np.ndarray):\n    \"\"\"Inits static properties that have to do with grid nodes.\"\"\"\n    self._grid_lat = grid_lat.astype(np.float32)\n    self._grid_lon = grid_lon.astype(np.float32)\n    self._num_grid_nodes = grid_lat.shape[0] * grid_lon.shape[0]\n    (grid_nodes_lon, grid_nodes_lat) = np.meshgrid(grid_lon, grid_lat)\n    self._grid_nodes_lon = grid_nodes_lon.reshape([-1]).astype(np.float32)\n    self._grid_nodes_lat = grid_nodes_lat.reshape([-1]).astype(np.float32)",
    "114": "def _init_grid2mesh_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Grid2Mesh graph.\"\"\"\n    assert self._grid_lat is not None and self._grid_lon is not None\n    (grid_indices, mesh_indices) = grid_mesh_connectivity.radius_query_indices(grid_latitude=self._grid_lat, grid_longitude=self._grid_lon, mesh=self._finest_mesh, radius=self._query_radius)\n    senders = grid_indices\n    receivers = mesh_indices\n    (senders_node_features, receivers_node_features, edge_features) = model_utils.get_bipartite_graph_spatial_features(senders_node_lat=self._grid_nodes_lat, senders_node_lon=self._grid_nodes_lon, receivers_node_lat=self._mesh_nodes_lat, receivers_node_lon=self._mesh_nodes_lon, senders=senders, receivers=receivers, edge_normalization_factor=None, **self._spatial_features_kwargs)\n    n_grid_node = np.array([self._num_grid_nodes])\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([mesh_indices.shape[0]])\n    grid_node_set = typed_graph.NodeSet(n_node=n_grid_node, features=senders_node_features)\n    mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=receivers_node_features)\n    edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n    nodes = {'grid_nodes': grid_node_set, 'mesh_nodes': mesh_node_set}\n    edges = {typed_graph.EdgeSetKey('grid2mesh', ('grid_nodes', 'mesh_nodes')): edge_set}\n    grid2mesh_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n    return grid2mesh_graph",
    "115": "def _init_mesh_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Mesh graph.\"\"\"\n    merged_mesh = icosahedral_mesh.merge_meshes(self._meshes)\n    (senders, receivers) = icosahedral_mesh.faces_to_edges(merged_mesh.faces)\n    assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n    (node_features, edge_features) = model_utils.get_graph_spatial_features(node_lat=self._mesh_nodes_lat, node_lon=self._mesh_nodes_lon, senders=senders, receivers=receivers, **self._spatial_features_kwargs)\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([senders.shape[0]])\n    assert n_mesh_node == len(node_features)\n    mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=node_features)\n    edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n    nodes = {'mesh_nodes': mesh_node_set}\n    edges = {typed_graph.EdgeSetKey('mesh', ('mesh_nodes', 'mesh_nodes')): edge_set}\n    mesh_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n    return mesh_graph",
    "116": "def _init_mesh2grid_graph(self) -> typed_graph.TypedGraph:\n    \"\"\"Build Mesh2Grid graph.\"\"\"\n    (grid_indices, mesh_indices) = grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=self._grid_lat, grid_longitude=self._grid_lon, mesh=self._finest_mesh)\n    senders = mesh_indices\n    receivers = grid_indices\n    assert self._mesh_nodes_lat is not None and self._mesh_nodes_lon is not None\n    (senders_node_features, receivers_node_features, edge_features) = model_utils.get_bipartite_graph_spatial_features(senders_node_lat=self._mesh_nodes_lat, senders_node_lon=self._mesh_nodes_lon, receivers_node_lat=self._grid_nodes_lat, receivers_node_lon=self._grid_nodes_lon, senders=senders, receivers=receivers, edge_normalization_factor=self._mesh2grid_edge_normalization_factor, **self._spatial_features_kwargs)\n    n_grid_node = np.array([self._num_grid_nodes])\n    n_mesh_node = np.array([self._num_mesh_nodes])\n    n_edge = np.array([senders.shape[0]])\n    grid_node_set = typed_graph.NodeSet(n_node=n_grid_node, features=receivers_node_features)\n    mesh_node_set = typed_graph.NodeSet(n_node=n_mesh_node, features=senders_node_features)\n    edge_set = typed_graph.EdgeSet(n_edge=n_edge, indices=typed_graph.EdgesIndices(senders=senders, receivers=receivers), features=edge_features)\n    nodes = {'grid_nodes': grid_node_set, 'mesh_nodes': mesh_node_set}\n    edges = {typed_graph.EdgeSetKey('mesh2grid', ('mesh_nodes', 'grid_nodes')): edge_set}\n    mesh2grid_graph = typed_graph.TypedGraph(context=typed_graph.Context(n_graph=np.array([1]), features=()), nodes=nodes, edges=edges)\n    return mesh2grid_graph",
    "117": "def _run_grid2mesh_gnn(self, grid_node_features: chex.Array) -> tuple[chex.Array, chex.Array]:\n    \"\"\"Runs the grid2mesh_gnn, extracting latent mesh and grid nodes.\"\"\"\n    batch_size = grid_node_features.shape[1]\n    grid2mesh_graph = self._grid2mesh_graph_structure\n    assert grid2mesh_graph is not None\n    grid_nodes = grid2mesh_graph.nodes['grid_nodes']\n    mesh_nodes = grid2mesh_graph.nodes['mesh_nodes']\n    new_grid_nodes = grid_nodes._replace(features=jnp.concatenate([grid_node_features, _add_batch_second_axis(grid_nodes.features.astype(grid_node_features.dtype), batch_size)], axis=-1))\n    dummy_mesh_node_features = jnp.zeros((self._num_mesh_nodes,) + grid_node_features.shape[1:], dtype=grid_node_features.dtype)\n    new_mesh_nodes = mesh_nodes._replace(features=jnp.concatenate([dummy_mesh_node_features, _add_batch_second_axis(mesh_nodes.features.astype(dummy_mesh_node_features.dtype), batch_size)], axis=-1))\n    grid2mesh_edges_key = grid2mesh_graph.edge_key_by_name('grid2mesh')\n    edges = grid2mesh_graph.edges[grid2mesh_edges_key]\n    new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(dummy_mesh_node_features.dtype), batch_size))\n    input_graph = self._grid2mesh_graph_structure._replace(edges={grid2mesh_edges_key: new_edges}, nodes={'grid_nodes': new_grid_nodes, 'mesh_nodes': new_mesh_nodes})\n    grid2mesh_out = self._grid2mesh_gnn(input_graph)\n    latent_mesh_nodes = grid2mesh_out.nodes['mesh_nodes'].features\n    latent_grid_nodes = grid2mesh_out.nodes['grid_nodes'].features\n    return (latent_mesh_nodes, latent_grid_nodes)",
    "118": "def _run_mesh_gnn(self, latent_mesh_nodes: chex.Array) -> chex.Array:\n    \"\"\"Runs the mesh_gnn, extracting updated latent mesh nodes.\"\"\"\n    batch_size = latent_mesh_nodes.shape[1]\n    mesh_graph = self._mesh_graph_structure\n    assert mesh_graph is not None\n    mesh_edges_key = mesh_graph.edge_key_by_name('mesh')\n    edges = mesh_graph.edges[mesh_edges_key]\n    msg = 'The setup currently requires to only have one kind of edge in the mesh GNN.'\n    assert len(mesh_graph.edges) == 1, msg\n    new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(latent_mesh_nodes.dtype), batch_size))\n    nodes = mesh_graph.nodes['mesh_nodes']\n    nodes = nodes._replace(features=latent_mesh_nodes)\n    input_graph = mesh_graph._replace(edges={mesh_edges_key: new_edges}, nodes={'mesh_nodes': nodes})\n    return self._mesh_gnn(input_graph).nodes['mesh_nodes'].features",
    "119": "def _run_mesh2grid_gnn(self, updated_latent_mesh_nodes: chex.Array, latent_grid_nodes: chex.Array) -> chex.Array:\n    \"\"\"Runs the mesh2grid_gnn, extracting the output grid nodes.\"\"\"\n    batch_size = updated_latent_mesh_nodes.shape[1]\n    mesh2grid_graph = self._mesh2grid_graph_structure\n    assert mesh2grid_graph is not None\n    mesh_nodes = mesh2grid_graph.nodes['mesh_nodes']\n    grid_nodes = mesh2grid_graph.nodes['grid_nodes']\n    new_mesh_nodes = mesh_nodes._replace(features=updated_latent_mesh_nodes)\n    new_grid_nodes = grid_nodes._replace(features=latent_grid_nodes)\n    mesh2grid_key = mesh2grid_graph.edge_key_by_name('mesh2grid')\n    edges = mesh2grid_graph.edges[mesh2grid_key]\n    new_edges = edges._replace(features=_add_batch_second_axis(edges.features.astype(latent_grid_nodes.dtype), batch_size))\n    input_graph = mesh2grid_graph._replace(edges={mesh2grid_key: new_edges}, nodes={'mesh_nodes': new_mesh_nodes, 'grid_nodes': new_grid_nodes})\n    output_graph = self._mesh2grid_gnn(input_graph)\n    output_grid_nodes = output_graph.nodes['grid_nodes'].features\n    return output_grid_nodes",
    "120": "def _inputs_to_grid_node_features(self, inputs: xarray.Dataset, forcings: xarray.Dataset) -> chex.Array:\n    \"\"\"xarrays -> [num_grid_nodes, batch, num_channels].\"\"\"\n    stacked_inputs = model_utils.dataset_to_stacked(inputs)\n    stacked_forcings = model_utils.dataset_to_stacked(forcings)\n    stacked_inputs = xarray.concat([stacked_inputs, stacked_forcings], dim='channels')\n    grid_xarray_lat_lon_leading = model_utils.lat_lon_to_leading_axes(stacked_inputs)\n    return xarray_jax.unwrap(grid_xarray_lat_lon_leading.data).reshape((-1,) + grid_xarray_lat_lon_leading.data.shape[2:])",
    "121": "def _grid_node_outputs_to_prediction(self, grid_node_outputs: chex.Array, targets_template: xarray.Dataset) -> xarray.Dataset:\n    \"\"\"[num_grid_nodes, batch, num_outputs] -> xarray.\"\"\"\n    assert self._grid_lat is not None and self._grid_lon is not None\n    grid_shape = (self._grid_lat.shape[0], self._grid_lon.shape[0])\n    grid_outputs_lat_lon_leading = grid_node_outputs.reshape(grid_shape + grid_node_outputs.shape[1:])\n    dims = ('lat', 'lon', 'batch', 'channels')\n    grid_xarray_lat_lon_leading = xarray_jax.DataArray(data=grid_outputs_lat_lon_leading, dims=dims)\n    grid_xarray = model_utils.restore_leading_axes(grid_xarray_lat_lon_leading)\n    return model_utils.stacked_to_dataset(grid_xarray.variable, targets_template)",
    "123": "def _get_max_edge_distance(mesh):\n    (senders, receivers) = icosahedral_mesh.faces_to_edges(mesh.faces)\n    edge_distances = np.linalg.norm(mesh.vertices[senders] - mesh.vertices[receivers], axis=-1)\n    return edge_distances.max()",
    "124": "def _grid_lat_lon_to_coordinates(grid_latitude: np.ndarray, grid_longitude: np.ndarray) -> np.ndarray:\n    \"\"\"Lat [num_lat] lon [num_lon] to 3d coordinates [num_lat, num_lon, 3].\"\"\"\n    (phi_grid, theta_grid) = np.meshgrid(np.deg2rad(grid_longitude), np.deg2rad(90 - grid_latitude))\n    return np.stack([np.cos(phi_grid) * np.sin(theta_grid), np.sin(phi_grid) * np.sin(theta_grid), np.cos(theta_grid)], axis=-1)",
    "125": "def radius_query_indices(*, grid_latitude: np.ndarray, grid_longitude: np.ndarray, mesh: icosahedral_mesh.TriangularMesh, radius: float) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns mesh-grid edge indices for radius query.\n\n  Args:\n    grid_latitude: Latitude values for the grid [num_lat_points]\n    grid_longitude: Longitude values for the grid [num_lon_points]\n    mesh: Mesh object.\n    radius: Radius of connectivity in R3. for a sphere of unit radius.\n\n  Returns:\n    tuple with `grid_indices` and `mesh_indices` indicating edges between the\n    grid and the mesh such that the distances in a straight line (not geodesic)\n    are smaller than or equal to `radius`.\n    * grid_indices: Indices of shape [num_edges], that index into a\n      [num_lat_points, num_lon_points] grid, after flattening the leading axes.\n    * mesh_indices: Indices of shape [num_edges], that index into mesh.vertices.\n  \"\"\"\n    grid_positions = _grid_lat_lon_to_coordinates(grid_latitude, grid_longitude).reshape([-1, 3])\n    mesh_positions = mesh.vertices\n    kd_tree = scipy.spatial.cKDTree(mesh_positions)\n    query_indices = kd_tree.query_ball_point(x=grid_positions, r=radius)\n    grid_edge_indices = []\n    mesh_edge_indices = []\n    for (grid_index, mesh_neighbors) in enumerate(query_indices):\n        grid_edge_indices.append(np.repeat(grid_index, len(mesh_neighbors)))\n        mesh_edge_indices.append(mesh_neighbors)\n    grid_edge_indices = np.concatenate(grid_edge_indices, axis=0).astype(int)\n    mesh_edge_indices = np.concatenate(mesh_edge_indices, axis=0).astype(int)\n    return (grid_edge_indices, mesh_edge_indices)",
    "126": "def in_mesh_triangle_indices(*, grid_latitude: np.ndarray, grid_longitude: np.ndarray, mesh: icosahedral_mesh.TriangularMesh) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Returns mesh-grid edge indices for grid points contained in mesh triangles.\n\n  Args:\n    grid_latitude: Latitude values for the grid [num_lat_points]\n    grid_longitude: Longitude values for the grid [num_lon_points]\n    mesh: Mesh object.\n\n  Returns:\n    tuple with `grid_indices` and `mesh_indices` indicating edges between the\n    grid and the mesh vertices of the triangle that contain each grid point.\n    The number of edges is always num_lat_points * num_lon_points * 3\n    * grid_indices: Indices of shape [num_edges], that index into a\n      [num_lat_points, num_lon_points] grid, after flattening the leading axes.\n    * mesh_indices: Indices of shape [num_edges], that index into mesh.vertices.\n  \"\"\"\n    grid_positions = _grid_lat_lon_to_coordinates(grid_latitude, grid_longitude).reshape([-1, 3])\n    mesh_trimesh = trimesh.Trimesh(vertices=mesh.vertices, faces=mesh.faces)\n    (_, _, query_face_indices) = trimesh.proximity.closest_point(mesh_trimesh, grid_positions)\n    mesh_edge_indices = mesh.faces[query_face_indices]\n    grid_indices = np.arange(grid_positions.shape[0])\n    grid_edge_indices = np.tile(grid_indices.reshape([-1, 1]), [1, 3])\n    mesh_edge_indices = mesh_edge_indices.reshape([-1])\n    grid_edge_indices = grid_edge_indices.reshape([-1])\n    return (grid_edge_indices, mesh_edge_indices)",
    "127": "class GridMeshConnectivityTest(absltest.TestCase):\n\n    def test_grid_lat_lon_to_coordinates(self):\n        grid_latitude = np.array([-45.0, 0.0, 45])\n        grid_longitude = np.array([0.0, 90.0, 180.0, 270.0])\n        inv_sqrt2 = 1 / np.sqrt(2)\n        expected_coordinates = np.array([[[inv_sqrt2, 0.0, -inv_sqrt2], [0.0, inv_sqrt2, -inv_sqrt2], [-inv_sqrt2, 0.0, -inv_sqrt2], [0.0, -inv_sqrt2, -inv_sqrt2]], [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, -1.0, 0.0]], [[inv_sqrt2, 0.0, inv_sqrt2], [0.0, inv_sqrt2, inv_sqrt2], [-inv_sqrt2, 0.0, inv_sqrt2], [0.0, -inv_sqrt2, inv_sqrt2]]])\n        coordinates = grid_mesh_connectivity._grid_lat_lon_to_coordinates(grid_latitude, grid_longitude)\n        np.testing.assert_allclose(expected_coordinates, coordinates, atol=1e-15)\n\n    def test_radius_query_indices_smoke(self):\n        grid_latitude = np.linspace(-75, 75, 6)\n        grid_longitude = np.arange(12) * 30.0\n        mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n        grid_mesh_connectivity.radius_query_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh, radius=0.2)\n\n    def test_in_mesh_triangle_indices_smoke(self):\n        grid_latitude = np.linspace(-75, 75, 6)\n        grid_longitude = np.arange(12) * 30.0\n        mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n        grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh)",
    "128": "def test_grid_lat_lon_to_coordinates(self):\n    grid_latitude = np.array([-45.0, 0.0, 45])\n    grid_longitude = np.array([0.0, 90.0, 180.0, 270.0])\n    inv_sqrt2 = 1 / np.sqrt(2)\n    expected_coordinates = np.array([[[inv_sqrt2, 0.0, -inv_sqrt2], [0.0, inv_sqrt2, -inv_sqrt2], [-inv_sqrt2, 0.0, -inv_sqrt2], [0.0, -inv_sqrt2, -inv_sqrt2]], [[1.0, 0.0, 0.0], [0.0, 1.0, 0.0], [-1.0, 0.0, 0.0], [0.0, -1.0, 0.0]], [[inv_sqrt2, 0.0, inv_sqrt2], [0.0, inv_sqrt2, inv_sqrt2], [-inv_sqrt2, 0.0, inv_sqrt2], [0.0, -inv_sqrt2, inv_sqrt2]]])\n    coordinates = grid_mesh_connectivity._grid_lat_lon_to_coordinates(grid_latitude, grid_longitude)\n    np.testing.assert_allclose(expected_coordinates, coordinates, atol=1e-15)",
    "129": "def test_radius_query_indices_smoke(self):\n    grid_latitude = np.linspace(-75, 75, 6)\n    grid_longitude = np.arange(12) * 30.0\n    mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n    grid_mesh_connectivity.radius_query_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh, radius=0.2)",
    "130": "def test_in_mesh_triangle_indices_smoke(self):\n    grid_latitude = np.linspace(-75, 75, 6)\n    grid_longitude = np.arange(12) * 30.0\n    mesh = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=3)[-1]\n    grid_mesh_connectivity.in_mesh_triangle_indices(grid_latitude=grid_latitude, grid_longitude=grid_longitude, mesh=mesh)",
    "131": "class TriangularMesh(NamedTuple):\n    \"\"\"Data structure for triangular meshes.\n\n  Attributes:\n    vertices: spatial positions of the vertices of the mesh of shape\n        [num_vertices, num_dims].\n    faces: triangular faces of the mesh of shape [num_faces, 3]. Contains\n        integer indices into `vertices`.\n\n  \"\"\"\n    vertices: np.ndarray\n    faces: np.ndarray",
    "132": "def merge_meshes(mesh_list: Sequence[TriangularMesh]) -> TriangularMesh:\n    \"\"\"Merges all meshes into one. Assumes the last mesh is the finest.\n\n  Args:\n     mesh_list: Sequence of meshes, from coarse to fine refinement levels. The\n       vertices and faces may contain those from preceding, coarser levels.\n\n  Returns:\n     `TriangularMesh` for which the vertices correspond to the highest\n     resolution mesh in the hierarchy, and the faces are the join set of the\n     faces at all levels of the hierarchy.\n  \"\"\"\n    for (mesh_i, mesh_ip1) in itertools.pairwise(mesh_list):\n        num_nodes_mesh_i = mesh_i.vertices.shape[0]\n        assert np.allclose(mesh_i.vertices, mesh_ip1.vertices[:num_nodes_mesh_i])\n    return TriangularMesh(vertices=mesh_list[-1].vertices, faces=np.concatenate([mesh.faces for mesh in mesh_list], axis=0))",
    "133": "def get_hierarchy_of_triangular_meshes_for_sphere(splits: int) -> List[TriangularMesh]:\n    \"\"\"Returns a sequence of meshes, each with triangularization sphere.\n\n  Starting with a regular icosahedron (12 vertices, 20 faces, 30 edges) with\n  circumscribed unit sphere. Then, each triangular face is iteratively\n  subdivided into 4 triangular faces `splits` times. The new vertices are then\n  projected back onto the unit sphere. All resulting meshes are returned in a\n  list, from lowest to highest resolution.\n\n  The vertices in each face are specified in counter-clockwise order as\n  observed from the outside the icosahedron.\n\n  Args:\n     splits: How many times to split each triangle.\n  Returns:\n     Sequence of `TriangularMesh`s of length `splits + 1` each with:\n\n       vertices: [num_vertices, 3] vertex positions in 3D, all with unit norm.\n       faces: [num_faces, 3] with triangular faces joining sets of 3 vertices.\n           Each row contains three indices into the vertices array, indicating\n           the vertices adjacent to the face. Always with positive orientation\n           (counterclock-wise when looking from the outside).\n  \"\"\"\n    current_mesh = get_icosahedron()\n    output_meshes = [current_mesh]\n    for _ in range(splits):\n        current_mesh = _two_split_unit_sphere_triangle_faces(current_mesh)\n        output_meshes.append(current_mesh)\n    return output_meshes",
    "134": "def get_icosahedron() -> TriangularMesh:\n    \"\"\"Returns a regular icosahedral mesh with circumscribed unit sphere.\n\n  See https://en.wikipedia.org/wiki/Regular_icosahedron#Cartesian_coordinates\n  for details on the construction of the regular icosahedron.\n\n  The vertices in each face are specified in counter-clockwise order as observed\n  from the outside of the icosahedron.\n\n  Returns:\n     TriangularMesh with:\n\n     vertices: [num_vertices=12, 3] vertex positions in 3D, all with unit norm.\n     faces: [num_faces=20, 3] with triangular faces joining sets of 3 vertices.\n         Each row contains three indices into the vertices array, indicating\n         the vertices adjacent to the face. Always with positive orientation (\n         counterclock-wise when looking from the outside).\n\n  \"\"\"\n    phi = (1 + np.sqrt(5)) / 2\n    vertices = []\n    for c1 in [1.0, -1.0]:\n        for c2 in [phi, -phi]:\n            vertices.append((c1, c2, 0.0))\n            vertices.append((0.0, c1, c2))\n            vertices.append((c2, 0.0, c1))\n    vertices = np.array(vertices, dtype=np.float32)\n    vertices /= np.linalg.norm([1.0, phi])\n    faces = [(0, 1, 2), (0, 6, 1), (8, 0, 2), (8, 4, 0), (3, 8, 2), (3, 2, 7), (7, 2, 1), (0, 4, 6), (4, 11, 6), (6, 11, 5), (1, 5, 7), (4, 10, 11), (4, 8, 10), (10, 8, 3), (10, 3, 9), (11, 10, 9), (11, 9, 5), (5, 9, 7), (9, 3, 7), (1, 6, 5)]\n    angle_between_faces = 2 * np.arcsin(phi / np.sqrt(3))\n    rotation_angle = (np.pi - angle_between_faces) / 2\n    rotation = transform.Rotation.from_euler(seq='y', angles=rotation_angle)\n    rotation_matrix = rotation.as_matrix()\n    vertices = np.dot(vertices, rotation_matrix)\n    return TriangularMesh(vertices=vertices.astype(np.float32), faces=np.array(faces, dtype=np.int32))",
    "135": "def _two_split_unit_sphere_triangle_faces(triangular_mesh: TriangularMesh) -> TriangularMesh:\n    \"\"\"Splits each triangular face into 4 triangles keeping the orientation.\"\"\"\n    new_vertices_builder = _ChildVerticesBuilder(triangular_mesh.vertices)\n    new_faces = []\n    for (ind1, ind2, ind3) in triangular_mesh.faces:\n        ind12 = new_vertices_builder.get_new_child_vertex_index((ind1, ind2))\n        ind23 = new_vertices_builder.get_new_child_vertex_index((ind2, ind3))\n        ind31 = new_vertices_builder.get_new_child_vertex_index((ind3, ind1))\n        new_faces.extend([[ind1, ind12, ind31], [ind12, ind2, ind23], [ind31, ind23, ind3], [ind12, ind23, ind31]])\n    return TriangularMesh(vertices=new_vertices_builder.get_all_vertices(), faces=np.array(new_faces, dtype=np.int32))",
    "136": "class _ChildVerticesBuilder(object):\n    \"\"\"Bookkeeping of new child vertices added to an existing set of vertices.\"\"\"\n\n    def __init__(self, parent_vertices):\n        self._child_vertices_index_mapping = {}\n        self._parent_vertices = parent_vertices\n        self._all_vertices_list = list(parent_vertices)\n\n    def _get_child_vertex_key(self, parent_vertex_indices):\n        return tuple(sorted(parent_vertex_indices))\n\n    def _create_child_vertex(self, parent_vertex_indices):\n        \"\"\"Creates a new vertex.\"\"\"\n        child_vertex_position = self._parent_vertices[list(parent_vertex_indices)].mean(0)\n        child_vertex_position /= np.linalg.norm(child_vertex_position)\n        child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n        self._child_vertices_index_mapping[child_vertex_key] = len(self._all_vertices_list)\n        self._all_vertices_list.append(child_vertex_position)\n\n    def get_new_child_vertex_index(self, parent_vertex_indices):\n        \"\"\"Returns index for a child vertex, creating it if necessary.\"\"\"\n        child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n        if child_vertex_key not in self._child_vertices_index_mapping:\n            self._create_child_vertex(parent_vertex_indices)\n        return self._child_vertices_index_mapping[child_vertex_key]\n\n    def get_all_vertices(self):\n        \"\"\"Returns an array with old vertices.\"\"\"\n        return np.array(self._all_vertices_list)",
    "138": "def _get_child_vertex_key(self, parent_vertex_indices):\n    return tuple(sorted(parent_vertex_indices))",
    "139": "def _create_child_vertex(self, parent_vertex_indices):\n    \"\"\"Creates a new vertex.\"\"\"\n    child_vertex_position = self._parent_vertices[list(parent_vertex_indices)].mean(0)\n    child_vertex_position /= np.linalg.norm(child_vertex_position)\n    child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n    self._child_vertices_index_mapping[child_vertex_key] = len(self._all_vertices_list)\n    self._all_vertices_list.append(child_vertex_position)",
    "140": "def get_new_child_vertex_index(self, parent_vertex_indices):\n    \"\"\"Returns index for a child vertex, creating it if necessary.\"\"\"\n    child_vertex_key = self._get_child_vertex_key(parent_vertex_indices)\n    if child_vertex_key not in self._child_vertices_index_mapping:\n        self._create_child_vertex(parent_vertex_indices)\n    return self._child_vertices_index_mapping[child_vertex_key]",
    "142": "def faces_to_edges(faces: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Transforms polygonal faces to sender and receiver indices.\n\n  It does so by transforming every face into N_i edges. Such if the triangular\n  face has indices [0, 1, 2], three edges are added 0->1, 1->2, and 2->0.\n\n  If all faces have consistent orientation, and the surface represented by the\n  faces is closed, then every edge in a polygon with a certain orientation\n  is also part of another polygon with the opposite orientation. In this\n  situation, the edges returned by the method are always bidirectional.\n\n  Args:\n    faces: Integer array of shape [num_faces, 3]. Contains node indices\n        adjacent to each face.\n  Returns:\n    Tuple with sender/receiver indices, each of shape [num_edges=num_faces*3].\n\n  \"\"\"\n    assert faces.ndim == 2\n    assert faces.shape[-1] == 3\n    senders = np.concatenate([faces[:, 0], faces[:, 1], faces[:, 2]])\n    receivers = np.concatenate([faces[:, 1], faces[:, 2], faces[:, 0]])\n    return (senders, receivers)",
    "144": "class IcosahedralMeshTest(parameterized.TestCase):\n\n    def test_icosahedron(self):\n        mesh = icosahedral_mesh.get_icosahedron()\n        _assert_valid_mesh(mesh, num_expected_vertices=12, num_expected_faces=20)\n\n    @parameterized.parameters(list(range(5)))\n    def test_get_hierarchy_of_triangular_meshes_for_sphere(self, splits):\n        meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n        prev_vertices = None\n        for (mesh_i, mesh) in enumerate(meshes):\n            (num_expected_vertices, num_expected_faces) = _get_mesh_spec(mesh_i)\n            _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces)\n            if prev_vertices is not None:\n                leading_mesh_vertices = mesh.vertices[:prev_vertices.shape[0]]\n                np.testing.assert_array_equal(leading_mesh_vertices, prev_vertices)\n            if mesh_i < len(meshes) - 1:\n                prev_vertices = mesh.vertices\n\n    @parameterized.parameters(list(range(4)))\n    def test_merge_meshes(self, splits):\n        mesh_hierarchy = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n        mesh = icosahedral_mesh.merge_meshes(mesh_hierarchy)\n        expected_faces = np.concatenate([m.faces for m in mesh_hierarchy], axis=0)\n        np.testing.assert_array_equal(mesh.vertices, mesh_hierarchy[-1].vertices)\n        np.testing.assert_array_equal(mesh.faces, expected_faces)\n\n    def test_faces_to_edges(self):\n        faces = np.array([[0, 1, 2], [3, 4, 5]])\n        expected_edges = np.array([[0, 1], [3, 4], [1, 2], [4, 5], [2, 0], [5, 3]])\n        expected_senders = expected_edges[:, 0]\n        expected_receivers = expected_edges[:, 1]\n        (senders, receivers) = icosahedral_mesh.faces_to_edges(faces)\n        np.testing.assert_array_equal(senders, expected_senders)\n        np.testing.assert_array_equal(receivers, expected_receivers)",
    "146": "@parameterized.parameters(list(range(5)))\ndef test_get_hierarchy_of_triangular_meshes_for_sphere(self, splits):\n    meshes = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n    prev_vertices = None\n    for (mesh_i, mesh) in enumerate(meshes):\n        (num_expected_vertices, num_expected_faces) = _get_mesh_spec(mesh_i)\n        _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces)\n        if prev_vertices is not None:\n            leading_mesh_vertices = mesh.vertices[:prev_vertices.shape[0]]\n            np.testing.assert_array_equal(leading_mesh_vertices, prev_vertices)\n        if mesh_i < len(meshes) - 1:\n            prev_vertices = mesh.vertices",
    "147": "@parameterized.parameters(list(range(4)))\ndef test_merge_meshes(self, splits):\n    mesh_hierarchy = icosahedral_mesh.get_hierarchy_of_triangular_meshes_for_sphere(splits=splits)\n    mesh = icosahedral_mesh.merge_meshes(mesh_hierarchy)\n    expected_faces = np.concatenate([m.faces for m in mesh_hierarchy], axis=0)\n    np.testing.assert_array_equal(mesh.vertices, mesh_hierarchy[-1].vertices)\n    np.testing.assert_array_equal(mesh.faces, expected_faces)",
    "148": "def test_faces_to_edges(self):\n    faces = np.array([[0, 1, 2], [3, 4, 5]])\n    expected_edges = np.array([[0, 1], [3, 4], [1, 2], [4, 5], [2, 0], [5, 3]])\n    expected_senders = expected_edges[:, 0]\n    expected_receivers = expected_edges[:, 1]\n    (senders, receivers) = icosahedral_mesh.faces_to_edges(faces)\n    np.testing.assert_array_equal(senders, expected_senders)\n    np.testing.assert_array_equal(receivers, expected_receivers)",
    "149": "def _assert_valid_mesh(mesh, num_expected_vertices, num_expected_faces):\n    vertices = mesh.vertices\n    faces = mesh.faces\n    chex.assert_shape(vertices, [num_expected_vertices, 3])\n    chex.assert_shape(faces, [num_expected_faces, 3])\n    vertices_norm = np.linalg.norm(vertices, axis=-1)\n    np.testing.assert_allclose(vertices_norm, 1.0, rtol=1e-06)\n    _assert_positive_face_orientation(vertices, faces)",
    "150": "def _assert_positive_face_orientation(vertices, faces):\n    face_orientation = np.cross(vertices[faces[:, 1]] - vertices[faces[:, 0]], vertices[faces[:, 2]] - vertices[faces[:, 1]])\n    face_orientation /= np.linalg.norm(face_orientation, axis=-1, keepdims=True)\n    face_centers = vertices[faces].mean(1)\n    face_centers /= np.linalg.norm(face_centers, axis=-1, keepdims=True)\n    dot_center_orientation = np.einsum('ik,ik->i', face_orientation, face_centers)\n    np.testing.assert_allclose(dot_center_orientation, 1.0, atol=0.0006)",
    "151": "class LossFunction(Protocol):\n    \"\"\"A loss function.\n\n  This is a protocol so it's fine to use a plain function which 'quacks like'\n  this. This is just to document the interface.\n  \"\"\"\n\n    def __call__(self, predictions: xarray.Dataset, targets: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n        \"\"\"Computes a loss function.\n\n    Args:\n      predictions: Dataset of predictions.\n      targets: Dataset of targets.\n      **optional_kwargs: Implementations may support extra optional kwargs.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n    \"\"\"",
    "152": "def __call__(self, predictions: xarray.Dataset, targets: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n    \"\"\"Computes a loss function.\n\n    Args:\n      predictions: Dataset of predictions.\n      targets: Dataset of targets.\n      **optional_kwargs: Implementations may support extra optional kwargs.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n    \"\"\"",
    "153": "def weighted_mse_per_level(predictions: xarray.Dataset, targets: xarray.Dataset, per_variable_weights: Mapping[str, float]) -> LossAndDiagnostics:\n    \"\"\"Latitude- and pressure-level-weighted MSE loss.\"\"\"\n\n    def loss(prediction, target):\n        loss = (prediction - target) ** 2\n        loss *= normalized_latitude_weights(target).astype(loss.dtype)\n        if 'level' in target.dims:\n            loss *= normalized_level_weights(target).astype(loss.dtype)\n        return _mean_preserving_batch(loss)\n    losses = xarray_tree.map_structure(loss, predictions, targets)\n    return sum_per_variable_losses(losses, per_variable_weights)",
    "155": "def _mean_preserving_batch(x: xarray.DataArray) -> xarray.DataArray:\n    return x.mean([d for d in x.dims if d != 'batch'], skipna=False)",
    "156": "def sum_per_variable_losses(per_variable_losses: Mapping[str, xarray.DataArray], weights: Mapping[str, float]) -> LossAndDiagnostics:\n    \"\"\"Weighted sum of per-variable losses.\"\"\"\n    if not set(weights.keys()).issubset(set(per_variable_losses.keys())):\n        raise ValueError(f'Passing a weight that does not correspond to any variable {set(weights.keys()) - set(per_variable_losses.keys())}')\n    weighted_per_variable_losses = {name: loss * weights.get(name, 1) for (name, loss) in per_variable_losses.items()}\n    total = xarray.concat(weighted_per_variable_losses.values(), dim='variable', join='exact').sum('variable', skipna=False)\n    return (total, per_variable_losses)",
    "158": "def normalized_latitude_weights(data: xarray.DataArray) -> xarray.DataArray:\n    \"\"\"Weights based on latitude, roughly proportional to grid cell area.\n\n  This method supports two use cases only (both for equispaced values):\n  * Latitude values such that the closest value to the pole is at latitude\n    (90 - d_lat/2), where d_lat is the difference between contiguous latitudes.\n    For example: [-89, -87, -85, ..., 85, 87, 89]) (d_lat = 2)\n    In this case each point with `lat` value represents a sphere slice between\n    `lat - d_lat/2` and `lat + d_lat/2`, and the area of this slice would be\n    proportional to:\n    `sin(lat + d_lat/2) - sin(lat - d_lat/2) = 2 * sin(d_lat/2) * cos(lat)`, and\n    we can simply omit the term `2 * sin(d_lat/2)` which is just a constant\n    that cancels during normalization.\n  * Latitude values that fall exactly at the poles.\n    For example: [-90, -88, -86, ..., 86, 88, 90]) (d_lat = 2)\n    In this case each point with `lat` value also represents\n    a sphere slice between `lat - d_lat/2` and `lat + d_lat/2`,\n    except for the points at the poles, that represent a slice between\n    `90 - d_lat/2` and `90` or, `-90` and  `-90 + d_lat/2`.\n    The areas of the first type of point are still proportional to:\n    * sin(lat + d_lat/2) - sin(lat - d_lat/2) = 2 * sin(d_lat/2) * cos(lat)\n    but for the points at the poles now is:\n    * sin(90) - sin(90 - d_lat/2) = 2 * sin(d_lat/4) ^ 2\n    and we will be using these weights, depending on whether we are looking at\n    pole cells, or non-pole cells (omitting the common factor of 2 which will be\n    absorbed by the normalization).\n\n    It can be shown via a limit, or simple geometry, that in the small angles\n    regime, the proportion of area per pole-point is equal to 1/8th\n    the proportion of area covered by each of the nearest non-pole point, and we\n    test for this in the test.\n\n  Args:\n    data: `DataArray` with latitude coordinates.\n  Returns:\n    Unit mean latitude weights.\n  \"\"\"\n    latitude = data.coords['lat']\n    if np.any(np.isclose(np.abs(latitude), 90.0)):\n        weights = _weight_for_latitude_vector_with_poles(latitude)\n    else:\n        weights = _weight_for_latitude_vector_without_poles(latitude)\n    return weights / weights.mean(skipna=False)",
    "159": "def _weight_for_latitude_vector_without_poles(latitude):\n    \"\"\"Weights for uniform latitudes of the form [+-90-+d/2, ..., -+90+-d/2].\"\"\"\n    delta_latitude = np.abs(_check_uniform_spacing_and_get_delta(latitude))\n    if not np.isclose(np.max(latitude), 90 - delta_latitude / 2) or not np.isclose(np.min(latitude), -90 + delta_latitude / 2):\n        raise ValueError(f'Latitude vector {latitude} does not start/end at +- (90 - delta_latitude/2) degrees.')\n    return np.cos(np.deg2rad(latitude))",
    "160": "def _weight_for_latitude_vector_with_poles(latitude):\n    \"\"\"Weights for uniform latitudes of the form [+- 90, ..., -+90].\"\"\"\n    delta_latitude = np.abs(_check_uniform_spacing_and_get_delta(latitude))\n    if not np.isclose(np.max(latitude), 90.0) or not np.isclose(np.min(latitude), -90.0):\n        raise ValueError(f'Latitude vector {latitude} does not start/end at +- 90 degrees.')\n    weights = np.cos(np.deg2rad(latitude)) * np.sin(np.deg2rad(delta_latitude / 2))\n    weights[[0, -1]] = np.sin(np.deg2rad(delta_latitude / 4)) ** 2\n    return weights",
    "162": "def get_graph_spatial_features(*, node_lat: np.ndarray, node_lon: np.ndarray, senders: np.ndarray, receivers: np.ndarray, add_node_positions: bool, add_node_latitude: bool, add_node_longitude: bool, add_relative_positions: bool, relative_longitude_local_coordinates: bool, relative_latitude_local_coordinates: bool, sine_cosine_encoding: bool=False, encoding_num_freqs: int=10, encoding_multiplicative_factor: float=1.2) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Computes spatial features for the nodes.\n\n  Args:\n    node_lat: Latitudes in the [-90, 90] interval of shape [num_nodes]\n    node_lon: Longitudes in the [0, 360] interval of shape [num_nodes]\n    senders: Sender indices of shape [num_edges]\n    receivers: Receiver indices of shape [num_edges]\n    add_node_positions: Add unit norm absolute positions.\n    add_node_latitude: Add a feature for latitude (cos(90 - lat))\n        Note even if this is set to False, the model may be able to infer the\n        longitude from relative features, unless\n        `relative_latitude_local_coordinates` is also True, or if there is any\n        bias on the relative edge sizes for different longitudes.\n    add_node_longitude: Add features for longitude (cos(lon), sin(lon)).\n        Note even if this is set to False, the model may be able to infer the\n        longitude from relative features, unless\n        `relative_longitude_local_coordinates` is also True, or if there is any\n        bias on the relative edge sizes for different longitudes.\n    add_relative_positions: Whether to relative positions in R3 to the edges.\n    relative_longitude_local_coordinates: If True, relative positions are\n        computed in a local space where the receiver is at 0 longitude.\n    relative_latitude_local_coordinates: If True, relative positions are\n        computed in a local space where the receiver is at 0 latitude.\n    sine_cosine_encoding: If True, we will transform the node/edge features\n        with sine and cosine functions, similar to NERF.\n    encoding_num_freqs: frequency parameter\n    encoding_multiplicative_factor: used for calculating the frequency.\n\n  Returns:\n    Arrays of shape: [num_nodes, num_features] and [num_edges, num_features].\n    with node and edge features.\n\n  \"\"\"\n    num_nodes = node_lat.shape[0]\n    num_edges = senders.shape[0]\n    dtype = node_lat.dtype\n    (node_phi, node_theta) = lat_lon_deg_to_spherical(node_lat, node_lon)\n    node_features = []\n    if add_node_positions:\n        node_features.extend(spherical_to_cartesian(node_phi, node_theta))\n    if add_node_latitude:\n        node_features.append(np.cos(node_theta))\n    if add_node_longitude:\n        node_features.append(np.cos(node_phi))\n        node_features.append(np.sin(node_phi))\n    if not node_features:\n        node_features = np.zeros([num_nodes, 0], dtype=dtype)\n    else:\n        node_features = np.stack(node_features, axis=-1)\n    edge_features = []\n    if add_relative_positions:\n        relative_position = get_relative_position_in_receiver_local_coordinates(node_phi=node_phi, node_theta=node_theta, senders=senders, receivers=receivers, latitude_local_coordinates=relative_latitude_local_coordinates, longitude_local_coordinates=relative_longitude_local_coordinates)\n        relative_edge_distances = np.linalg.norm(relative_position, axis=-1, keepdims=True)\n        max_edge_distance = relative_edge_distances.max()\n        edge_features.append(relative_edge_distances / max_edge_distance)\n        edge_features.append(relative_position / max_edge_distance)\n    if not edge_features:\n        edge_features = np.zeros([num_edges, 0], dtype=dtype)\n    else:\n        edge_features = np.concatenate(edge_features, axis=-1)\n    if sine_cosine_encoding:\n\n        def sine_cosine_transform(x: np.ndarray) -> np.ndarray:\n            freqs = encoding_multiplicative_factor ** np.arange(encoding_num_freqs)\n            phases = freqs * x[..., None]\n            x_sin = np.sin(phases)\n            x_cos = np.cos(phases)\n            x_cat = np.concatenate([x_sin, x_cos], axis=-1)\n            return x_cat.reshape([x.shape[0], -1])\n        node_features = sine_cosine_transform(node_features)\n        edge_features = sine_cosine_transform(edge_features)\n    return (node_features, edge_features)",
    "163": "def sine_cosine_transform(x: np.ndarray) -> np.ndarray:\n    freqs = encoding_multiplicative_factor ** np.arange(encoding_num_freqs)\n    phases = freqs * x[..., None]\n    x_sin = np.sin(phases)\n    x_cos = np.cos(phases)\n    x_cat = np.concatenate([x_sin, x_cos], axis=-1)\n    return x_cat.reshape([x.shape[0], -1])",
    "165": "def restore_leading_axes(grid_xarray: xarray.DataArray) -> xarray.DataArray:\n    \"\"\"Reorders xarray so batch/time/level axes come first (if present).\"\"\"\n    input_dims = list(grid_xarray.dims)\n    output_dims = list(input_dims)\n    for leading_key in ['level', 'time', 'batch']:\n        if leading_key in input_dims:\n            output_dims.remove(leading_key)\n            output_dims.insert(0, leading_key)\n    return grid_xarray.transpose(*output_dims)",
    "167": "def spherical_to_lat_lon(phi: np.ndarray, theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n    lon = np.mod(np.rad2deg(phi), 360)\n    lat = 90 - np.rad2deg(theta)\n    return (lat, lon)",
    "169": "def spherical_to_cartesian(phi: np.ndarray, theta: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    return (np.cos(phi) * np.sin(theta), np.sin(phi) * np.sin(theta), np.cos(theta))",
    "170": "def get_relative_position_in_receiver_local_coordinates(node_phi: np.ndarray, node_theta: np.ndarray, senders: np.ndarray, receivers: np.ndarray, latitude_local_coordinates: bool, longitude_local_coordinates: bool) -> np.ndarray:\n    \"\"\"Returns relative position features for the edges.\n\n  The relative positions will be computed in a rotated space for a local\n  coordinate system as defined by the receiver. The relative positions are\n  simply obtained by subtracting sender position minues receiver position in\n  that local coordinate system after the rotation in R^3.\n\n  Args:\n    node_phi: [num_nodes] with polar angles.\n    node_theta: [num_nodes] with azimuthal angles.\n    senders: [num_edges] with indices.\n    receivers: [num_edges] with indices.\n    latitude_local_coordinates: Whether to rotate edges such that in the\n        positions are computed such that the receiver is always at latitude 0.\n    longitude_local_coordinates: Whether to rotate edges such that in the\n        positions are computed such that the receiver is always at longitude 0.\n\n  Returns:\n    Array of relative positions in R3 [num_edges, 3]\n  \"\"\"\n    node_pos = np.stack(spherical_to_cartesian(node_phi, node_theta), axis=-1)\n    if not (latitude_local_coordinates or longitude_local_coordinates):\n        return node_pos[senders] - node_pos[receivers]\n    rotation_matrices = get_rotation_matrices_to_local_coordinates(reference_phi=node_phi, reference_theta=node_theta, rotate_latitude=latitude_local_coordinates, rotate_longitude=longitude_local_coordinates)\n    edge_rotation_matrices = rotation_matrices[receivers]\n    receiver_pos_in_rotated_space = rotate_with_matrices(edge_rotation_matrices, node_pos[receivers])\n    sender_pos_in_in_rotated_space = rotate_with_matrices(edge_rotation_matrices, node_pos[senders])\n    return sender_pos_in_in_rotated_space - receiver_pos_in_rotated_space",
    "171": "def get_rotation_matrices_to_local_coordinates(reference_phi: np.ndarray, reference_theta: np.ndarray, rotate_latitude: bool, rotate_longitude: bool) -> np.ndarray:\n    \"\"\"Returns a rotation matrix to rotate to a point based on a reference vector.\n\n  The rotation matrix is build such that, a vector in the\n  same coordinate system at the reference point that points towards the pole\n  before the rotation, continues to point towards the pole after the rotation.\n\n  Args:\n    reference_phi: [leading_axis] Polar angles of the reference.\n    reference_theta: [leading_axis] Azimuthal angles of the reference.\n    rotate_latitude: Whether to produce a rotation matrix that would rotate\n        R^3 vectors to zero latitude.\n    rotate_longitude: Whether to produce a rotation matrix that would rotate\n        R^3 vectors to zero longitude.\n\n  Returns:\n    Matrices of shape [leading_axis] such that when applied to the reference\n        position with `rotate_with_matrices(rotation_matrices, reference_pos)`\n\n        * phi goes to 0. if \"rotate_longitude\" is True.\n\n        * theta goes to np.pi / 2 if \"rotate_latitude\" is True.\n\n        The rotation consists of:\n        * rotate_latitude = False, rotate_longitude = True:\n            Latitude preserving rotation.\n        * rotate_latitude = True, rotate_longitude = True:\n            Latitude preserving rotation, followed by longitude preserving\n            rotation.\n        * rotate_latitude = True, rotate_longitude = False:\n            Latitude preserving rotation, followed by longitude preserving\n            rotation, and the inverse of the latitude preserving rotation. Note\n            this is computationally different from rotating the longitude only\n            and is. We do it like this, so the polar geodesic curve, continues\n            to be aligned with one of the axis after the rotation.\n\n  \"\"\"\n    if rotate_longitude and rotate_latitude:\n        azimuthal_rotation = -reference_phi\n        polar_rotation = -reference_theta + np.pi / 2\n        return transform.Rotation.from_euler('zy', np.stack([azimuthal_rotation, polar_rotation], axis=1)).as_matrix()\n    elif rotate_longitude:\n        azimuthal_rotation = -reference_phi\n        return transform.Rotation.from_euler('z', -reference_phi).as_matrix()\n    elif rotate_latitude:\n        azimuthal_rotation = -reference_phi\n        polar_rotation = -reference_theta + np.pi / 2\n        return transform.Rotation.from_euler('zyz', np.stack([azimuthal_rotation, polar_rotation, -azimuthal_rotation], axis=1)).as_matrix()\n    else:\n        raise ValueError('At least one of longitude and latitude should be rotated.')",
    "173": "def get_bipartite_graph_spatial_features(*, senders_node_lat: np.ndarray, senders_node_lon: np.ndarray, senders: np.ndarray, receivers_node_lat: np.ndarray, receivers_node_lon: np.ndarray, receivers: np.ndarray, add_node_positions: bool, add_node_latitude: bool, add_node_longitude: bool, add_relative_positions: bool, edge_normalization_factor: Optional[float]=None, relative_longitude_local_coordinates: bool, relative_latitude_local_coordinates: bool) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Computes spatial features for the nodes.\n\n  This function is almost identical to `get_graph_spatial_features`. The only\n  difference is that sender nodes and receiver nodes can be in different arrays.\n  This is necessary to enable combination with typed Graph.\n\n  Args:\n    senders_node_lat: Latitudes in the [-90, 90] interval of shape\n      [num_sender_nodes]\n    senders_node_lon: Longitudes in the [0, 360] interval of shape\n      [num_sender_nodes]\n    senders: Sender indices of shape [num_edges], indices in [0,\n      num_sender_nodes)\n    receivers_node_lat: Latitudes in the [-90, 90] interval of shape\n      [num_receiver_nodes]\n    receivers_node_lon: Longitudes in the [0, 360] interval of shape\n      [num_receiver_nodes]\n    receivers: Receiver indices of shape [num_edges], indices in [0,\n      num_receiver_nodes)\n    add_node_positions: Add unit norm absolute positions.\n    add_node_latitude: Add a feature for latitude (cos(90 - lat)) Note even if\n      this is set to False, the model may be able to infer the longitude from\n      relative features, unless `relative_latitude_local_coordinates` is also\n      True, or if there is any bias on the relative edge sizes for different\n      longitudes.\n    add_node_longitude: Add features for longitude (cos(lon), sin(lon)). Note\n      even if this is set to False, the model may be able to infer the longitude\n      from relative features, unless `relative_longitude_local_coordinates` is\n      also True, or if there is any bias on the relative edge sizes for\n      different longitudes.\n    add_relative_positions: Whether to relative positions in R3 to the edges.\n    edge_normalization_factor: Allows explicitly controlling edge normalization.\n      If None, defaults to max edge length. This supports using pre-trained\n      model weights with a different graph structure to what it was trained on.\n    relative_longitude_local_coordinates: If True, relative positions are\n      computed in a local space where the receiver is at 0 longitude.\n    relative_latitude_local_coordinates: If True, relative positions are\n      computed in a local space where the receiver is at 0 latitude.\n\n  Returns:\n    Arrays of shape: [num_nodes, num_features] and [num_edges, num_features].\n    with node and edge features.\n\n  \"\"\"\n    num_senders = senders_node_lat.shape[0]\n    num_receivers = receivers_node_lat.shape[0]\n    num_edges = senders.shape[0]\n    dtype = senders_node_lat.dtype\n    assert receivers_node_lat.dtype == dtype\n    (senders_node_phi, senders_node_theta) = lat_lon_deg_to_spherical(senders_node_lat, senders_node_lon)\n    (receivers_node_phi, receivers_node_theta) = lat_lon_deg_to_spherical(receivers_node_lat, receivers_node_lon)\n    senders_node_features = []\n    receivers_node_features = []\n    if add_node_positions:\n        senders_node_features.extend(spherical_to_cartesian(senders_node_phi, senders_node_theta))\n        receivers_node_features.extend(spherical_to_cartesian(receivers_node_phi, receivers_node_theta))\n    if add_node_latitude:\n        senders_node_features.append(np.cos(senders_node_theta))\n        receivers_node_features.append(np.cos(receivers_node_theta))\n    if add_node_longitude:\n        senders_node_features.append(np.cos(senders_node_phi))\n        senders_node_features.append(np.sin(senders_node_phi))\n        receivers_node_features.append(np.cos(receivers_node_phi))\n        receivers_node_features.append(np.sin(receivers_node_phi))\n    if not senders_node_features:\n        senders_node_features = np.zeros([num_senders, 0], dtype=dtype)\n        receivers_node_features = np.zeros([num_receivers, 0], dtype=dtype)\n    else:\n        senders_node_features = np.stack(senders_node_features, axis=-1)\n        receivers_node_features = np.stack(receivers_node_features, axis=-1)\n    edge_features = []\n    if add_relative_positions:\n        relative_position = get_bipartite_relative_position_in_receiver_local_coordinates(senders_node_phi=senders_node_phi, senders_node_theta=senders_node_theta, receivers_node_phi=receivers_node_phi, receivers_node_theta=receivers_node_theta, senders=senders, receivers=receivers, latitude_local_coordinates=relative_latitude_local_coordinates, longitude_local_coordinates=relative_longitude_local_coordinates)\n        relative_edge_distances = np.linalg.norm(relative_position, axis=-1, keepdims=True)\n        if edge_normalization_factor is None:\n            edge_normalization_factor = relative_edge_distances.max()\n        edge_features.append(relative_edge_distances / edge_normalization_factor)\n        edge_features.append(relative_position / edge_normalization_factor)\n    if not edge_features:\n        edge_features = np.zeros([num_edges, 0], dtype=dtype)\n    else:\n        edge_features = np.concatenate(edge_features, axis=-1)\n    return (senders_node_features, receivers_node_features, edge_features)",
    "174": "def get_bipartite_relative_position_in_receiver_local_coordinates(senders_node_phi: np.ndarray, senders_node_theta: np.ndarray, senders: np.ndarray, receivers_node_phi: np.ndarray, receivers_node_theta: np.ndarray, receivers: np.ndarray, latitude_local_coordinates: bool, longitude_local_coordinates: bool) -> np.ndarray:\n    \"\"\"Returns relative position features for the edges.\n\n  This function is equivalent to\n  `get_relative_position_in_receiver_local_coordinates`, but adapted to work\n  with bipartite typed graphs.\n\n  The relative positions will be computed in a rotated space for a local\n  coordinate system as defined by the receiver. The relative positions are\n  simply obtained by subtracting sender position minues receiver position in\n  that local coordinate system after the rotation in R^3.\n\n  Args:\n    senders_node_phi: [num_sender_nodes] with polar angles.\n    senders_node_theta: [num_sender_nodes] with azimuthal angles.\n    senders: [num_edges] with indices into sender nodes.\n    receivers_node_phi: [num_sender_nodes] with polar angles.\n    receivers_node_theta: [num_sender_nodes] with azimuthal angles.\n    receivers: [num_edges] with indices into receiver nodes.\n    latitude_local_coordinates: Whether to rotate edges such that in the\n      positions are computed such that the receiver is always at latitude 0.\n    longitude_local_coordinates: Whether to rotate edges such that in the\n      positions are computed such that the receiver is always at longitude 0.\n\n  Returns:\n    Array of relative positions in R3 [num_edges, 3]\n  \"\"\"\n    senders_node_pos = np.stack(spherical_to_cartesian(senders_node_phi, senders_node_theta), axis=-1)\n    receivers_node_pos = np.stack(spherical_to_cartesian(receivers_node_phi, receivers_node_theta), axis=-1)\n    if not (latitude_local_coordinates or longitude_local_coordinates):\n        return senders_node_pos[senders] - receivers_node_pos[receivers]\n    receiver_rotation_matrices = get_rotation_matrices_to_local_coordinates(reference_phi=receivers_node_phi, reference_theta=receivers_node_theta, rotate_latitude=latitude_local_coordinates, rotate_longitude=longitude_local_coordinates)\n    edge_rotation_matrices = receiver_rotation_matrices[receivers]\n    receiver_pos_in_rotated_space = rotate_with_matrices(edge_rotation_matrices, receivers_node_pos[receivers])\n    sender_pos_in_in_rotated_space = rotate_with_matrices(edge_rotation_matrices, senders_node_pos[senders])\n    return sender_pos_in_in_rotated_space - receiver_pos_in_rotated_space",
    "175": "def variable_to_stacked(variable: xarray.Variable, sizes: Mapping[str, int], preserved_dims: Tuple[str, ...]=('batch', 'lat', 'lon')) -> xarray.Variable:\n    \"\"\"Converts an xarray.Variable to preserved_dims + (\"channels\",).\n\n  Any dimensions other than those included in preserved_dims get stacked into a\n  final \"channels\" dimension. If any of the preserved_dims are missing then they\n  are added, with the data broadcast/tiled to match the sizes specified in\n  `sizes`.\n\n  Args:\n    variable: An xarray.Variable.\n    sizes: Mapping including sizes for any dimensions which are not present in\n      `variable` but are needed for the output. This may be needed for example\n      for a static variable with only (\"lat\", \"lon\") dims, or if you want to\n      encode just the latitude coordinates (a variable with dims (\"lat\",)).\n    preserved_dims: dimensions of variable to not be folded in channels.\n\n  Returns:\n    An xarray.Variable with dimensions preserved_dims + (\"channels\",).\n  \"\"\"\n    stack_to_channels_dims = [d for d in variable.dims if d not in preserved_dims]\n    if stack_to_channels_dims:\n        variable = variable.stack(channels=stack_to_channels_dims)\n    dims = {dim: variable.sizes.get(dim) or sizes[dim] for dim in preserved_dims}\n    dims['channels'] = variable.sizes.get('channels', 1)\n    return variable.set_dims(dims)",
    "176": "def dataset_to_stacked(dataset: xarray.Dataset, sizes: Optional[Mapping[str, int]]=None, preserved_dims: Tuple[str, ...]=('batch', 'lat', 'lon')) -> xarray.DataArray:\n    \"\"\"Converts an xarray.Dataset to a single stacked array.\n\n  This takes each consistuent data_var, converts it into BHWC layout\n  using `variable_to_stacked`, then concats them all along the channels axis.\n\n  Args:\n    dataset: An xarray.Dataset.\n    sizes: Mapping including sizes for any dimensions which are not present in\n      the `dataset` but are needed for the output. See variable_to_stacked.\n    preserved_dims: dimensions from the dataset that should not be folded in\n      the predictions channels.\n\n  Returns:\n    An xarray.DataArray with dimensions preserved_dims + (\"channels\",).\n    Existing coordinates for preserved_dims axes will be preserved, however\n    there will be no coordinates for \"channels\".\n  \"\"\"\n    data_vars = [variable_to_stacked(dataset.variables[name], sizes or dataset.sizes, preserved_dims) for name in sorted(dataset.data_vars.keys())]\n    coords = {dim: coord for (dim, coord) in dataset.coords.items() if dim in preserved_dims}\n    return xarray.DataArray(data=xarray.Variable.concat(data_vars, dim='channels'), coords=coords)",
    "177": "def stacked_to_dataset(stacked_array: xarray.Variable, template_dataset: xarray.Dataset, preserved_dims: Tuple[str, ...]=('batch', 'lat', 'lon')) -> xarray.Dataset:\n    \"\"\"The inverse of dataset_to_stacked.\n\n  Requires a template dataset to demonstrate the variables/shapes/coordinates\n  required.\n  All variables must have preserved_dims dimensions.\n\n  Args:\n    stacked_array: Data in BHWC layout, encoded the same as dataset_to_stacked\n      would if it was asked to encode `template_dataset`.\n    template_dataset: A template Dataset (or other mapping of DataArrays)\n      demonstrating the shape of output required (variables, shapes,\n      coordinates etc).\n    preserved_dims: dimensions from the target_template that were not folded in\n      the predictions channels. The preserved_dims need to be a subset of the\n      dims of all the variables of template_dataset.\n\n  Returns:\n    An xarray.Dataset (or other mapping of DataArrays) with the same shape and\n    type as template_dataset.\n  \"\"\"\n    unstack_from_channels_sizes = {}\n    var_names = sorted(template_dataset.keys())\n    for name in var_names:\n        template_var = template_dataset[name]\n        if not all((dim in template_var.dims for dim in preserved_dims)):\n            raise ValueError(f'stacked_to_dataset requires all Variables to have {preserved_dims} dimensions, but found only {template_var.dims}.')\n        unstack_from_channels_sizes[name] = {dim: size for (dim, size) in template_var.sizes.items() if dim not in preserved_dims}\n    channels = {name: np.prod(list(unstack_sizes.values()), dtype=np.int64) for (name, unstack_sizes) in unstack_from_channels_sizes.items()}\n    total_expected_channels = sum(channels.values())\n    found_channels = stacked_array.sizes['channels']\n    if total_expected_channels != found_channels:\n        raise ValueError(f'Expected {total_expected_channels} channels but found {found_channels}, when trying to convert a stacked array of shape {stacked_array.sizes} to a dataset of shape {template_dataset}.')\n    data_vars = {}\n    index = 0\n    for name in var_names:\n        template_var = template_dataset[name]\n        var = stacked_array.isel({'channels': slice(index, index + channels[name])})\n        index += channels[name]\n        var = var.unstack({'channels': unstack_from_channels_sizes[name]})\n        var = var.transpose(*template_var.dims)\n        data_vars[name] = xarray.DataArray(data=var, coords=template_var.coords, name=template_var.name)\n    return type(template_dataset)(data_vars)",
    "178": "def normalize(values: xarray.Dataset, scales: xarray.Dataset, locations: Optional[xarray.Dataset]) -> xarray.Dataset:\n    \"\"\"Normalize variables using the given scales and (optionally) locations.\"\"\"\n\n    def normalize_array(array):\n        if array.name is None:\n            raise ValueError(\"Can't look up normalization constants because array has no name.\")\n        if locations is not None:\n            if array.name in locations:\n                array = array - locations[array.name].astype(array.dtype)\n            else:\n                logging.warning('No normalization location found for %s', array.name)\n        if array.name in scales:\n            array = array / scales[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization scale found for %s', array.name)\n        return array\n    return xarray_tree.map_structure(normalize_array, values)",
    "179": "def normalize_array(array):\n    if array.name is None:\n        raise ValueError(\"Can't look up normalization constants because array has no name.\")\n    if locations is not None:\n        if array.name in locations:\n            array = array - locations[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization location found for %s', array.name)\n    if array.name in scales:\n        array = array / scales[array.name].astype(array.dtype)\n    else:\n        logging.warning('No normalization scale found for %s', array.name)\n    return array",
    "180": "def unnormalize(values: xarray.Dataset, scales: xarray.Dataset, locations: Optional[xarray.Dataset]) -> xarray.Dataset:\n    \"\"\"Unnormalize variables using the given scales and (optionally) locations.\"\"\"\n\n    def unnormalize_array(array):\n        if array.name is None:\n            raise ValueError(\"Can't look up normalization constants because array has no name.\")\n        if array.name in scales:\n            array = array * scales[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization scale found for %s', array.name)\n        if locations is not None:\n            if array.name in locations:\n                array = array + locations[array.name].astype(array.dtype)\n            else:\n                logging.warning('No normalization location found for %s', array.name)\n        return array\n    return xarray_tree.map_structure(unnormalize_array, values)",
    "181": "def unnormalize_array(array):\n    if array.name is None:\n        raise ValueError(\"Can't look up normalization constants because array has no name.\")\n    if array.name in scales:\n        array = array * scales[array.name].astype(array.dtype)\n    else:\n        logging.warning('No normalization scale found for %s', array.name)\n    if locations is not None:\n        if array.name in locations:\n            array = array + locations[array.name].astype(array.dtype)\n        else:\n            logging.warning('No normalization location found for %s', array.name)\n    return array",
    "182": "class InputsAndResiduals(predictor_base.Predictor):\n    \"\"\"Wraps with a residual connection, normalizing inputs and target residuals.\n\n  The inner predictor is given inputs that are normalized using `locations`\n  and `scales` to roughly zero-mean unit variance.\n\n  For target variables that are present in the inputs, the inner predictor is\n  trained to predict residuals (target - last_frame_of_input) that have been\n  normalized using `residual_scales` (and optionally `residual_locations`) to\n  roughly unit variance / zero mean.\n\n  This replaces `residual.Predictor` in the case where you want normalization\n  that's based on the scales of the residuals.\n\n  Since we return the underlying predictor's loss on the normalized residuals,\n  if the underlying predictor is a sum of per-variable losses, the normalization\n  will affect the relative weighting of the per-variable loss terms (hopefully\n  in a good way).\n\n  For target variables *not* present in the inputs, the inner predictor is\n  trained to predict targets directly, that have been normalized in the same\n  way as the inputs.\n\n  The transforms applied to the targets (the residual connection and the\n  normalization) are applied in reverse to the predictions before returning\n  them.\n  \"\"\"\n\n    def __init__(self, predictor: predictor_base.Predictor, stddev_by_level: xarray.Dataset, mean_by_level: xarray.Dataset, diffs_stddev_by_level: xarray.Dataset):\n        self._predictor = predictor\n        self._scales = stddev_by_level\n        self._locations = mean_by_level\n        self._residual_scales = diffs_stddev_by_level\n        self._residual_locations = None\n\n    def _unnormalize_prediction_and_add_input(self, inputs, norm_prediction):\n        if norm_prediction.sizes.get('time') != 1:\n            raise ValueError('normalization.InputsAndResiduals only supports predicting a single timestep.')\n        if norm_prediction.name in inputs:\n            prediction = unnormalize(norm_prediction, self._residual_scales, self._residual_locations)\n            last_input = inputs[norm_prediction.name].isel(time=-1)\n            prediction = prediction + last_input\n            return prediction\n        else:\n            return unnormalize(norm_prediction, self._scales, self._locations)\n\n    def _subtract_input_and_normalize_target(self, inputs, target):\n        if target.sizes.get('time') != 1:\n            raise ValueError('normalization.InputsAndResiduals only supports wrapping predictorsthat predict a single timestep.')\n        if target.name in inputs:\n            target_residual = target\n            last_input = inputs[target.name].isel(time=-1)\n            target_residual = target_residual - last_input\n            return normalize(target_residual, self._residual_scales, self._residual_locations)\n        else:\n            return normalize(target, self._scales, self._locations)\n\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n        norm_inputs = normalize(inputs, self._scales, self._locations)\n        norm_forcings = normalize(forcings, self._scales, self._locations)\n        norm_predictions = self._predictor(norm_inputs, targets_template, forcings=norm_forcings, **kwargs)\n        return xarray_tree.map_structure(lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred), norm_predictions)\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n        \"\"\"Returns the loss computed on normalized inputs and targets.\"\"\"\n        norm_inputs = normalize(inputs, self._scales, self._locations)\n        norm_forcings = normalize(forcings, self._scales, self._locations)\n        norm_target_residuals = xarray_tree.map_structure(lambda t: self._subtract_input_and_normalize_target(inputs, t), targets)\n        return self._predictor.loss(norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n\n    def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n        \"\"\"The loss computed on normalized data, with unnormalized predictions.\"\"\"\n        norm_inputs = normalize(inputs, self._scales, self._locations)\n        norm_forcings = normalize(forcings, self._scales, self._locations)\n        norm_target_residuals = xarray_tree.map_structure(lambda t: self._subtract_input_and_normalize_target(inputs, t), targets)\n        ((loss, scalars), norm_predictions) = self._predictor.loss_and_predictions(norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n        predictions = xarray_tree.map_structure(lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred), norm_predictions)\n        return ((loss, scalars), predictions)",
    "183": "def __init__(self, predictor: predictor_base.Predictor, stddev_by_level: xarray.Dataset, mean_by_level: xarray.Dataset, diffs_stddev_by_level: xarray.Dataset):\n    self._predictor = predictor\n    self._scales = stddev_by_level\n    self._locations = mean_by_level\n    self._residual_scales = diffs_stddev_by_level\n    self._residual_locations = None",
    "184": "def _unnormalize_prediction_and_add_input(self, inputs, norm_prediction):\n    if norm_prediction.sizes.get('time') != 1:\n        raise ValueError('normalization.InputsAndResiduals only supports predicting a single timestep.')\n    if norm_prediction.name in inputs:\n        prediction = unnormalize(norm_prediction, self._residual_scales, self._residual_locations)\n        last_input = inputs[norm_prediction.name].isel(time=-1)\n        prediction = prediction + last_input\n        return prediction\n    else:\n        return unnormalize(norm_prediction, self._scales, self._locations)",
    "185": "def _subtract_input_and_normalize_target(self, inputs, target):\n    if target.sizes.get('time') != 1:\n        raise ValueError('normalization.InputsAndResiduals only supports wrapping predictorsthat predict a single timestep.')\n    if target.name in inputs:\n        target_residual = target\n        last_input = inputs[target.name].isel(time=-1)\n        target_residual = target_residual - last_input\n        return normalize(target_residual, self._residual_scales, self._residual_locations)\n    else:\n        return normalize(target, self._scales, self._locations)",
    "186": "def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> xarray.Dataset:\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_predictions = self._predictor(norm_inputs, targets_template, forcings=norm_forcings, **kwargs)\n    return xarray_tree.map_structure(lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred), norm_predictions)",
    "187": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> predictor_base.LossAndDiagnostics:\n    \"\"\"Returns the loss computed on normalized inputs and targets.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(lambda t: self._subtract_input_and_normalize_target(inputs, t), targets)\n    return self._predictor.loss(norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)",
    "188": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **kwargs) -> Tuple[predictor_base.LossAndDiagnostics, xarray.Dataset]:\n    \"\"\"The loss computed on normalized data, with unnormalized predictions.\"\"\"\n    norm_inputs = normalize(inputs, self._scales, self._locations)\n    norm_forcings = normalize(forcings, self._scales, self._locations)\n    norm_target_residuals = xarray_tree.map_structure(lambda t: self._subtract_input_and_normalize_target(inputs, t), targets)\n    ((loss, scalars), norm_predictions) = self._predictor.loss_and_predictions(norm_inputs, norm_target_residuals, forcings=norm_forcings, **kwargs)\n    predictions = xarray_tree.map_structure(lambda pred: self._unnormalize_prediction_and_add_input(inputs, pred), norm_predictions)\n    return ((loss, scalars), predictions)",
    "189": "class Predictor(abc.ABC):\n    \"\"\"A possibly-trainable predictor of weather, exposing an xarray-based API.\n\n  Typically wraps an underlying JAX model and handles translating the xarray\n  Dataset values to and from plain JAX arrays that are convenient for input to\n  (and output from) the underlying model.\n\n  Different subclasses may exist to wrap different kinds of underlying model,\n  e.g. models taking stacked inputs/outputs, models taking separate 2D and 3D\n  inputs/outputs, autoregressive models.\n\n  You can also implement a specific model directly as a Predictor if you want,\n  for example if it has quite specific/unique requirements for its input/output\n  or loss function, or if it's convenient to implement directly using xarray.\n  \"\"\"\n\n    @abc.abstractmethod\n    def __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> xarray.Dataset:\n        \"\"\"Makes predictions.\n\n    This is only used by the Experiment for inference / evaluation, with\n    training going via the .loss method. So it should default to making\n    predictions for evaluation, although you can also support making predictions\n    for use in the loss via an is_training argument -- see\n    LossFunctionPredictor which helps with that.\n\n    Args:\n      inputs: An xarray.Dataset of inputs.\n      targets_template: An xarray.Dataset or other mapping of xarray.DataArrays,\n        with the same shape as the targets, to demonstrate what kind of\n        predictions are required. You can use this to determine which variables,\n        levels and lead times must be predicted.\n        You are free to raise an error if you don't support predicting what is\n        requested.\n      forcings: An xarray.Dataset of forcings terms. Forcings are variables\n        that can be fed to the model, but do not need to be predicted. This is\n        often because this variable can be computed analytically (e.g. the toa\n        radiation of the sun is mostly a function of geometry) or are considered\n        to be controlled for the experiment (e.g., impose a scenario of C02\n        emission into the atmosphere). Unlike `inputs`, the `forcings` can\n        include information \"from the future\", that is, information at target\n        times specified in the `targets_template`.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      Predictions, as an xarray.Dataset or other mapping of DataArrays which\n      is capable of being evaluated against targets with shape given by\n      targets_template.\n      For probabilistic predictors which can return multiple samples from a\n      predictive distribution, these should (by convention) be returned along\n      an additional 'sample' dimension.\n    \"\"\"\n\n    def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n        \"\"\"Computes a training loss, for predictors that are trainable.\n\n    Why make this the Predictor's responsibility, rather than letting callers\n    compute their own loss function using predictions obtained from\n    Predictor.__call__?\n\n    Doing it this way gives Predictors more control over their training setup.\n    For example, some predictors may wish to train using different targets to\n    the ones they predict at evaluation time -- perhaps different lead times and\n    variables, perhaps training to predict transformed versions of targets\n    where the transform needs to be inverted at evaluation time, etc.\n\n    It's also necessary for generative models (VAEs, GANs, ...) where the\n    training loss is more complex and isn't expressible as a parameter-free\n    function of predictions and targets.\n\n    Args:\n      inputs: An xarray.Dataset.\n      targets: An xarray.Dataset or other mapping of xarray.DataArrays. See\n        docs on __call__ for an explanation about the targets.\n      forcings: xarray.Dataset of forcing terms.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n        You need not include the loss itself in this dict; it will be added for\n        you.\n    \"\"\"\n        del targets, forcings, optional_kwargs\n        batch_size = inputs.sizes['batch']\n        dummy_loss = xarray_jax.DataArray(jnp.zeros(batch_size), dims=('batch',))\n        return (dummy_loss, {})\n\n    def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> Tuple[LossAndDiagnostics, xarray.Dataset]:\n        \"\"\"Like .loss but also returns corresponding predictions.\n\n    Implementing this is optional as it's not used directly by the Experiment,\n    but it is required by autoregressive.Predictor when applying an inner\n    Predictor autoregressively at training time; we need a loss at each step but\n    also predictions to feed back in for the next step.\n\n    Note the loss itself may not be directly regressing the predictions towards\n    targets, the loss may be computed in terms of transformed predictions and\n    targets (or in some other way). For this reason we can't always cleanly\n    separate this into step 1: get predictions, step 2: compute loss from them,\n    hence the need for this combined method.\n\n    Args:\n      inputs:\n      targets:\n      forcings:\n      **optional_kwargs:\n        As for self.loss.\n\n    Returns:\n      (loss, diagnostics)\n        As for self.loss\n      predictions:\n        The predictions which the loss relates to. These should be of the same\n        shape as what you would get from\n        `self.__call__(inputs, targets_template=targets)`, and should be in the\n        same 'domain' as the inputs (i.e. they shouldn't be transformed\n        differently to how the predictor expects its inputs).\n    \"\"\"\n        raise NotImplementedError",
    "190": "@abc.abstractmethod\ndef __call__(self, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> xarray.Dataset:\n    \"\"\"Makes predictions.\n\n    This is only used by the Experiment for inference / evaluation, with\n    training going via the .loss method. So it should default to making\n    predictions for evaluation, although you can also support making predictions\n    for use in the loss via an is_training argument -- see\n    LossFunctionPredictor which helps with that.\n\n    Args:\n      inputs: An xarray.Dataset of inputs.\n      targets_template: An xarray.Dataset or other mapping of xarray.DataArrays,\n        with the same shape as the targets, to demonstrate what kind of\n        predictions are required. You can use this to determine which variables,\n        levels and lead times must be predicted.\n        You are free to raise an error if you don't support predicting what is\n        requested.\n      forcings: An xarray.Dataset of forcings terms. Forcings are variables\n        that can be fed to the model, but do not need to be predicted. This is\n        often because this variable can be computed analytically (e.g. the toa\n        radiation of the sun is mostly a function of geometry) or are considered\n        to be controlled for the experiment (e.g., impose a scenario of C02\n        emission into the atmosphere). Unlike `inputs`, the `forcings` can\n        include information \"from the future\", that is, information at target\n        times specified in the `targets_template`.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      Predictions, as an xarray.Dataset or other mapping of DataArrays which\n      is capable of being evaluated against targets with shape given by\n      targets_template.\n      For probabilistic predictors which can return multiple samples from a\n      predictive distribution, these should (by convention) be returned along\n      an additional 'sample' dimension.\n    \"\"\"",
    "191": "def loss(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> LossAndDiagnostics:\n    \"\"\"Computes a training loss, for predictors that are trainable.\n\n    Why make this the Predictor's responsibility, rather than letting callers\n    compute their own loss function using predictions obtained from\n    Predictor.__call__?\n\n    Doing it this way gives Predictors more control over their training setup.\n    For example, some predictors may wish to train using different targets to\n    the ones they predict at evaluation time -- perhaps different lead times and\n    variables, perhaps training to predict transformed versions of targets\n    where the transform needs to be inverted at evaluation time, etc.\n\n    It's also necessary for generative models (VAEs, GANs, ...) where the\n    training loss is more complex and isn't expressible as a parameter-free\n    function of predictions and targets.\n\n    Args:\n      inputs: An xarray.Dataset.\n      targets: An xarray.Dataset or other mapping of xarray.DataArrays. See\n        docs on __call__ for an explanation about the targets.\n      forcings: xarray.Dataset of forcing terms.\n      **optional_kwargs: Implementations may support extra optional kwargs,\n        provided they set appropriate defaults for them.\n\n    Returns:\n      loss: A DataArray with dimensions ('batch',) containing losses for each\n        element of the batch. These will be averaged to give the final\n        loss, locally and across replicas.\n      diagnostics: Mapping of additional quantities to log by name alongside the\n        loss. These will will typically correspond to terms in the loss. They\n        should also have dimensions ('batch',) and will be averaged over the\n        batch before logging.\n        You need not include the loss itself in this dict; it will be added for\n        you.\n    \"\"\"\n    del targets, forcings, optional_kwargs\n    batch_size = inputs.sizes['batch']\n    dummy_loss = xarray_jax.DataArray(jnp.zeros(batch_size), dims=('batch',))\n    return (dummy_loss, {})",
    "192": "def loss_and_predictions(self, inputs: xarray.Dataset, targets: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> Tuple[LossAndDiagnostics, xarray.Dataset]:\n    \"\"\"Like .loss but also returns corresponding predictions.\n\n    Implementing this is optional as it's not used directly by the Experiment,\n    but it is required by autoregressive.Predictor when applying an inner\n    Predictor autoregressively at training time; we need a loss at each step but\n    also predictions to feed back in for the next step.\n\n    Note the loss itself may not be directly regressing the predictions towards\n    targets, the loss may be computed in terms of transformed predictions and\n    targets (or in some other way). For this reason we can't always cleanly\n    separate this into step 1: get predictions, step 2: compute loss from them,\n    hence the need for this combined method.\n\n    Args:\n      inputs:\n      targets:\n      forcings:\n      **optional_kwargs:\n        As for self.loss.\n\n    Returns:\n      (loss, diagnostics)\n        As for self.loss\n      predictions:\n        The predictions which the loss relates to. These should be of the same\n        shape as what you would get from\n        `self.__call__(inputs, targets_template=targets)`, and should be in the\n        same 'domain' as the inputs (i.e. they shouldn't be transformed\n        differently to how the predictor expects its inputs).\n    \"\"\"\n    raise NotImplementedError",
    "194": "def __call__(self, rng: chex.PRNGKey, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, **optional_kwargs) -> xarray.Dataset:\n    ...",
    "195": "def chunked_prediction(predictor_fn: PredictorFn, rng: chex.PRNGKey, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, num_steps_per_chunk: int=1, verbose: bool=False) -> xarray.Dataset:\n    \"\"\"Outputs a long trajectory by iteratively concatenating chunked predictions.\n\n  Args:\n    predictor_fn: Function to use to make predictions for each chunk.\n    rng: Random key.\n    inputs: Inputs for the model.\n    targets_template: Template for the target prediction, requires targets\n        equispaced in time.\n    forcings: Optional forcing for the model.\n    num_steps_per_chunk: How many of the steps in `targets_template` to predict\n        at each call of `predictor_fn`. It must evenly divide the number of\n        steps in `targets_template`.\n    verbose: Whether to log the current chunk being predicted.\n\n  Returns:\n    Predictions for the targets template.\n\n  \"\"\"\n    chunks_list = []\n    for prediction_chunk in chunked_prediction_generator(predictor_fn=predictor_fn, rng=rng, inputs=inputs, targets_template=targets_template, forcings=forcings, num_steps_per_chunk=num_steps_per_chunk, verbose=verbose):\n        chunks_list.append(jax.device_get(prediction_chunk))\n    return xarray.concat(chunks_list, dim='time')",
    "196": "def chunked_prediction_generator(predictor_fn: PredictorFn, rng: chex.PRNGKey, inputs: xarray.Dataset, targets_template: xarray.Dataset, forcings: xarray.Dataset, num_steps_per_chunk: int=1, verbose: bool=False) -> Iterator[xarray.Dataset]:\n    \"\"\"Outputs a long trajectory by yielding chunked predictions.\n\n  Args:\n    predictor_fn: Function to use to make predictions for each chunk.\n    rng: Random key.\n    inputs: Inputs for the model.\n    targets_template: Template for the target prediction, requires targets\n        equispaced in time.\n    forcings: Optional forcing for the model.\n    num_steps_per_chunk: How many of the steps in `targets_template` to predict\n        at each call of `predictor_fn`. It must evenly divide the number of\n        steps in `targets_template`.\n    verbose: Whether to log the current chunk being predicted.\n\n  Yields:\n    The predictions for each chunked step of the chunked rollout, such as\n    if all predictions are concatenated in time this would match the targets\n    template in structure.\n\n  \"\"\"\n    inputs = xarray.Dataset(inputs)\n    targets_template = xarray.Dataset(targets_template)\n    forcings = xarray.Dataset(forcings)\n    if 'datetime' in inputs.coords:\n        del inputs.coords['datetime']\n    if 'datetime' in targets_template.coords:\n        output_datetime = targets_template.coords['datetime']\n        del targets_template.coords['datetime']\n    else:\n        output_datetime = None\n    if 'datetime' in forcings.coords:\n        del forcings.coords['datetime']\n    num_target_steps = targets_template.dims['time']\n    (num_chunks, remainder) = divmod(num_target_steps, num_steps_per_chunk)\n    if remainder != 0:\n        raise ValueError(f'The number of steps per chunk {num_steps_per_chunk} must evenly divide the number of target steps {num_target_steps} ')\n    if len(np.unique(np.diff(targets_template.coords['time'].data))) > 1:\n        raise ValueError('The targets time coordinates must be evenly spaced')\n    targets_chunk_time = targets_template.time.isel(time=slice(0, num_steps_per_chunk))\n    current_inputs = inputs\n    for chunk_index in range(num_chunks):\n        if verbose:\n            logging.info('Chunk %d/%d', chunk_index, num_chunks)\n            logging.flush()\n        target_offset = num_steps_per_chunk * chunk_index\n        target_slice = slice(target_offset, target_offset + num_steps_per_chunk)\n        current_targets_template = targets_template.isel(time=target_slice)\n        actual_target_time = current_targets_template.coords['time']\n        current_targets_template = current_targets_template.assign_coords(time=targets_chunk_time).compute()\n        current_forcings = forcings.isel(time=target_slice)\n        current_forcings = current_forcings.assign_coords(time=targets_chunk_time)\n        current_forcings = current_forcings.compute()\n        (rng, this_rng) = jax.random.split(rng)\n        predictions = predictor_fn(rng=this_rng, inputs=current_inputs, targets_template=current_targets_template, forcings=current_forcings)\n        next_frame = xarray.merge([predictions, current_forcings])\n        next_inputs = _get_next_inputs(current_inputs, next_frame)\n        next_inputs = next_inputs.assign_coords(time=current_inputs.coords['time'])\n        current_inputs = next_inputs\n        predictions = predictions.assign_coords(time=actual_target_time)\n        if output_datetime is not None:\n            predictions.coords['datetime'] = output_datetime.isel(time=target_slice)\n        yield predictions\n        del predictions",
    "197": "def _get_next_inputs(prev_inputs: xarray.Dataset, next_frame: xarray.Dataset) -> xarray.Dataset:\n    \"\"\"Computes next inputs, from previous inputs and predictions.\"\"\"\n    non_predicted_or_forced_inputs = list(set(prev_inputs.keys()) - set(next_frame.keys()))\n    if 'time' in prev_inputs[non_predicted_or_forced_inputs].dims:\n        raise ValueError('Found an input with a time index that is not predicted or forced.')\n    next_inputs_keys = list(set(next_frame.keys()).intersection(set(prev_inputs.keys())))\n    next_inputs = next_frame[next_inputs_keys]\n    num_inputs = prev_inputs.dims['time']\n    return xarray.concat([prev_inputs, next_inputs], dim='time', data_vars='different').tail(time=num_inputs)",
    "198": "def extend_targets_template(targets_template: xarray.Dataset, required_num_steps: int) -> xarray.Dataset:\n    \"\"\"Extends `targets_template` to `required_num_steps` with lazy arrays.\n\n  It uses lazy dask arrays of zeros, so it does not require instantiating the\n  array in memory.\n\n  Args:\n    targets_template: Input template to extend.\n    required_num_steps: Number of steps required in the returned template.\n\n  Returns:\n    `xarray.Dataset` identical in variables and timestep to `targets_template`\n    full of `dask.array.zeros` such that the time axis has `required_num_steps`.\n\n  \"\"\"\n    time = targets_template.coords['time']\n    timestep = time[0].data\n    if time.shape[0] > 1:\n        assert np.all(timestep == time[1:] - time[:-1])\n    extended_time = (np.arange(required_num_steps) + 1) * timestep\n    if 'datetime' in targets_template.coords:\n        datetime = targets_template.coords['datetime']\n        extended_datetime = datetime[0].data - timestep + extended_time\n    else:\n        extended_datetime = None\n    datetime = targets_template.coords['time']\n\n    def extend_time(data_array: xarray.DataArray) -> xarray.DataArray:\n        dims = data_array.dims\n        shape = list(data_array.shape)\n        shape[dims.index('time')] = required_num_steps\n        dask_data = dask.array.zeros(shape=tuple(shape), chunks=-1, dtype=data_array.dtype)\n        coords = dict(data_array.coords)\n        coords['time'] = extended_time\n        if extended_datetime is not None:\n            coords['datetime'] = ('time', extended_datetime)\n        return xarray.DataArray(dims=dims, data=dask_data, coords=coords)\n    return xarray_tree.map_structure(extend_time, targets_template)",
    "199": "def extend_time(data_array: xarray.DataArray) -> xarray.DataArray:\n    dims = data_array.dims\n    shape = list(data_array.shape)\n    shape[dims.index('time')] = required_num_steps\n    dask_data = dask.array.zeros(shape=tuple(shape), chunks=-1, dtype=data_array.dtype)\n    coords = dict(data_array.coords)\n    coords['time'] = extended_time\n    if extended_datetime is not None:\n        coords['datetime'] = ('time', extended_datetime)\n    return xarray.DataArray(dims=dims, data=dask_data, coords=coords)",
    "200": "def _get_grid_lat_lon_coords(num_lat: int, num_lon: int) -> tuple[np.ndarray, np.ndarray]:\n    \"\"\"Generates a linear latitude-longitude grid of the given size.\n\n  Args:\n    num_lat: Size of the latitude dimension of the grid.\n    num_lon: Size of the longitude dimension of the grid.\n\n  Returns:\n    A tuple `(lat, lon)` containing 1D arrays with the latitude and longitude\n    coordinates in degrees of the generated grid.\n  \"\"\"\n    lat = np.linspace(-90.0, 90.0, num=num_lat, endpoint=True)\n    lon = np.linspace(0.0, 360.0, num=num_lon, endpoint=False)\n    return (lat, lon)",
    "201": "class SolarRadiationTest(parameterized.TestCase):\n\n    def setUp(self):\n        super().setUp()\n        np.random.seed(0)\n\n    def test_missing_dim_raises_value_error(self):\n        data = xa.DataArray(np.random.randn(2, 2), coords=[np.array([0.1, 0.2]), np.array([0.0, 0.5])], dims=['lon', 'x'])\n        with self.assertRaisesRegex(ValueError, '.* dimensions are missing in `data_array_like`.'):\n            solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=360)\n\n    def test_missing_coordinate_raises_value_error(self):\n        data = xa.Dataset(data_vars={'var1': (['x', 'lat', 'lon'], np.random.randn(2, 3, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2]), 'lon': np.array([0.0, 0.5])})\n        with self.assertRaisesRegex(ValueError, '.* coordinates are missing in `data_array_like`.'):\n            solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=360)\n\n    def test_shape_multiple_timestamps(self):\n        data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.random.randn(2, 4, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2, 0.3]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n        actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=2)\n        self.assertEqual(('time', 'lat', 'lon'), actual.dims)\n        self.assertEqual((2, 4, 2), actual.shape)\n\n    def test_shape_single_timestamp(self):\n        data = xa.Dataset(data_vars={'var1': (['lat', 'lon'], np.random.randn(4, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2, 0.3]), 'lon': np.array([0.0, 0.5]), 'datetime': np.datetime64(10, 'D')})\n        actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=2)\n        self.assertEqual(('lat', 'lon'), actual.dims)\n        self.assertEqual((4, 2), actual.shape)\n\n    @parameterized.named_parameters(dict(testcase_name='one_timestamp_jitted', periods=1, repeats=3, use_jit=True), dict(testcase_name='one_timestamp_non_jitted', periods=1, repeats=3, use_jit=False), dict(testcase_name='ten_timestamps_non_jitted', periods=10, repeats=1, use_jit=False))\n    def test_full_spatial_resolution(self, periods: int, repeats: int, use_jit: bool):\n        timestamps = pd.date_range(start='2023-09-25', periods=periods, freq='6h')\n        (lat, lon) = _get_grid_lat_lon_coords(num_lat=721, num_lon=1440)\n\n        def benchmark() -> None:\n            solar_radiation.get_toa_incident_solar_radiation(timestamps, lat, lon, integration_period='1h', num_integration_bins=360, use_jit=use_jit).block_until_ready()\n        results = timeit.repeat(benchmark, repeat=repeats, number=1)\n        logging.info('Times to compute `tisr` for input of shape `%d, %d, %d` (seconds): %s', len(timestamps), len(lat), len(lon), np.array2string(np.array(results), precision=1))",
    "203": "def test_missing_dim_raises_value_error(self):\n    data = xa.DataArray(np.random.randn(2, 2), coords=[np.array([0.1, 0.2]), np.array([0.0, 0.5])], dims=['lon', 'x'])\n    with self.assertRaisesRegex(ValueError, '.* dimensions are missing in `data_array_like`.'):\n        solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=360)",
    "204": "def test_missing_coordinate_raises_value_error(self):\n    data = xa.Dataset(data_vars={'var1': (['x', 'lat', 'lon'], np.random.randn(2, 3, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2]), 'lon': np.array([0.0, 0.5])})\n    with self.assertRaisesRegex(ValueError, '.* coordinates are missing in `data_array_like`.'):\n        solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=360)",
    "205": "def test_shape_multiple_timestamps(self):\n    data = xa.Dataset(data_vars={'var1': (['time', 'lat', 'lon'], np.random.randn(2, 4, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2, 0.3]), 'lon': np.array([0.0, 0.5]), 'time': np.array([100, 200], dtype='timedelta64[s]'), 'datetime': xa.Variable('time', np.array([10, 20], dtype='datetime64[D]'))})\n    actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=2)\n    self.assertEqual(('time', 'lat', 'lon'), actual.dims)\n    self.assertEqual((2, 4, 2), actual.shape)",
    "206": "def test_shape_single_timestamp(self):\n    data = xa.Dataset(data_vars={'var1': (['lat', 'lon'], np.random.randn(4, 2))}, coords={'lat': np.array([0.0, 0.1, 0.2, 0.3]), 'lon': np.array([0.0, 0.5]), 'datetime': np.datetime64(10, 'D')})\n    actual = solar_radiation.get_toa_incident_solar_radiation_for_xarray(data, integration_period='1h', num_integration_bins=2)\n    self.assertEqual(('lat', 'lon'), actual.dims)\n    self.assertEqual((4, 2), actual.shape)",
    "207": "@parameterized.named_parameters(dict(testcase_name='one_timestamp_jitted', periods=1, repeats=3, use_jit=True), dict(testcase_name='one_timestamp_non_jitted', periods=1, repeats=3, use_jit=False), dict(testcase_name='ten_timestamps_non_jitted', periods=10, repeats=1, use_jit=False))\ndef test_full_spatial_resolution(self, periods: int, repeats: int, use_jit: bool):\n    timestamps = pd.date_range(start='2023-09-25', periods=periods, freq='6h')\n    (lat, lon) = _get_grid_lat_lon_coords(num_lat=721, num_lon=1440)\n\n    def benchmark() -> None:\n        solar_radiation.get_toa_incident_solar_radiation(timestamps, lat, lon, integration_period='1h', num_integration_bins=360, use_jit=use_jit).block_until_ready()\n    results = timeit.repeat(benchmark, repeat=repeats, number=1)\n    logging.info('Times to compute `tisr` for input of shape `%d, %d, %d` (seconds): %s', len(timestamps), len(lat), len(lon), np.array2string(np.array(results), precision=1))",
    "209": "class GetTsiTest(parameterized.TestCase):\n\n    @parameterized.named_parameters(dict(testcase_name='reference_tsi_data', loader=solar_radiation.reference_tsi_data, expected_tsi=np.array([1361.0])), dict(testcase_name='era5_tsi_data', loader=solar_radiation.era5_tsi_data, expected_tsi=np.array([1360.944])))\n    def test_mid_2020_lookup(self, loader: solar_radiation.TsiDataLoader, expected_tsi: np.ndarray):\n        tsi_data = loader()\n        tsi = solar_radiation.get_tsi([np.datetime64('2020-07-02T00:00:00')], tsi_data)\n        np.testing.assert_allclose(expected_tsi, tsi)\n\n    @parameterized.named_parameters(dict(testcase_name='beginning_2020_left_boundary', timestamps=[np.datetime64('2020-01-01T00:00:00')], expected_tsi=np.array([1000.0])), dict(testcase_name='mid_2020_exact', timestamps=[np.datetime64('2020-07-02T00:00:00')], expected_tsi=np.array([1000.0])), dict(testcase_name='beginning_2021_interpolated', timestamps=[np.datetime64('2021-01-01T00:00:00')], expected_tsi=np.array([1150.0])), dict(testcase_name='mid_2021_lookup', timestamps=[np.datetime64('2021-07-02T12:00:00')], expected_tsi=np.array([1300.0])), dict(testcase_name='beginning_2022_interpolated', timestamps=[np.datetime64('2022-01-01T00:00:00')], expected_tsi=np.array([1250.0])), dict(testcase_name='mid_2022_lookup', timestamps=[np.datetime64('2022-07-02T12:00:00')], expected_tsi=np.array([1200.0])), dict(testcase_name='beginning_2023_right_boundary', timestamps=[np.datetime64('2023-01-01T00:00:00')], expected_tsi=np.array([1200.0])))\n    def test_interpolation(self, timestamps: Sequence[np.datetime64], expected_tsi: np.ndarray):\n        tsi_data = xa.DataArray(np.array([1000.0, 1300.0, 1200.0]), dims=['time'], coords={'time': np.array([2020.5, 2021.5, 2022.5])})\n        tsi = solar_radiation.get_tsi(timestamps, tsi_data)\n        np.testing.assert_allclose(expected_tsi, tsi)",
    "210": "@parameterized.named_parameters(dict(testcase_name='reference_tsi_data', loader=solar_radiation.reference_tsi_data, expected_tsi=np.array([1361.0])), dict(testcase_name='era5_tsi_data', loader=solar_radiation.era5_tsi_data, expected_tsi=np.array([1360.944])))\ndef test_mid_2020_lookup(self, loader: solar_radiation.TsiDataLoader, expected_tsi: np.ndarray):\n    tsi_data = loader()\n    tsi = solar_radiation.get_tsi([np.datetime64('2020-07-02T00:00:00')], tsi_data)\n    np.testing.assert_allclose(expected_tsi, tsi)",
    "211": "@parameterized.named_parameters(dict(testcase_name='beginning_2020_left_boundary', timestamps=[np.datetime64('2020-01-01T00:00:00')], expected_tsi=np.array([1000.0])), dict(testcase_name='mid_2020_exact', timestamps=[np.datetime64('2020-07-02T00:00:00')], expected_tsi=np.array([1000.0])), dict(testcase_name='beginning_2021_interpolated', timestamps=[np.datetime64('2021-01-01T00:00:00')], expected_tsi=np.array([1150.0])), dict(testcase_name='mid_2021_lookup', timestamps=[np.datetime64('2021-07-02T12:00:00')], expected_tsi=np.array([1300.0])), dict(testcase_name='beginning_2022_interpolated', timestamps=[np.datetime64('2022-01-01T00:00:00')], expected_tsi=np.array([1250.0])), dict(testcase_name='mid_2022_lookup', timestamps=[np.datetime64('2022-07-02T12:00:00')], expected_tsi=np.array([1200.0])), dict(testcase_name='beginning_2023_right_boundary', timestamps=[np.datetime64('2023-01-01T00:00:00')], expected_tsi=np.array([1200.0])))\ndef test_interpolation(self, timestamps: Sequence[np.datetime64], expected_tsi: np.ndarray):\n    tsi_data = xa.DataArray(np.array([1000.0, 1300.0, 1200.0]), dims=['time'], coords={'time': np.array([2020.5, 2021.5, 2022.5])})\n    tsi = solar_radiation.get_tsi(timestamps, tsi_data)\n    np.testing.assert_allclose(expected_tsi, tsi)",
    "213": "class EdgesIndices(NamedTuple):\n    \"\"\"Represents indices to nodes adjacent to the edges.\"\"\"\n    senders: ArrayLike\n    receivers: ArrayLike",
    "215": "class Context(NamedTuple):\n    n_graph: ArrayLike\n    features: ArrayLikeTree",
    "217": "class TypedGraph(NamedTuple):\n    \"\"\"A graph with typed nodes and edges.\n\n  A typed graph is made of a context, multiple sets of nodes and multiple\n  sets of edges connecting those nodes (as indicated by the EdgeSetKey).\n  \"\"\"\n    context: Context\n    nodes: Mapping[str, NodeSet]\n    edges: Mapping[EdgeSetKey, EdgeSet]\n\n    def edge_key_by_name(self, name: str) -> EdgeSetKey:\n        found_key = [k for k in self.edges.keys() if k.name == name]\n        if len(found_key) != 1:\n            raise KeyError(\"invalid edge key '{}'. Available edges: [{}]\".format(name, ', '.join((x.name for x in self.edges.keys()))))\n        return found_key[0]\n\n    def edge_by_name(self, name: str) -> EdgeSet:\n        return self.edges[self.edge_key_by_name(name)]",
    "218": "def edge_key_by_name(self, name: str) -> EdgeSetKey:\n    found_key = [k for k in self.edges.keys() if k.name == name]\n    if len(found_key) != 1:\n        raise KeyError(\"invalid edge key '{}'. Available edges: [{}]\".format(name, ', '.join((x.name for x in self.edges.keys()))))\n    return found_key[0]",
    "220": "def GraphNetwork(update_edge_fn: Mapping[str, jraph.GNUpdateEdgeFn], update_node_fn: Mapping[str, GNUpdateNodeFn], update_global_fn: Optional[GNUpdateGlobalFn]=None, aggregate_edges_for_nodes_fn: jraph.AggregateEdgesToNodesFn=jraph.segment_sum, aggregate_nodes_for_globals_fn: jraph.AggregateNodesToGlobalsFn=jraph.segment_sum, aggregate_edges_for_globals_fn: jraph.AggregateEdgesToGlobalsFn=jraph.segment_sum):\n    \"\"\"Returns a method that applies a configured GraphNetwork.\n\n  This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n  extended to Typed Graphs with multiple edge sets and node sets and extended to\n  allow aggregating not only edges received by the nodes, but also edges sent by\n  the nodes.\n\n  Example usage::\n\n    gn = GraphNetwork(update_edge_function,\n    update_node_function, **kwargs)\n    # Conduct multiple rounds of message passing with the same parameters:\n    for _ in range(num_message_passing_steps):\n      graph = gn(graph)\n\n  Args:\n    update_edge_fn: mapping of functions used to update a subset of the edge\n      types, indexed by edge type name.\n    update_node_fn: mapping of functions used to update a subset of the node\n      types, indexed by node type name.\n    update_global_fn: function used to update the globals or None to deactivate\n      globals updates.\n    aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n      node.\n    aggregate_nodes_for_globals_fn: function used to aggregate the nodes for the\n      globals.\n    aggregate_edges_for_globals_fn: function used to aggregate the edges for the\n      globals.\n\n  Returns:\n    A method that applies the configured GraphNetwork.\n  \"\"\"\n\n    def _apply_graph_net(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        \"\"\"Applies a configured GraphNetwork to a graph.\n\n    This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n    extended to Typed Graphs with multiple edge sets and node sets and extended\n    to allow aggregating not only edges received by the nodes, but also edges\n    sent by the nodes.\n\n    Args:\n      graph: a `TypedGraph` containing the graph.\n\n    Returns:\n      Updated `TypedGraph`.\n    \"\"\"\n        updated_graph = graph\n        updated_edges = dict(updated_graph.edges)\n        for (edge_set_name, edge_fn) in update_edge_fn.items():\n            edge_set_key = graph.edge_key_by_name(edge_set_name)\n            updated_edges[edge_set_key] = _edge_update(updated_graph, edge_fn, edge_set_key)\n        updated_graph = updated_graph._replace(edges=updated_edges)\n        updated_nodes = dict(updated_graph.nodes)\n        for (node_set_key, node_fn) in update_node_fn.items():\n            updated_nodes[node_set_key] = _node_update(updated_graph, node_fn, node_set_key, aggregate_edges_for_nodes_fn)\n        updated_graph = updated_graph._replace(nodes=updated_nodes)\n        if update_global_fn:\n            updated_context = _global_update(updated_graph, update_global_fn, aggregate_edges_for_globals_fn, aggregate_nodes_for_globals_fn)\n            updated_graph = updated_graph._replace(context=updated_context)\n        return updated_graph\n    return _apply_graph_net",
    "221": "def _apply_graph_net(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    \"\"\"Applies a configured GraphNetwork to a graph.\n\n    This implementation follows Algorithm 1 in https://arxiv.org/abs/1806.01261\n    extended to Typed Graphs with multiple edge sets and node sets and extended\n    to allow aggregating not only edges received by the nodes, but also edges\n    sent by the nodes.\n\n    Args:\n      graph: a `TypedGraph` containing the graph.\n\n    Returns:\n      Updated `TypedGraph`.\n    \"\"\"\n    updated_graph = graph\n    updated_edges = dict(updated_graph.edges)\n    for (edge_set_name, edge_fn) in update_edge_fn.items():\n        edge_set_key = graph.edge_key_by_name(edge_set_name)\n        updated_edges[edge_set_key] = _edge_update(updated_graph, edge_fn, edge_set_key)\n    updated_graph = updated_graph._replace(edges=updated_edges)\n    updated_nodes = dict(updated_graph.nodes)\n    for (node_set_key, node_fn) in update_node_fn.items():\n        updated_nodes[node_set_key] = _node_update(updated_graph, node_fn, node_set_key, aggregate_edges_for_nodes_fn)\n    updated_graph = updated_graph._replace(nodes=updated_nodes)\n    if update_global_fn:\n        updated_context = _global_update(updated_graph, update_global_fn, aggregate_edges_for_globals_fn, aggregate_nodes_for_globals_fn)\n        updated_graph = updated_graph._replace(context=updated_context)\n    return updated_graph",
    "222": "def _edge_update(graph, edge_fn, edge_set_key):\n    \"\"\"Updates an edge set of a given key.\"\"\"\n    sender_nodes = graph.nodes[edge_set_key.node_sets[0]]\n    receiver_nodes = graph.nodes[edge_set_key.node_sets[1]]\n    edge_set = graph.edges[edge_set_key]\n    senders = edge_set.indices.senders\n    receivers = edge_set.indices.receivers\n    sent_attributes = tree.tree_map(lambda n: n[senders], sender_nodes.features)\n    received_attributes = tree.tree_map(lambda n: n[receivers], receiver_nodes.features)\n    n_edge = edge_set.n_edge\n    sum_n_edge = senders.shape[0]\n    global_features = tree.tree_map(lambda g: jnp.repeat(g, n_edge, axis=0, total_repeat_length=sum_n_edge), graph.context.features)\n    new_features = edge_fn(edge_set.features, sent_attributes, received_attributes, global_features)\n    return edge_set._replace(features=new_features)",
    "223": "def _node_update(graph, node_fn, node_set_key, aggregation_fn):\n    \"\"\"Updates an edge set of a given key.\"\"\"\n    node_set = graph.nodes[node_set_key]\n    sum_n_node = tree.tree_leaves(node_set.features)[0].shape[0]\n    sent_features = {}\n    for (edge_set_key, edge_set) in graph.edges.items():\n        sender_node_set_key = edge_set_key.node_sets[0]\n        if sender_node_set_key == node_set_key:\n            assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n            senders = edge_set.indices.senders\n            sent_features[edge_set_key.name] = tree.tree_map(lambda e: aggregation_fn(e, senders, sum_n_node), edge_set.features)\n    received_features = {}\n    for (edge_set_key, edge_set) in graph.edges.items():\n        receiver_node_set_key = edge_set_key.node_sets[1]\n        if receiver_node_set_key == node_set_key:\n            assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n            receivers = edge_set.indices.receivers\n            received_features[edge_set_key.name] = tree.tree_map(lambda e: aggregation_fn(e, receivers, sum_n_node), edge_set.features)\n    n_node = node_set.n_node\n    global_features = tree.tree_map(lambda g: jnp.repeat(g, n_node, axis=0, total_repeat_length=sum_n_node), graph.context.features)\n    new_features = node_fn(node_set.features, sent_features, received_features, global_features)\n    return node_set._replace(features=new_features)",
    "224": "def _global_update(graph, global_fn, edge_aggregation_fn, node_aggregation_fn):\n    \"\"\"Updates an edge set of a given key.\"\"\"\n    n_graph = graph.context.n_graph.shape[0]\n    graph_idx = jnp.arange(n_graph)\n    edge_features = {}\n    for (edge_set_key, edge_set) in graph.edges.items():\n        assert isinstance(edge_set.indices, typed_graph.EdgesIndices)\n        sum_n_edge = edge_set.indices.senders.shape[0]\n        edge_gr_idx = jnp.repeat(graph_idx, edge_set.n_edge, axis=0, total_repeat_length=sum_n_edge)\n        edge_features[edge_set_key.name] = tree.tree_map(lambda e: edge_aggregation_fn(e, edge_gr_idx, n_graph), edge_set.features)\n    node_features = {}\n    for (node_set_key, node_set) in graph.nodes.items():\n        sum_n_node = tree.tree_leaves(node_set.features)[0].shape[0]\n        node_gr_idx = jnp.repeat(graph_idx, node_set.n_node, axis=0, total_repeat_length=sum_n_node)\n        node_features[node_set_key] = tree.tree_map(lambda n: node_aggregation_fn(n, node_gr_idx, n_graph), node_set.features)\n    new_features = global_fn(node_features, edge_features, graph.context.features)\n    return graph.context._replace(features=new_features)",
    "225": "def InteractionNetwork(update_edge_fn: Mapping[str, jraph.InteractionUpdateEdgeFn], update_node_fn: Mapping[str, Union[InteractionUpdateNodeFn, InteractionUpdateNodeFnNoSentEdges]], aggregate_edges_for_nodes_fn: jraph.AggregateEdgesToNodesFn=jraph.segment_sum, include_sent_messages_in_node_update: bool=False):\n    \"\"\"Returns a method that applies a configured InteractionNetwork.\n\n  An interaction network computes interactions on the edges based on the\n  previous edges features, and on the features of the nodes sending into those\n  edges. It then updates the nodes based on the incoming updated edges.\n  See https://arxiv.org/abs/1612.00222 for more details.\n\n  This implementation extends the behavior to `TypedGraphs` adding an option\n  to include edge features for which a node is a sender in the arguments to\n  the node update function.\n\n  Args:\n    update_edge_fn: mapping of functions used to update a subset of the edge\n      types, indexed by edge type name.\n    update_node_fn: mapping of functions used to update a subset of the node\n      types, indexed by node type name.\n    aggregate_edges_for_nodes_fn: function used to aggregate messages to each\n      node.\n    include_sent_messages_in_node_update: pass edge features for which a node is\n      a sender to the node update function.\n  \"\"\"\n    wrapped_update_edge_fn = tree.tree_map(lambda fn: lambda e, s, r, g: fn(e, s, r), update_edge_fn)\n    if include_sent_messages_in_node_update:\n        wrapped_update_node_fn = tree.tree_map(lambda fn: lambda n, s, r, g: fn(n, s, r), update_node_fn)\n    else:\n        wrapped_update_node_fn = tree.tree_map(lambda fn: lambda n, s, r, g: fn(n, r), update_node_fn)\n    return GraphNetwork(update_edge_fn=wrapped_update_edge_fn, update_node_fn=wrapped_update_node_fn, aggregate_edges_for_nodes_fn=aggregate_edges_for_nodes_fn)",
    "226": "def GraphMapFeatures(embed_edge_fn: Optional[Mapping[str, jraph.EmbedEdgeFn]]=None, embed_node_fn: Optional[Mapping[str, jraph.EmbedNodeFn]]=None, embed_global_fn: Optional[jraph.EmbedGlobalFn]=None):\n    \"\"\"Returns function which embeds the components of a graph independently.\n\n  Args:\n    embed_edge_fn: mapping of functions used to embed each edge type,\n      indexed by edge type name.\n    embed_node_fn: mapping of functions used to embed each node type,\n      indexed by node type name.\n    embed_global_fn: function used to embed the globals.\n  \"\"\"\n\n    def _embed(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n        updated_edges = dict(graph.edges)\n        if embed_edge_fn:\n            for (edge_set_name, embed_fn) in embed_edge_fn.items():\n                edge_set_key = graph.edge_key_by_name(edge_set_name)\n                edge_set = graph.edges[edge_set_key]\n                updated_edges[edge_set_key] = edge_set._replace(features=embed_fn(edge_set.features))\n        updated_nodes = dict(graph.nodes)\n        if embed_node_fn:\n            for (node_set_key, embed_fn) in embed_node_fn.items():\n                node_set = graph.nodes[node_set_key]\n                updated_nodes[node_set_key] = node_set._replace(features=embed_fn(node_set.features))\n        updated_context = graph.context\n        if embed_global_fn:\n            updated_context = updated_context._replace(features=embed_global_fn(updated_context.features))\n        return graph._replace(edges=updated_edges, nodes=updated_nodes, context=updated_context)\n    return _embed",
    "227": "def _embed(graph: typed_graph.TypedGraph) -> typed_graph.TypedGraph:\n    updated_edges = dict(graph.edges)\n    if embed_edge_fn:\n        for (edge_set_name, embed_fn) in embed_edge_fn.items():\n            edge_set_key = graph.edge_key_by_name(edge_set_name)\n            edge_set = graph.edges[edge_set_key]\n            updated_edges[edge_set_key] = edge_set._replace(features=embed_fn(edge_set.features))\n    updated_nodes = dict(graph.nodes)\n    if embed_node_fn:\n        for (node_set_key, embed_fn) in embed_node_fn.items():\n            node_set = graph.nodes[node_set_key]\n            updated_nodes[node_set_key] = node_set._replace(features=embed_fn(node_set.features))\n    updated_context = graph.context\n    if embed_global_fn:\n        updated_context = updated_context._replace(features=embed_global_fn(updated_context.features))\n    return graph._replace(edges=updated_edges, nodes=updated_nodes, context=updated_context)",
    "229": "def DataArray(data, coords=None, dims=None, name=None, attrs=None, jax_coords=None) -> xarray.DataArray:\n    \"\"\"Like xarray.DataArray, but supports using JAX arrays.\n\n  Args:\n    data: As for xarray.DataArray, except jax arrays are also supported.\n    coords: Coordinates for the array, see xarray.DataArray. These coordinates\n      must be based on plain numpy arrays or something convertible to plain\n      numpy arrays. Their values will form a static part of the data structure\n      from the point of view of jax.tree_util. In particular this means these\n      coordinates will be passed as plain numpy arrays even inside a JIT'd\n      function, and the JIT'd function will be recompiled under the hood if the\n      coordinates of DataArrays passed into it change.\n      If this is not convenient for you, see also jax_coords below.\n    dims: See xarray.DataArray.\n    name: See xarray.DataArray.\n    attrs: See xarray.DataArray.\n    jax_coords: Additional coordinates, which *can* use JAX arrays. These\n      coordinates will be treated as JAX data from the point of view of\n      jax.tree_util, that means when JIT'ing they will be passed as tracers and\n      computation involving them will be JIT'd.\n      Unfortunately a side-effect of this is that they can't be used as index\n      coordinates (because xarray's indexing logic is not JIT-able). If you\n      specify a coordinate with the same name as a dimension here, it will not\n      be set as an index coordinate; this behaviour is different to the default\n      for `coords`, and it means that things like `.sel` based on the jax\n      coordinate will not work.\n      Note we require `jax_coords` to be explicitly specified via a different\n      constructor argument to `coords`, rather than just looking for jax arrays\n      within the `coords` and treating them differently. This is because it\n      affects the way jax.tree_util treats them, which is somewhat orthogonal to\n      whether the value is passed in as numpy or not, and generally needs to be\n      handled consistently so is something we encourage explicit control over.\n\n  Returns:\n    An instance of xarray.DataArray. Where JAX arrays are used as data or\n    coords, they will be wrapped with JaxArrayWrapper and can be unwrapped via\n    `unwrap` and `unwrap_data`.\n  \"\"\"\n    result = xarray.DataArray(wrap(data), dims=dims, name=name, attrs=attrs or {})\n    return assign_coords(result, coords=coords, jax_coords=jax_coords)",
    "230": "def Dataset(data_vars, coords=None, attrs=None, jax_coords=None) -> xarray.Dataset:\n    \"\"\"Like xarray.Dataset, but can wrap JAX arrays.\n\n  Args:\n    data_vars: As for xarray.Dataset, except jax arrays are also supported.\n    coords: Coordinates for the dataset, see xarray.Dataset. These coordinates\n      must be based on plain numpy arrays or something convertible to plain\n      numpy arrays. Their values will form a static part of the data structure\n      from the point of view of jax.tree_util. In particular this means these\n      coordinates will be passed as plain numpy arrays even inside a JIT'd\n      function, and the JIT'd function will be recompiled under the hood if the\n      coordinates of DataArrays passed into it change.\n      If this is not convenient for you, see also jax_coords below.\n    attrs: See xarray.Dataset.\n    jax_coords: Additional coordinates, which *can* use JAX arrays. These\n      coordinates will be treated as JAX data from the point of view of\n      jax.tree_util, that means when JIT'ing they will be passed as tracers and\n      computation involving them will be JIT'd.\n      Unfortunately a side-effect of this is that they can't be used as index\n      coordinates (because xarray's indexing logic is not JIT-able). If you\n      specify a coordinate with the same name as a dimension here, it will not\n      be set as an index coordinate; this behaviour is different to the default\n      for `coords`, and it means that things like `.sel` based on the jax\n      coordinate will not work.\n      Note we require `jax_coords` to be explicitly specified via a different\n      constructor argument to `coords`, rather than just looking for jax arrays\n      within the `coords` and treating them differently. This is because it\n      affects the way jax.tree_util treats them, which is somewhat orthogonal to\n      whether the value is passed in as numpy or not, and generally needs to be\n      handled consistently so is something we encourage explicit control over.\n\n  Returns:\n    An instance of xarray.Dataset. Where JAX arrays are used as data, they\n    will be wrapped with JaxArrayWrapper.\n  \"\"\"\n    wrapped_data_vars = {}\n    for (name, var_like) in data_vars.items():\n        if isinstance(var_like, jax.Array):\n            wrapped_data_vars[name] = wrap(var_like)\n        elif isinstance(var_like, tuple):\n            wrapped_data_vars[name] = (var_like[0], wrap(var_like[1])) + var_like[2:]\n        else:\n            wrapped_data_vars[name] = var_like\n    result = xarray.Dataset(data_vars=wrapped_data_vars, attrs=attrs)\n    return assign_coords(result, coords=coords, jax_coords=jax_coords)",
    "231": "def assign_coords(x: DatasetOrDataArray, *, coords: Optional[Mapping[Hashable, Any]]=None, jax_coords: Optional[Mapping[Hashable, Any]]=None) -> DatasetOrDataArray:\n    \"\"\"Replacement for assign_coords which works in presence of jax_coords.\n\n  `jax_coords` allow certain specified coordinates to have their data passed as\n  JAX arrays (including through jax.jit boundaries). The compromise in return is\n  that they are not created as index coordinates and cannot be used for .sel\n  and other coordinate-based indexing operations. See docs for `jax_coords` on\n  xarray_jax.Dataset and xarray_jax.DataArray for more information.\n\n  This function can be used to set jax_coords on an existing DataArray or\n  Dataset, and also to set a mix of jax and non-jax coordinates. It implements\n  some workarounds to prevent xarray trying and failing to create IndexVariables\n  from jax arrays under the hood.\n\n  If you have any jax_coords with the same name as a dimension, you'll need to\n  use this function instead of data_array.assign_coords or dataset.assign_coords\n  in general, to avoid an xarray bug where it tries (and in our case fails) to\n  create indexes for existing jax coords. See\n  https://github.com/pydata/xarray/issues/7885.\n\n  Args:\n    x: An xarray Dataset or DataArray.\n    coords: Dict of (non-JAX) coords, or None if not assigning any.\n    jax_coords: Dict of JAX coords, or None if not assigning any. See docs for\n      xarray_jax.Dataset / DataArray for more information on jax_coords.\n\n  Returns:\n    The Dataset or DataArray with coordinates assigned, similarly to\n    Dataset.assign_coords / DataArray.assign_coords.\n  \"\"\"\n    coords = {} if coords is None else dict(coords)\n    jax_coords = {} if jax_coords is None else dict(jax_coords)\n    existing_jax_coords = get_jax_coords(x)\n    jax_coords = existing_jax_coords | jax_coords\n    x = x.drop_vars(existing_jax_coords.keys())\n    renamed_jax_coords = {}\n    for (name, coord) in jax_coords.items():\n        if isinstance(coord, xarray.DataArray):\n            coord = coord.variable\n        if isinstance(coord, xarray.Variable):\n            coord = coord.copy(deep=False)\n        else:\n            coord = Variable((name,), coord)\n        coord.attrs[_JAX_COORD_ATTR_NAME] = True\n        renamed_jax_coords[f'__NONINDEX_{name}'] = coord\n    x = x.assign_coords(coords=coords | renamed_jax_coords)\n    rename_back_mapping = {f'__NONINDEX_{name}': name for name in jax_coords}\n    if isinstance(x, xarray.Dataset):\n        return x.rename_vars(rename_back_mapping)\n    else:\n        return x.rename(rename_back_mapping)",
    "233": "def assign_jax_coords(x: DatasetOrDataArray, jax_coords: Optional[Mapping[Hashable, Any]]=None, **jax_coords_kwargs) -> DatasetOrDataArray:\n    \"\"\"Assigns only jax_coords, with same API as xarray's assign_coords.\"\"\"\n    return assign_coords(x, jax_coords=jax_coords or jax_coords_kwargs)",
    "235": "def unwrap(value, require_jax=False):\n    \"\"\"Unwraps wrapped JAX arrays used in xarray, passing through other values.\"\"\"\n    if isinstance(value, JaxArrayWrapper):\n        return value.jax_array\n    elif isinstance(value, jax.Array):\n        return value\n    elif require_jax:\n        raise TypeError(f'Expected JAX array, found {type(value)}.')\n    else:\n        return value",
    "236": "def _wrapped(func):\n    \"\"\"Surrounds a function with JAX array unwrapping/wrapping.\"\"\"\n\n    def wrapped_func(*args, **kwargs):\n        (args, kwargs) = tree.map_structure(unwrap, (args, kwargs))\n        result = func(*args, **kwargs)\n        return tree.map_structure(wrap, result)\n    return wrapped_func",
    "238": "def unwrap_data(value: Union[xarray.Variable, xarray.DataArray], require_jax: bool=False) -> Union[jax.Array, np.ndarray]:\n    \"\"\"The unwrapped (see unwrap) data of a an xarray.Variable or DataArray.\"\"\"\n    return unwrap(value.data, require_jax=require_jax)",
    "239": "def unwrap_vars(dataset: Mapping[Hashable, xarray.DataArray], require_jax: bool=False) -> Mapping[str, Union[jax.Array, np.ndarray]]:\n    \"\"\"The unwrapped data (see unwrap) of the variables in a dataset.\"\"\"\n    return {str(name): unwrap_data(var, require_jax=require_jax) for (name, var) in dataset.items()}",
    "240": "def unwrap_coords(dataset: Union[xarray.Dataset, xarray.DataArray], require_jax: bool=False) -> Mapping[str, Union[jax.Array, np.ndarray]]:\n    \"\"\"The unwrapped data (see unwrap) of the coords in a Dataset or DataArray.\"\"\"\n    return {str(name): unwrap_data(var, require_jax=require_jax) for (name, var) in dataset.coords.items()}",
    "242": "def jax_vars(dataset: Mapping[Hashable, xarray.DataArray]) -> Mapping[str, jax.Array]:\n    \"\"\"Like unwrap_vars, but will complain if vars are not all jax arrays.\"\"\"\n    return cast(Mapping[str, jax.Array], unwrap_vars(dataset, require_jax=True))",
    "243": "class JaxArrayWrapper(np.lib.mixins.NDArrayOperatorsMixin):\n    \"\"\"Wraps a JAX array into a duck-typed array suitable for use with xarray.\n\n  This uses an older duck-typed array protocol based on __array_ufunc__ and\n  __array_function__ which works with numpy and xarray. (In newer versions\n  of xarray it implements xarray.namedarray._typing._array_function.)\n\n  This is in the process of being superseded by the Python array API standard\n  (https://data-apis.org/array-api/latest/index.html), but JAX hasn't\n  implemented it yet. Once they have, we should be able to get rid of\n  this wrapper and use JAX arrays directly with xarray.\n\n  \"\"\"\n\n    def __init__(self, jax_array):\n        self.jax_array = jax_array\n\n    def __array_ufunc__(self, ufunc, method, *args, **kwargs):\n        for x in args:\n            if not isinstance(x, (jax.typing.ArrayLike, type(self))):\n                return NotImplemented\n        if method != '__call__':\n            return NotImplemented\n        try:\n            func = getattr(jnp, ufunc.__name__)\n        except AttributeError:\n            return NotImplemented\n        kwargs.pop('out', None)\n        return _wrapped(func)(*args, **kwargs)\n\n    def __array_function__(self, func, types, args, kwargs):\n        try:\n            func = getattr(jnp, func.__name__)\n        except AttributeError:\n            return NotImplemented\n        return _wrapped(func)(*args, **kwargs)\n\n    def __repr__(self):\n        return f'xarray_jax.JaxArrayWrapper({repr(self.jax_array)})'\n\n    @property\n    def shape(self):\n        return self.jax_array.shape\n\n    @property\n    def dtype(self):\n        return self.jax_array.dtype\n\n    @property\n    def ndim(self):\n        return self.jax_array.ndim\n\n    @property\n    def size(self):\n        return self.jax_array.size\n\n    @property\n    def real(self):\n        return self.jax_array.real\n\n    @property\n    def imag(self):\n        return self.jax_array.imag\n\n    def __array__(self, dtype=None, context=None):\n        return np.asarray(self.jax_array, dtype=dtype)\n    __getitem__ = _wrapped(lambda array, *args: array.__getitem__(*args))\n    astype = _wrapped(lambda array, *args, **kwargs: array.astype(*args))\n    transpose = _wrapped(jnp.transpose)\n    reshape = _wrapped(jnp.reshape)\n    all = _wrapped(jnp.all)",
    "245": "def __array_ufunc__(self, ufunc, method, *args, **kwargs):\n    for x in args:\n        if not isinstance(x, (jax.typing.ArrayLike, type(self))):\n            return NotImplemented\n    if method != '__call__':\n        return NotImplemented\n    try:\n        func = getattr(jnp, ufunc.__name__)\n    except AttributeError:\n        return NotImplemented\n    kwargs.pop('out', None)\n    return _wrapped(func)(*args, **kwargs)",
    "247": "def __repr__(self):\n    return f'xarray_jax.JaxArrayWrapper({repr(self.jax_array)})'",
    "249": "@property\ndef dtype(self):\n    return self.jax_array.dtype",
    "251": "@property\ndef size(self):\n    return self.jax_array.size",
    "253": "@property\ndef imag(self):\n    return self.jax_array.imag",
    "255": "def apply_ufunc(func, *args, require_jax=False, **apply_ufunc_kwargs):\n    \"\"\"Like xarray.apply_ufunc but for jax-specific ufuncs.\n\n  Many numpy ufuncs will work fine out of the box with xarray_jax and\n  JaxArrayWrapper, since JaxArrayWrapper quacks (mostly) like a numpy array and\n  will convert many numpy operations to jax ops under the hood. For these\n  situations, xarray.apply_ufunc should work fine.\n\n  But sometimes you need a jax-specific ufunc which needs to be given a\n  jax array as input or return a jax array as output. In that case you should\n  use this helper as it will remove any JaxArrayWrapper before calling the func,\n  and wrap the result afterwards before handing it back to xarray.\n\n  Args:\n    func: A function that works with jax arrays (e.g. using functions from\n      jax.numpy) but otherwise meets the spec for the func argument to\n      xarray.apply_ufunc.\n    *args: xarray arguments to be mapped to arguments for func\n      (see xarray.apply_ufunc).\n    require_jax: Whether to require that inputs are based on jax arrays or allow\n      those based on plain numpy arrays too.\n    **apply_ufunc_kwargs: See xarray.apply_ufunc.\n\n  Returns:\n    Corresponding xarray results (see xarray.apply_ufunc).\n  \"\"\"\n\n    def wrapped_func(*maybe_wrapped_args):\n        unwrapped_args = [unwrap(a, require_jax) for a in maybe_wrapped_args]\n        result = func(*unwrapped_args)\n        return jax.tree_util.tree_map(wrap, result)\n    return xarray.apply_ufunc(wrapped_func, *args, **apply_ufunc_kwargs)",
    "257": "def pmap(fn: Callable[..., Any], dim: str, axis_name: Optional[str]=None, devices: ...=None, backend: ...=None) -> Callable[..., Any]:\n    \"\"\"Wraps a subset of jax.pmap functionality to handle xarray input/output.\n\n  Constraints:\n    * Any Dataset or DataArray passed to the function must have `dim` as the\n      first dimension. This will be checked. You can ensure this if necessary\n      by calling `.transpose(dim, ...)` beforehand.\n    * All args and return values will be mapped over the first dimension,\n      it will use in_axes=0, out_axes=0.\n    * No support for static_broadcasted_argnums, donate_argnums etc.\n\n  Args:\n    fn: Function to be pmap'd which takes and returns trees which may contain\n      xarray Dataset/DataArray. Any Dataset/DataArrays passed as input must use\n      `dim` as the first dimension on all arrays.\n    dim: The xarray dimension name corresponding to the first dimension that is\n      pmapped over (pmap is called with in_axes=0, out_axes=0).\n    axis_name: Used by jax to identify the mapped axis so that parallel\n      collectives can be applied. Defaults to same as `dim`.\n    devices:\n    backend:\n      See jax.pmap.\n\n  Returns:\n    A pmap'd version of `fn`, which takes and returns Dataset/DataArray with an\n    extra leading dimension `dim` relative to what the original `fn` sees.\n  \"\"\"\n    input_treedef = None\n    output_treedef = None\n\n    def fn_passed_to_pmap(*flat_args):\n        assert input_treedef is not None\n\n        def check_and_remove_leading_dim(dims):\n            try:\n                index = dims.index(dim)\n            except ValueError:\n                index = None\n            if index != 0:\n                raise ValueError(f'Expected dim {dim} at index 0, found at {index}.')\n            return dims[1:]\n        with dims_change_on_unflatten(check_and_remove_leading_dim):\n            args = jax.tree_util.tree_unflatten(input_treedef, flat_args)\n        result = fn(*args)\n        nonlocal output_treedef\n        (flat_result, output_treedef) = jax.tree_util.tree_flatten(result)\n        return flat_result\n    pmapped_fn = jax.pmap(fn_passed_to_pmap, axis_name=axis_name or dim, in_axes=0, out_axes=0, devices=devices, backend=backend)\n\n    def result_fn(*args):\n        nonlocal input_treedef\n        (flat_args, input_treedef) = jax.tree_util.tree_flatten(args)\n        flat_result = pmapped_fn(*flat_args)\n        assert output_treedef is not None\n        with dims_change_on_unflatten(lambda dims: (dim,) + dims):\n            return jax.tree_util.tree_unflatten(output_treedef, flat_result)\n    return result_fn",
    "258": "def fn_passed_to_pmap(*flat_args):\n    assert input_treedef is not None\n\n    def check_and_remove_leading_dim(dims):\n        try:\n            index = dims.index(dim)\n        except ValueError:\n            index = None\n        if index != 0:\n            raise ValueError(f'Expected dim {dim} at index 0, found at {index}.')\n        return dims[1:]\n    with dims_change_on_unflatten(check_and_remove_leading_dim):\n        args = jax.tree_util.tree_unflatten(input_treedef, flat_args)\n    result = fn(*args)\n    nonlocal output_treedef\n    (flat_result, output_treedef) = jax.tree_util.tree_flatten(result)\n    return flat_result",
    "260": "def result_fn(*args):\n    nonlocal input_treedef\n    (flat_args, input_treedef) = jax.tree_util.tree_flatten(args)\n    flat_result = pmapped_fn(*flat_args)\n    assert output_treedef is not None\n    with dims_change_on_unflatten(lambda dims: (dim,) + dims):\n        return jax.tree_util.tree_unflatten(output_treedef, flat_result)",
    "261": "@contextlib.contextmanager\ndef dims_change_on_unflatten(dims_change_fn: DimsChangeFn):\n    \"\"\"Can be used to change the dims used when unflattening arrays into xarrays.\n\n  This is useful when some axes were added to / removed from the underlying jax\n  arrays after they were flattened using jax.tree_util.tree_flatten, and you\n  want to unflatten them again afterwards using the original treedef but\n  adjusted for the added/removed dimensions.\n\n  It can also be used with jax.tree_util.tree_map, when it's called with a\n  function that adds/removes axes or otherwise changes the axis order.\n\n  When dimensions are removed, any coordinates using those removed dimensions\n  will also be removed on unflatten.\n\n  This is implemented as a context manager that sets some thread-local state\n  affecting the behaviour of our unflatten functions, because it's not possible\n  to directly modify the treedef to change the dims/coords in it (and with\n  tree_map, the treedef isn't exposed to you anyway).\n\n  Args:\n    dims_change_fn: Maps a tuple of dimension names for the original\n      Variable/DataArray/Dataset that was flattened, to an updated tuple of\n      dimensions which should be used when unflattening.\n\n  Yields:\n    To a context manager in whose scope jax.tree_util.tree_unflatten and\n    jax.tree_util.tree_map will apply the dims_change_fn before reconstructing\n    xarrays from jax arrays.\n  \"\"\"\n    token = _DIMS_CHANGE_ON_UNFLATTEN_FN.set(dims_change_fn)\n    try:\n        yield\n    finally:\n        _DIMS_CHANGE_ON_UNFLATTEN_FN.reset(token)",
    "263": "def _unflatten_variable(aux: Tuple[Hashable, ...], children: Tuple[jax.typing.ArrayLike]) -> xarray.Variable:\n    \"\"\"Unflattens a Variable for jax.tree_util.\"\"\"\n    dims = aux\n    dims_change_fn = _DIMS_CHANGE_ON_UNFLATTEN_FN.get(None)\n    if dims_change_fn:\n        dims = dims_change_fn(dims)\n    return Variable(dims=dims, data=children[0])",
    "264": "def _split_static_and_jax_coords(coords: xarray.core.coordinates.Coordinates) -> Tuple[Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]]:\n    static_coord_vars = {}\n    jax_coord_vars = {}\n    for (name, coord) in coords.items():\n        if coord.attrs.get(_JAX_COORD_ATTR_NAME, False):\n            jax_coord_vars[name] = coord.variable\n        else:\n            assert not isinstance(coord, (jax.Array, JaxArrayWrapper))\n            static_coord_vars[name] = coord.variable\n    return (static_coord_vars, jax_coord_vars)",
    "266": "class _HashableCoords(collections.abc.Mapping):\n    \"\"\"Wraps a dict of xarray Variables as hashable, used for static coordinates.\n\n  This needs to be hashable so that when an xarray.Dataset is passed to a\n  jax.jit'ed function, jax can check whether it's seen an array with the\n  same static coordinates(*) before or whether it needs to recompile the\n  function for the new values of the static coordinates.\n\n  (*) note jax_coords are not included in this; their value can be different\n  on different calls without triggering a recompile.\n  \"\"\"\n\n    def __init__(self, coord_vars: Mapping[Hashable, xarray.Variable]):\n        self._variables = coord_vars\n\n    def __repr__(self) -> str:\n        return f'_HashableCoords({repr(self._variables)})'\n\n    def __getitem__(self, key: Hashable) -> xarray.Variable:\n        return self._variables[key]\n\n    def __len__(self) -> int:\n        return len(self._variables)\n\n    def __iter__(self) -> Iterator[Hashable]:\n        return iter(self._variables)\n\n    def __hash__(self):\n        if not hasattr(self, '_hash'):\n            self._hash = hash(frozenset(((name, var.data.tobytes()) for (name, var) in self._variables.items())))\n        return self._hash\n\n    def __eq__(self, other):\n        if self is other:\n            return True\n        elif not isinstance(other, type(self)):\n            return NotImplemented\n        elif self._variables is other._variables:\n            return True\n        else:\n            return self._variables.keys() == other._variables.keys() and all((variable.equals(other._variables[name]) for (name, variable) in self._variables.items()))",
    "268": "def __repr__(self) -> str:\n    return f'_HashableCoords({repr(self._variables)})'",
    "270": "def __len__(self) -> int:\n    return len(self._variables)",
    "272": "def __hash__(self):\n    if not hasattr(self, '_hash'):\n        self._hash = hash(frozenset(((name, var.data.tobytes()) for (name, var) in self._variables.items())))\n    return self._hash",
    "273": "def __eq__(self, other):\n    if self is other:\n        return True\n    elif not isinstance(other, type(self)):\n        return NotImplemented\n    elif self._variables is other._variables:\n        return True\n    else:\n        return self._variables.keys() == other._variables.keys() and all((variable.equals(other._variables[name]) for (name, variable) in self._variables.items()))",
    "274": "def _flatten_data_array(v: xarray.DataArray) -> Tuple[Tuple[xarray.Variable, Mapping[Hashable, xarray.Variable]], Tuple[Optional[Hashable], _HashableCoords]]:\n    \"\"\"Flattens a DataArray for jax.tree_util.\"\"\"\n    (static_coord_vars, jax_coord_vars) = _split_static_and_jax_coords(v.coords)\n    children = (v.variable, jax_coord_vars)\n    aux = (v.name, _HashableCoords(static_coord_vars))\n    return (children, aux)",
    "275": "def _unflatten_data_array(aux: Tuple[Optional[Hashable], _HashableCoords], children: Tuple[xarray.Variable, Mapping[Hashable, xarray.Variable]]) -> xarray.DataArray:\n    \"\"\"Unflattens a DataArray for jax.tree_util.\"\"\"\n    (variable, jax_coord_vars) = children\n    (name, static_coord_vars) = aux\n    static_coord_vars = _drop_with_none_of_dims(static_coord_vars, variable.dims)\n    return DataArray(variable, name=name, coords=static_coord_vars, jax_coords=jax_coord_vars)",
    "276": "def _flatten_dataset(dataset: xarray.Dataset) -> Tuple[Tuple[Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]], _HashableCoords]:\n    \"\"\"Flattens a Dataset for jax.tree_util.\"\"\"\n    variables = {name: data_array.variable for (name, data_array) in dataset.data_vars.items()}\n    (static_coord_vars, jax_coord_vars) = _split_static_and_jax_coords(dataset.coords)\n    children = (variables, jax_coord_vars)\n    aux = _HashableCoords(static_coord_vars)\n    return (children, aux)",
    "277": "def _unflatten_dataset(aux: _HashableCoords, children: Tuple[Mapping[Hashable, xarray.Variable], Mapping[Hashable, xarray.Variable]]) -> xarray.Dataset:\n    \"\"\"Unflattens a Dataset for jax.tree_util.\"\"\"\n    (data_vars, jax_coord_vars) = children\n    static_coord_vars = aux\n    dataset = xarray.Dataset(data_vars)\n    static_coord_vars = _drop_with_none_of_dims(static_coord_vars, dataset.dims)\n    return assign_coords(dataset, coords=static_coord_vars, jax_coords=jax_coord_vars)",
    "278": "class XarrayJaxTest(absltest.TestCase):\n\n    def test_jax_array_wrapper_with_numpy_api(self):\n        ones = jnp.ones((3, 4), dtype=np.float32)\n        x = xarray_jax.JaxArrayWrapper(ones)\n        x = np.abs((x + 2) * (x - 3))\n        x = x[:-1, 1:3]\n        x = np.concatenate([x, x + 1], axis=0)\n        x = np.transpose(x, (1, 0))\n        x = np.reshape(x, (-1,))\n        x = x.astype(np.int32)\n        self.assertIsInstance(x, xarray_jax.JaxArrayWrapper)\n        self.assertIsInstance(np.asarray(x), np.ndarray)\n\n    def test_jax_xarray_variable(self):\n\n        def ops_via_xarray(inputs):\n            x = xarray_jax.Variable(('lat', 'lon'), inputs)\n            x = np.abs((x + 2) * (x - 3))\n            x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n            x = xarray.Variable.concat([x, x + 1], dim='lat')\n            x = x.transpose('lon', 'lat')\n            x = x.stack(channels=('lon', 'lat'))\n            x = x.sum()\n            return xarray_jax.jax_data(x)\n        ones = jnp.ones((3, 4), dtype=np.float32)\n        result = ops_via_xarray(ones)\n        self.assertIsInstance(result, jax.Array)\n        jax.jit(ops_via_xarray)(ones)\n        jax.grad(ops_via_xarray)(ones)\n\n    def test_jax_xarray_data_array(self):\n\n        def ops_via_xarray(inputs):\n            x = xarray_jax.DataArray(dims=('lat', 'lon'), data=inputs, coords={'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n            x = np.abs((x + 2) * (x - 3))\n            x = x.sel({'lat': slice(0, 20)})\n            y = xarray_jax.DataArray(dims=('lat', 'lon'), data=ones, coords={'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n            x = xarray.concat([x, y], dim='lat')\n            x = x.transpose('lon', 'lat')\n            x = x.stack(channels=('lon', 'lat'))\n            x = x.unstack()\n            x = x.sum()\n            return xarray_jax.jax_data(x)\n        ones = jnp.ones((3, 4), dtype=np.float32)\n        result = ops_via_xarray(ones)\n        self.assertIsInstance(result, jax.Array)\n        jax.jit(ops_via_xarray)(ones)\n        jax.grad(ops_via_xarray)(ones)\n\n    def test_jax_xarray_dataset(self):\n\n        def ops_via_xarray(foo, bar):\n            x = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n            x = np.abs((x + 2) * (x - 3))\n            x = x.sel({'lat': slice(0, 20)})\n            y = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n            x = xarray.concat([x, y], dim='lat')\n            x = x.transpose('lon', 'lat', 'time')\n            x = x.stack(channels=('lon', 'lat'))\n            x = (x.foo + x.bar).sum()\n            return xarray_jax.jax_data(x)\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        result = ops_via_xarray(foo, bar)\n        self.assertIsInstance(result, jax.Array)\n        jax.jit(ops_via_xarray)(foo, bar)\n        jax.grad(ops_via_xarray)(foo, bar)\n\n    def test_jit_function_with_xarray_variable_arguments_and_return(self):\n        function = jax.jit(lambda v: v + 1)\n        with self.subTest('jax input'):\n            inputs = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n            _ = function(inputs)\n            outputs = function(inputs)\n            self.assertEqual(outputs.dims, inputs.dims)\n        with self.subTest('numpy input'):\n            inputs = xarray.Variable(('lat', 'lon'), np.ones((3, 4), dtype=np.float32))\n            _ = function(inputs)\n            outputs = function(inputs)\n            self.assertEqual(outputs.dims, inputs.dims)\n\n    def test_jit_problem_if_convert_to_plain_numpy_array(self):\n        inputs = xarray_jax.DataArray(data=jnp.ones((2,), dtype=np.float32), dims=('foo',))\n        with self.assertRaises(jax.errors.TracerArrayConversionError):\n            jax.jit(lambda data_array: data_array.values)(inputs)\n\n    def test_grad_function_with_xarray_variable_arguments(self):\n        x = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n        jax.grad(lambda v: xarray_jax.jax_data(v.sum()))(x)\n\n    def test_jit_function_with_xarray_data_array_arguments_and_return(self):\n        inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n        fn = jax.jit(lambda v: v + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertEqual(outputs.dims, inputs.dims)\n        chex.assert_trees_all_equal(outputs.coords, inputs.coords)\n\n    def test_jit_function_with_data_array_and_jax_coords(self):\n        inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': jnp.arange(4) * 10})\n        self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', inputs.indexes)\n\n        @jax.jit\n        def fn(v):\n            self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n            self.assertIn('lat', v.indexes)\n            self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n            self.assertNotIn('lon', v.indexes)\n            v = v + v.coords['lon']\n            return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', outputs.indexes)\n        self.assertEqual(outputs.dims, inputs.dims)\n        chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n        chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n        chex.assert_trees_all_equal(outputs.data, (inputs + inputs.coords['lon']).data)\n\n    def test_jit_function_with_xarray_dataset_arguments_and_return(self):\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        fn = jax.jit(lambda v: v + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertEqual({'foo', 'bar'}, outputs.data_vars.keys())\n        self.assertEqual(inputs.foo.dims, outputs.foo.dims)\n        self.assertEqual(inputs.bar.dims, outputs.bar.dims)\n        chex.assert_trees_all_equal(outputs.coords, inputs.coords)\n\n    def test_jit_function_with_dataset_and_jax_coords(self):\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': jnp.arange(4) * 10})\n        self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', inputs.indexes)\n\n        @jax.jit\n        def fn(v):\n            self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n            self.assertIn('lat', v.indexes)\n            self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n            self.assertNotIn('lon', v.indexes)\n            v = v + v.coords['lon']\n            return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n        _ = fn(inputs)\n        outputs = fn(inputs)\n        self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', outputs.indexes)\n        self.assertEqual(outputs.dims, inputs.dims)\n        chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n        chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n        outputs_dict = {key: outputs[key].data for key in outputs}\n        inputs_and_inputs_coords_dict = {key: (inputs + inputs.coords['lon'])[key].data for key in inputs + inputs.coords['lon']}\n        chex.assert_trees_all_equal(outputs_dict, inputs_and_inputs_coords_dict)\n\n    def test_flatten_unflatten_variable(self):\n        variable = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n        (children, aux) = xarray_jax._flatten_variable(variable)\n        hash(aux)\n        self.assertEqual(aux, aux)\n        roundtrip = xarray_jax._unflatten_variable(aux, children)\n        self.assertTrue(variable.equals(roundtrip))\n\n    def test_flatten_unflatten_data_array(self):\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': np.arange(4) * 10})\n        (children, aux) = xarray_jax._flatten_data_array(data_array)\n        hash(aux)\n        self.assertEqual(aux, aux)\n        roundtrip = xarray_jax._unflatten_data_array(aux, children)\n        self.assertTrue(data_array.equals(roundtrip))\n\n    def test_flatten_unflatten_dataset(self):\n        foo = jnp.ones((3, 4), dtype=np.float32)\n        bar = jnp.ones((2, 3, 4), dtype=np.float32)\n        dataset = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': np.arange(4) * 10})\n        (children, aux) = xarray_jax._flatten_dataset(dataset)\n        hash(aux)\n        self.assertEqual(aux, aux)\n        roundtrip = xarray_jax._unflatten_dataset(aux, children)\n        self.assertTrue(dataset.equals(roundtrip))\n\n    def test_flatten_unflatten_added_dim(self):\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n        (leaves, treedef) = jax.tree_util.tree_flatten(data_array)\n        leaves = [jnp.expand_dims(x, 0) for x in leaves]\n        with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n            with_new_dim = jax.tree_util.tree_unflatten(treedef, leaves)\n        self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n        xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))\n\n    def test_map_added_dim(self):\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n        with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n            with_new_dim = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, 0), data_array)\n        self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n        xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))\n\n    def test_map_remove_dim(self):\n        foo = jnp.ones((1, 3, 4), dtype=np.float32)\n        bar = jnp.ones((1, 2, 3, 4), dtype=np.float32)\n        dataset = xarray_jax.Dataset(data_vars={'foo': (('batch', 'lat', 'lon'), foo), 'bar': (('batch', 'time', 'lat', 'lon'), bar)}, coords={'batch': np.array([123]), 'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        with xarray_jax.dims_change_on_unflatten(lambda dims: dims[1:]):\n            with_removed_dim = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, 0), dataset)\n        self.assertEqual(('lat', 'lon'), with_removed_dim['foo'].dims)\n        self.assertEqual(('time', 'lat', 'lon'), with_removed_dim['bar'].dims)\n        self.assertNotIn('batch', with_removed_dim.dims)\n        self.assertNotIn('batch', with_removed_dim.coords)\n        xarray.testing.assert_identical(jax.device_get(dataset.isel(batch=0, drop=True)), jax.device_get(with_removed_dim))\n\n    def test_pmap(self):\n        devices = jax.local_device_count()\n        foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n        bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n        dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)})\n\n        def func(d):\n            self.assertNotIn('device', d.dims)\n            return d + 1\n        func = xarray_jax.pmap(func, dim='device')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n        dataset = dataset.drop_vars('foo')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n\n    def test_pmap_with_jax_coords(self):\n        devices = jax.local_device_count()\n        foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n        bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n        time = jnp.zeros((devices, 2), dtype=np.float32)\n        dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)}, coords={'lat': np.arange(3), 'lon': np.arange(4)}, jax_coords={'time': xarray_jax.Variable(('device', 'time'), time)})\n\n        def func(d):\n            self.assertNotIn('device', d.dims)\n            self.assertNotIn('device', d.coords['time'].dims)\n            self.assertIsInstance(d.coords['time'].data, xarray_jax.JaxArrayWrapper)\n            self.assertNotIn('time', d.indexes)\n            return d + 1\n        func = xarray_jax.pmap(func, dim='device')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n        dataset = dataset.drop_vars('foo')\n        result = func(dataset)\n        xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n\n    def test_pmap_with_tree_mix_of_xarray_and_jax_array(self):\n        devices = jax.local_device_count()\n        data_array = xarray_jax.DataArray(data=jnp.ones((devices, 3, 4), dtype=np.float32), dims=('device', 'lat', 'lon'))\n        plain_array = jnp.ones((devices, 2), dtype=np.float32)\n        inputs = {'foo': data_array, 'bar': plain_array}\n\n        def func(x):\n            return (x['foo'] + 1, x['bar'] + 1)\n        func = xarray_jax.pmap(func, dim='device')\n        (result_foo, result_bar) = func(inputs)\n        xarray.testing.assert_identical(jax.device_get(inputs['foo'] + 1), jax.device_get(result_foo))\n        np.testing.assert_array_equal(jax.device_get(inputs['bar'] + 1), jax.device_get(result_bar))\n\n    def test_pmap_complains_when_dim_not_first(self):\n        devices = jax.local_device_count()\n        data_array = xarray_jax.DataArray(data=jnp.ones((3, devices, 4), dtype=np.float32), dims=('lat', 'device', 'lon'))\n        func = xarray_jax.pmap(lambda x: x + 1, dim='device')\n        with self.assertRaisesRegex(ValueError, 'Expected dim device at index 0, found at 1'):\n            func(data_array)\n\n    def test_apply_ufunc(self):\n        inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 2], [3, 4]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n        result = xarray_jax.apply_ufunc(lambda x: jnp.sum(x, axis=-1), inputs, input_core_dims=[['x']])\n        expected_result = xarray_jax.DataArray(data=[4, 6], dims=('y',), coords={'y': [2, 3]})\n        xarray.testing.assert_identical(expected_result, jax.device_get(result))\n\n    def test_apply_ufunc_multiple_return_values(self):\n\n        def ufunc(array):\n            return (jnp.min(array, axis=-1), jnp.max(array, axis=-1))\n        inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 4], [3, 2]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n        result = xarray_jax.apply_ufunc(ufunc, inputs, input_core_dims=[['x']], output_core_dims=[[], []])\n        expected = (xarray_jax.DataArray(data=[1, 2], dims=('y',), coords={'y': [2, 3]}), xarray_jax.DataArray(data=[3, 4], dims=('y',), coords={'y': [2, 3]}))\n        xarray.testing.assert_identical(expected[0], jax.device_get(result[0]))\n        xarray.testing.assert_identical(expected[1], jax.device_get(result[1]))",
    "279": "def test_jax_array_wrapper_with_numpy_api(self):\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    x = xarray_jax.JaxArrayWrapper(ones)\n    x = np.abs((x + 2) * (x - 3))\n    x = x[:-1, 1:3]\n    x = np.concatenate([x, x + 1], axis=0)\n    x = np.transpose(x, (1, 0))\n    x = np.reshape(x, (-1,))\n    x = x.astype(np.int32)\n    self.assertIsInstance(x, xarray_jax.JaxArrayWrapper)\n    self.assertIsInstance(np.asarray(x), np.ndarray)",
    "280": "def test_jax_xarray_variable(self):\n\n    def ops_via_xarray(inputs):\n        x = xarray_jax.Variable(('lat', 'lon'), inputs)\n        x = np.abs((x + 2) * (x - 3))\n        x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n        x = xarray.Variable.concat([x, x + 1], dim='lat')\n        x = x.transpose('lon', 'lat')\n        x = x.stack(channels=('lon', 'lat'))\n        x = x.sum()\n        return xarray_jax.jax_data(x)\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    result = ops_via_xarray(ones)\n    self.assertIsInstance(result, jax.Array)\n    jax.jit(ops_via_xarray)(ones)\n    jax.grad(ops_via_xarray)(ones)",
    "281": "def ops_via_xarray(inputs):\n    x = xarray_jax.Variable(('lat', 'lon'), inputs)\n    x = np.abs((x + 2) * (x - 3))\n    x = x.isel({'lat': slice(0, -1), 'lon': slice(1, 3)})\n    x = xarray.Variable.concat([x, x + 1], dim='lat')\n    x = x.transpose('lon', 'lat')\n    x = x.stack(channels=('lon', 'lat'))\n    x = x.sum()\n    return xarray_jax.jax_data(x)",
    "282": "def test_jax_xarray_data_array(self):\n\n    def ops_via_xarray(inputs):\n        x = xarray_jax.DataArray(dims=('lat', 'lon'), data=inputs, coords={'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        x = np.abs((x + 2) * (x - 3))\n        x = x.sel({'lat': slice(0, 20)})\n        y = xarray_jax.DataArray(dims=('lat', 'lon'), data=ones, coords={'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n        x = xarray.concat([x, y], dim='lat')\n        x = x.transpose('lon', 'lat')\n        x = x.stack(channels=('lon', 'lat'))\n        x = x.unstack()\n        x = x.sum()\n        return xarray_jax.jax_data(x)\n    ones = jnp.ones((3, 4), dtype=np.float32)\n    result = ops_via_xarray(ones)\n    self.assertIsInstance(result, jax.Array)\n    jax.jit(ops_via_xarray)(ones)\n    jax.grad(ops_via_xarray)(ones)",
    "283": "def ops_via_xarray(inputs):\n    x = xarray_jax.DataArray(dims=('lat', 'lon'), data=inputs, coords={'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    x = np.abs((x + 2) * (x - 3))\n    x = x.sel({'lat': slice(0, 20)})\n    y = xarray_jax.DataArray(dims=('lat', 'lon'), data=ones, coords={'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n    x = xarray.concat([x, y], dim='lat')\n    x = x.transpose('lon', 'lat')\n    x = x.stack(channels=('lon', 'lat'))\n    x = x.unstack()\n    x = x.sum()\n    return xarray_jax.jax_data(x)",
    "284": "def test_jax_xarray_dataset(self):\n\n    def ops_via_xarray(foo, bar):\n        x = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n        x = np.abs((x + 2) * (x - 3))\n        x = x.sel({'lat': slice(0, 20)})\n        y = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n        x = xarray.concat([x, y], dim='lat')\n        x = x.transpose('lon', 'lat', 'time')\n        x = x.stack(channels=('lon', 'lat'))\n        x = (x.foo + x.bar).sum()\n        return xarray_jax.jax_data(x)\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    result = ops_via_xarray(foo, bar)\n    self.assertIsInstance(result, jax.Array)\n    jax.jit(ops_via_xarray)(foo, bar)\n    jax.grad(ops_via_xarray)(foo, bar)",
    "285": "def ops_via_xarray(foo, bar):\n    x = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    x = np.abs((x + 2) * (x - 3))\n    x = x.sel({'lat': slice(0, 20)})\n    y = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3, 6) * 10, 'lon': np.arange(4) * 10})\n    x = xarray.concat([x, y], dim='lat')\n    x = x.transpose('lon', 'lat', 'time')\n    x = x.stack(channels=('lon', 'lat'))\n    x = (x.foo + x.bar).sum()\n    return xarray_jax.jax_data(x)",
    "286": "def test_jit_function_with_xarray_variable_arguments_and_return(self):\n    function = jax.jit(lambda v: v + 1)\n    with self.subTest('jax input'):\n        inputs = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n        _ = function(inputs)\n        outputs = function(inputs)\n        self.assertEqual(outputs.dims, inputs.dims)\n    with self.subTest('numpy input'):\n        inputs = xarray.Variable(('lat', 'lon'), np.ones((3, 4), dtype=np.float32))\n        _ = function(inputs)\n        outputs = function(inputs)\n        self.assertEqual(outputs.dims, inputs.dims)",
    "288": "def test_grad_function_with_xarray_variable_arguments(self):\n    x = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n    jax.grad(lambda v: xarray_jax.jax_data(v.sum()))(x)",
    "289": "def test_jit_function_with_xarray_data_array_arguments_and_return(self):\n    inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n    fn = jax.jit(lambda v: v + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords, inputs.coords)",
    "290": "def test_jit_function_with_data_array_and_jax_coords(self):\n    inputs = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': jnp.arange(4) * 10})\n    self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', inputs.indexes)\n\n    @jax.jit\n    def fn(v):\n        self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n        self.assertIn('lat', v.indexes)\n        self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', v.indexes)\n        v = v + v.coords['lon']\n        return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', outputs.indexes)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n    chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n    chex.assert_trees_all_equal(outputs.data, (inputs + inputs.coords['lon']).data)",
    "291": "@jax.jit\ndef fn(v):\n    self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n    self.assertIn('lat', v.indexes)\n    self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', v.indexes)\n    v = v + v.coords['lon']\n    return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)",
    "292": "def test_jit_function_with_xarray_dataset_arguments_and_return(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    fn = jax.jit(lambda v: v + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertEqual({'foo', 'bar'}, outputs.data_vars.keys())\n    self.assertEqual(inputs.foo.dims, outputs.foo.dims)\n    self.assertEqual(inputs.bar.dims, outputs.bar.dims)\n    chex.assert_trees_all_equal(outputs.coords, inputs.coords)",
    "293": "def test_jit_function_with_dataset_and_jax_coords(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    inputs = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': jnp.arange(4) * 10})\n    self.assertIsInstance(inputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', inputs.indexes)\n\n    @jax.jit\n    def fn(v):\n        self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n        self.assertIn('lat', v.indexes)\n        self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('lon', v.indexes)\n        v = v + v.coords['lon']\n        return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)\n    _ = fn(inputs)\n    outputs = fn(inputs)\n    self.assertIsInstance(outputs.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', outputs.indexes)\n    self.assertEqual(outputs.dims, inputs.dims)\n    chex.assert_trees_all_equal(outputs.coords['lat'], inputs.coords['lat'])\n    chex.assert_trees_all_equal(outputs.coords['lon'].data, (inputs.coords['lon'] + 1).data)\n    outputs_dict = {key: outputs[key].data for key in outputs}\n    inputs_and_inputs_coords_dict = {key: (inputs + inputs.coords['lon'])[key].data for key in inputs + inputs.coords['lon']}\n    chex.assert_trees_all_equal(outputs_dict, inputs_and_inputs_coords_dict)",
    "294": "@jax.jit\ndef fn(v):\n    self.assertIsInstance(v.coords['lat'].data, np.ndarray)\n    self.assertIn('lat', v.indexes)\n    self.assertIsInstance(v.coords['lon'].data, xarray_jax.JaxArrayWrapper)\n    self.assertNotIn('lon', v.indexes)\n    v = v + v.coords['lon']\n    return xarray_jax.assign_jax_coords(v, lon=v.coords['lon'] + 1)",
    "295": "def test_flatten_unflatten_variable(self):\n    variable = xarray_jax.Variable(('lat', 'lon'), jnp.ones((3, 4), dtype=np.float32))\n    (children, aux) = xarray_jax._flatten_variable(variable)\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_variable(aux, children)\n    self.assertTrue(variable.equals(roundtrip))",
    "296": "def test_flatten_unflatten_data_array(self):\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3)}, jax_coords={'lon': np.arange(4) * 10})\n    (children, aux) = xarray_jax._flatten_data_array(data_array)\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_data_array(aux, children)\n    self.assertTrue(data_array.equals(roundtrip))",
    "297": "def test_flatten_unflatten_dataset(self):\n    foo = jnp.ones((3, 4), dtype=np.float32)\n    bar = jnp.ones((2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset(data_vars={'foo': (('lat', 'lon'), foo), 'bar': (('time', 'lat', 'lon'), bar)}, coords={'time': np.arange(2), 'lat': np.arange(3) * 10}, jax_coords={'lon': np.arange(4) * 10})\n    (children, aux) = xarray_jax._flatten_dataset(dataset)\n    hash(aux)\n    self.assertEqual(aux, aux)\n    roundtrip = xarray_jax._unflatten_dataset(aux, children)\n    self.assertTrue(dataset.equals(roundtrip))",
    "298": "def test_flatten_unflatten_added_dim(self):\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n    (leaves, treedef) = jax.tree_util.tree_flatten(data_array)\n    leaves = [jnp.expand_dims(x, 0) for x in leaves]\n    with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n        with_new_dim = jax.tree_util.tree_unflatten(treedef, leaves)\n    self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n    xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))",
    "299": "def test_map_added_dim(self):\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, 4), dtype=np.float32), dims=('lat', 'lon'), coords={'lat': np.arange(3), 'lon': np.arange(4) * 10})\n    with xarray_jax.dims_change_on_unflatten(lambda dims: ('new',) + dims):\n        with_new_dim = jax.tree_util.tree_map(lambda x: jnp.expand_dims(x, 0), data_array)\n    self.assertEqual(('new', 'lat', 'lon'), with_new_dim.dims)\n    xarray.testing.assert_identical(jax.device_get(data_array), jax.device_get(with_new_dim.isel(new=0)))",
    "300": "def test_map_remove_dim(self):\n    foo = jnp.ones((1, 3, 4), dtype=np.float32)\n    bar = jnp.ones((1, 2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset(data_vars={'foo': (('batch', 'lat', 'lon'), foo), 'bar': (('batch', 'time', 'lat', 'lon'), bar)}, coords={'batch': np.array([123]), 'time': np.arange(2), 'lat': np.arange(3) * 10, 'lon': np.arange(4) * 10})\n    with xarray_jax.dims_change_on_unflatten(lambda dims: dims[1:]):\n        with_removed_dim = jax.tree_util.tree_map(lambda x: jnp.squeeze(x, 0), dataset)\n    self.assertEqual(('lat', 'lon'), with_removed_dim['foo'].dims)\n    self.assertEqual(('time', 'lat', 'lon'), with_removed_dim['bar'].dims)\n    self.assertNotIn('batch', with_removed_dim.dims)\n    self.assertNotIn('batch', with_removed_dim.coords)\n    xarray.testing.assert_identical(jax.device_get(dataset.isel(batch=0, drop=True)), jax.device_get(with_removed_dim))",
    "301": "def test_pmap(self):\n    devices = jax.local_device_count()\n    foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n    bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n    dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)})\n\n    def func(d):\n        self.assertNotIn('device', d.dims)\n        return d + 1\n    func = xarray_jax.pmap(func, dim='device')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n    dataset = dataset.drop_vars('foo')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))",
    "303": "def test_pmap_with_jax_coords(self):\n    devices = jax.local_device_count()\n    foo = jnp.zeros((devices, 3, 4), dtype=np.float32)\n    bar = jnp.zeros((devices, 2, 3, 4), dtype=np.float32)\n    time = jnp.zeros((devices, 2), dtype=np.float32)\n    dataset = xarray_jax.Dataset({'foo': (('device', 'lat', 'lon'), foo), 'bar': (('device', 'time', 'lat', 'lon'), bar)}, coords={'lat': np.arange(3), 'lon': np.arange(4)}, jax_coords={'time': xarray_jax.Variable(('device', 'time'), time)})\n\n    def func(d):\n        self.assertNotIn('device', d.dims)\n        self.assertNotIn('device', d.coords['time'].dims)\n        self.assertIsInstance(d.coords['time'].data, xarray_jax.JaxArrayWrapper)\n        self.assertNotIn('time', d.indexes)\n        return d + 1\n    func = xarray_jax.pmap(func, dim='device')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))\n    dataset = dataset.drop_vars('foo')\n    result = func(dataset)\n    xarray.testing.assert_identical(jax.device_get(dataset + 1), jax.device_get(result))",
    "305": "def test_pmap_with_tree_mix_of_xarray_and_jax_array(self):\n    devices = jax.local_device_count()\n    data_array = xarray_jax.DataArray(data=jnp.ones((devices, 3, 4), dtype=np.float32), dims=('device', 'lat', 'lon'))\n    plain_array = jnp.ones((devices, 2), dtype=np.float32)\n    inputs = {'foo': data_array, 'bar': plain_array}\n\n    def func(x):\n        return (x['foo'] + 1, x['bar'] + 1)\n    func = xarray_jax.pmap(func, dim='device')\n    (result_foo, result_bar) = func(inputs)\n    xarray.testing.assert_identical(jax.device_get(inputs['foo'] + 1), jax.device_get(result_foo))\n    np.testing.assert_array_equal(jax.device_get(inputs['bar'] + 1), jax.device_get(result_bar))",
    "307": "def test_pmap_complains_when_dim_not_first(self):\n    devices = jax.local_device_count()\n    data_array = xarray_jax.DataArray(data=jnp.ones((3, devices, 4), dtype=np.float32), dims=('lat', 'device', 'lon'))\n    func = xarray_jax.pmap(lambda x: x + 1, dim='device')\n    with self.assertRaisesRegex(ValueError, 'Expected dim device at index 0, found at 1'):\n        func(data_array)",
    "308": "def test_apply_ufunc(self):\n    inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 2], [3, 4]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n    result = xarray_jax.apply_ufunc(lambda x: jnp.sum(x, axis=-1), inputs, input_core_dims=[['x']])\n    expected_result = xarray_jax.DataArray(data=[4, 6], dims=('y',), coords={'y': [2, 3]})\n    xarray.testing.assert_identical(expected_result, jax.device_get(result))",
    "309": "def test_apply_ufunc_multiple_return_values(self):\n\n    def ufunc(array):\n        return (jnp.min(array, axis=-1), jnp.max(array, axis=-1))\n    inputs = xarray_jax.DataArray(data=jnp.asarray([[1, 4], [3, 2]]), dims=('x', 'y'), coords={'x': [0, 1], 'y': [2, 3]})\n    result = xarray_jax.apply_ufunc(ufunc, inputs, input_core_dims=[['x']], output_core_dims=[[], []])\n    expected = (xarray_jax.DataArray(data=[1, 2], dims=('y',), coords={'y': [2, 3]}), xarray_jax.DataArray(data=[3, 4], dims=('y',), coords={'y': [2, 3]}))\n    xarray.testing.assert_identical(expected[0], jax.device_get(result[0]))\n    xarray.testing.assert_identical(expected[1], jax.device_get(result[1]))",
    "311": "def map_structure(func: Callable[..., Any], *structures: Any) -> Any:\n    \"\"\"Maps func through given structures with xarrays. See tree.map_structure.\"\"\"\n    if not callable(func):\n        raise TypeError(f'func must be callable, got: {func}')\n    if not structures:\n        raise ValueError('Must provide at least one structure')\n    first = structures[0]\n    if isinstance(first, xarray.Dataset):\n        data = {k: func(*[s[k] for s in structures]) for k in first.keys()}\n        if all((isinstance(a, (type(None), xarray.DataArray)) for a in data.values())):\n            data_arrays = [v.rename(k) for (k, v) in data.items() if v is not None]\n            try:\n                return xarray.merge(data_arrays, join='exact')\n            except ValueError:\n                pass\n        return data\n    if isinstance(first, dict):\n        return {k: map_structure(func, *[s[k] for s in structures]) for k in first.keys()}\n    if isinstance(first, (list, tuple, set)):\n        return type(first)((map_structure(func, *s) for s in zip(*structures)))\n    return func(*structures)",
    "312": "class XarrayTreeTest(absltest.TestCase):\n\n    def test_map_structure_maps_over_leaves_but_preserves_dataset_type(self):\n\n        def fn(leaf):\n            self.assertIsInstance(leaf, xarray.DataArray)\n            result = leaf + 1\n            result = result.rename(None)\n            return result\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertIsInstance(result, xarray.Dataset)\n        self.assertSameElements({'foo', 'bar'}, result.keys())\n\n    def test_map_structure_on_data_arrays(self):\n        data_arrays = dict(TEST_DATASET)\n        result = xarray_tree.map_structure(lambda x: x + 1, data_arrays)\n        self.assertIsInstance(result, dict)\n        self.assertSameElements({'foo', 'bar'}, result.keys())\n\n    def test_map_structure_on_dataset_plain_dict_when_coords_incompatible(self):\n\n        def fn(leaf):\n            if leaf.name == 'foo':\n                return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [1, 2]})\n            else:\n                return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [3, 4]})\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertIsInstance(result, dict)\n        self.assertSameElements({'foo', 'bar'}, result.keys())\n\n    def test_map_structure_on_dataset_drops_vars_with_none_return_values(self):\n\n        def fn(leaf):\n            return leaf if leaf.name == 'foo' else None\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertIsInstance(result, xarray.Dataset)\n        self.assertSameElements({'foo'}, result.keys())\n\n    def test_map_structure_on_dataset_returns_plain_dict_other_return_types(self):\n\n        def fn(leaf):\n            self.assertIsInstance(leaf, xarray.DataArray)\n            return 'not a DataArray'\n        result = xarray_tree.map_structure(fn, TEST_DATASET)\n        self.assertEqual({'foo': 'not a DataArray', 'bar': 'not a DataArray'}, result)\n\n    def test_map_structure_two_args_different_variable_orders(self):\n        dataset_different_order = TEST_DATASET[['bar', 'foo']]\n\n        def fn(arg1, arg2):\n            self.assertEqual(arg1.name, arg2.name)\n        xarray_tree.map_structure(fn, TEST_DATASET, dataset_different_order)",
    "313": "def test_map_structure_maps_over_leaves_but_preserves_dataset_type(self):\n\n    def fn(leaf):\n        self.assertIsInstance(leaf, xarray.DataArray)\n        result = leaf + 1\n        result = result.rename(None)\n        return result\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, xarray.Dataset)\n    self.assertSameElements({'foo', 'bar'}, result.keys())",
    "315": "def test_map_structure_on_data_arrays(self):\n    data_arrays = dict(TEST_DATASET)\n    result = xarray_tree.map_structure(lambda x: x + 1, data_arrays)\n    self.assertIsInstance(result, dict)\n    self.assertSameElements({'foo', 'bar'}, result.keys())",
    "316": "def test_map_structure_on_dataset_plain_dict_when_coords_incompatible(self):\n\n    def fn(leaf):\n        if leaf.name == 'foo':\n            return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [1, 2]})\n        else:\n            return xarray.DataArray(data=np.zeros(2), dims=('x',), coords={'x': [3, 4]})\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, dict)\n    self.assertSameElements({'foo', 'bar'}, result.keys())",
    "318": "def test_map_structure_on_dataset_drops_vars_with_none_return_values(self):\n\n    def fn(leaf):\n        return leaf if leaf.name == 'foo' else None\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertIsInstance(result, xarray.Dataset)\n    self.assertSameElements({'foo'}, result.keys())",
    "320": "def test_map_structure_on_dataset_returns_plain_dict_other_return_types(self):\n\n    def fn(leaf):\n        self.assertIsInstance(leaf, xarray.DataArray)\n        return 'not a DataArray'\n    result = xarray_tree.map_structure(fn, TEST_DATASET)\n    self.assertEqual({'foo': 'not a DataArray', 'bar': 'not a DataArray'}, result)",
    "322": "def test_map_structure_two_args_different_variable_orders(self):\n    dataset_different_order = TEST_DATASET[['bar', 'foo']]\n\n    def fn(arg1, arg2):\n        self.assertEqual(arg1.name, arg2.name)\n    xarray_tree.map_structure(fn, TEST_DATASET, dataset_different_order)"
}